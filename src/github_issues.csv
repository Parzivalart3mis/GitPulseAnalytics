repo,number,title,state,created_at,closed_at,labels
meta-llama/llama3,411,"Error from ""llama-model download --source meta --model-id Llama-4-Scout-17B-16E""",open,2025-11-05 23:30:18,,
meta-llama/llama3,409,Update generation.py,open,2025-10-09 20:33:12,,CLA Signed
meta-llama/llama3,408,Example teting,closed,2025-10-05 12:45:48,2025-10-05 12:53:14,
meta-llama/llama3,392,"sp_model = SentencePieceProcessor()  sp_model.Load(""/home/imss/zxhhhh/llama-3-8b/tokenizer.model"")",open,2025-03-17 14:00:53,,
meta-llama/llama3,356,Model being very repetitive,open,2024-10-29 05:40:34,,
meta-llama/llama3,295,Update download.sh,closed,2024-07-28 10:47:58,2025-10-10 13:59:19,CLA Signed
ollama/ollama,13275,Model request: Qwen-Next,open,2025-11-30 08:54:58,,
ollama/ollama,13274,.gitattributes: add app/webview to linguist-vendored,closed,2025-11-30 04:41:10,2025-11-30 04:46:10,
ollama/ollama,13273,Core dump on detecting ROCm GPUs in docker/podman,open,2025-11-29 22:49:44,,bug
ollama/ollama,13272,Copilot/create n8n automation,open,2025-11-29 20:33:23,,
ollama/ollama,13271,Pull of DeepSeek Coder fails with timeout on Arch Linux,open,2025-11-29 15:41:28,,bug
ollama/ollama,13270,"Model request: Fara-7B, An Efficient Agentic Model for Computer Use",open,2025-11-29 03:14:37,,
ollama/ollama,13269,Error 5: Unable to execute file in the temporary directory. Setup aborted.,open,2025-11-29 03:09:46,,bug
ollama/ollama,13268,model: add intellect-3 parser and prompt renderer,open,2025-11-28 23:12:16,,
ollama/ollama,13267,Models cant proces images,closed,2025-11-28 19:50:52,2025-11-28 20:45:54,bug
ollama/ollama,13266,Pulling a model changes server version information?,closed,2025-11-28 14:53:45,2025-11-29 11:13:10,bug
ollama/ollama,13265,Add Vulkan GPU support instructions in development.md,open,2025-11-28 14:21:37,,
ollama/ollama,13264,Upgrade go to 1.24.6 and crypto 0.45.0,open,2025-11-28 12:20:46,,
ollama/ollama,13263,cmd/bench: support writing benchmark output to file,open,2025-11-28 10:14:16,,
ollama/ollama,13262,The latest version of ollama cannot deploy the qwen3-instruct model that I fine-tuned.,open,2025-11-28 08:16:55,,bug
ollama/ollama,13261,Make low_vram_mode threshold configurable,open,2025-11-28 07:16:41,,bug
ollama/ollama,13260,model: fix unused result in BenchmarkBytePairEncoding,open,2025-11-27 19:54:04,,
ollama/ollama,13259,Multi-file GGUF models loading support,open,2025-11-27 03:09:21,,
ollama/ollama,13258,Incorrect Content type header by API,open,2025-11-27 03:01:29,,bug
ollama/ollama,13257,v0.13.0 -- Vision Not Working on GUI app (Windows 11),closed,2025-11-26 23:50:35,2025-11-27 00:08:22,bug
ollama/ollama,13256,#13255 Updated to Remove Apostrophes That Caused the curl Request to Not Complete,open,2025-11-26 17:18:01,,
ollama/ollama,13255,Apostrophe 'S' on tool-calling.mdx Page Causes curl Request Example to Not Complete,open,2025-11-26 17:12:51,,bug
ollama/ollama,13254,Hunyuan OCR 1B,closed,2025-11-26 16:50:54,2025-11-26 16:51:41,
ollama/ollama,13253,AI emotional empathy and multi level system prompts,closed,2025-11-26 13:35:08,2025-11-26 14:16:32,feature request
ollama/ollama,13252,How can I use deepseek-ocr:3b from the API?,open,2025-11-26 13:30:02,,bug
ollama/ollama,13251,Model request: tencent/HunyuanOCR,open,2025-11-26 08:12:31,,model
ollama/ollama,13250,Cannot load model even tho I have enough vRAM,open,2025-11-26 08:10:01,,bug
ollama/ollama,13249,could anyone tell me which version is stableÔºübecause i wanna make sure that i can use qwen3 text,open,2025-11-26 08:09:37,,
ollama/ollama,13248,conf: support human-readable context length format,open,2025-11-26 06:55:10,,
ollama/ollama,13247,"Qwen3-VL models (2B/4B) not utilizing GPU on Jetson Orin Nano Super (JetPack 6.2.1), while Qwen2.5-VL:3B works correctly",open,2025-11-26 06:38:34,,bug
ollama/ollama,13246,install Ollama on the Dell PowerMax GB10,open,2025-11-26 02:44:02,,
ollama/ollama,13245,build(deps): bump golang.org/x/crypto from 0.36.0 to 0.45.0,open,2025-11-26 02:27:06,,
ollama/ollama,13244,ggml: Use max graph memory allocation when reserving,open,2025-11-26 00:36:14,,
ollama/ollama,13243,olmo: model implementation ,open,2025-11-25 23:50:52,,
ollama/ollama,13242,llm: Don't always evict models on CPU-only systems,open,2025-11-25 23:06:01,,
ollama/ollama,13241,Nvidia GPUs not available as vulkan compute unit in docker,open,2025-11-25 23:04:13,,bug
ollama/ollama,13240,ollama/ollama: adding Arize to Read.me,open,2025-11-25 22:24:38,,
ollama/ollama,13239,non-stream mode returns done:false,open,2025-11-25 14:27:05,,bug
ollama/ollama,13238,server: fix missing logprobs in tool calls,open,2025-11-25 10:57:49,,
ollama/ollama,13237,docs: remove deprecated parameter,closed,2025-11-25 09:28:30,2025-11-26 02:03:09,
ollama/ollama,13236,ROCm GPU discovery times out on AMD Radeon AI PRO R9700 (gfx1201) ‚Äì CPU fallback only,open,2025-11-25 08:00:40,,bug
ollama/ollama,13235,VRAM runs out when loading models one after another,open,2025-11-25 05:25:33,,bug
ollama/ollama,13234,Model get stuck in stopping state!,open,2025-11-25 02:37:14,,bug
ollama/ollama,13233,bootstrapDevices() may silently fail without logging an error,closed,2025-11-24 22:35:30,2025-11-24 23:44:40,bug
ollama/ollama,13232,CPU/GPU usage remains high after response is completed,open,2025-11-24 22:05:00,,bug
ollama/ollama,13231,Fix output formatting in docs/faq.mdx,closed,2025-11-24 20:27:49,2025-11-29 00:19:21,
ollama/ollama,13230,`ollama serve` causes `ollama ps` to always return empty results,closed,2025-11-24 16:53:52,2025-11-25 01:31:20,bug
ollama/ollama,13229,Inconsistent Responses and `done:false` with mistral-small3.1:24b (caused by repeated token ?),open,2025-11-24 16:17:55,,bug
ollama/ollama,13228,docs: add AGENTS.md for AI coding assistants,open,2025-11-24 15:19:55,,
ollama/ollama,13227,"Ollama evicts previously loaded model, although system memory is suficient",open,2025-11-24 14:20:41,,bug
ollama/ollama,13226,Cannot change model location,open,2025-11-24 13:45:10,,"bug,app"
ollama/ollama,13225,GPU Offline Issues During Extended Operation,open,2025-11-24 09:58:37,,feature request
ollama/ollama,13224,Error loading mistral-small3.2:24b on AMD GPU,closed,2025-11-24 08:14:49,2025-11-25 06:46:10,bug
ollama/ollama,13223,Ollama Exposes the Local Model Directory via Network Requests in ollama/ollama,open,2025-11-24 04:22:44,,bug
ollama/ollama,13222,Add Atlas UI to community integrations,open,2025-11-24 02:38:41,,
ollama/ollama,13221,ü¶ô Stable build Ollama with latest bugs fixedü¶ô,closed,2025-11-24 00:02:01,2025-11-24 00:22:54,
ollama/ollama,13220,docs: fix link to modelfile.mdx,open,2025-11-23 20:31:06,,
ollama/ollama,13219,Request for 2 vision/document layout models,closed,2025-11-23 17:38:35,2025-11-25 01:35:26,
ollama/ollama,13218,Pulling heretic gpt-oss is broken,closed,2025-11-23 16:54:36,2025-11-23 21:08:03,bug
ollama/ollama,13217,Power adjustment during use,closed,2025-11-23 15:43:32,2025-11-25 01:37:58,
ollama/ollama,13216,cmd/bench: fix options table in cmd/bench/README.md,open,2025-11-23 15:42:07,,
ollama/ollama,13215,Gemma3n broken with Vulkan,open,2025-11-23 15:40:21,,"bug,vulkan"
ollama/ollama,13214,m4 macÊòæÁ§∫ÊúâÈóÆÈ¢ò,open,2025-11-23 14:21:24,,bug
ollama/ollama,13213,Add /api/models/size endpoint to report total storage usage,open,2025-11-23 14:18:15,,
ollama/ollama,13212,Vulkan is enabled by default and can't be disabled with OLLAMA_VULKAN=0,closed,2025-11-23 10:45:18,2025-11-23 11:10:39,bug
ollama/ollama,13211,Windows GUI - Version 0.13.0 some models (gemma3:27b) does't accept images,open,2025-11-23 08:50:33,,"bug,app"
ollama/ollama,13210,v0.13.0 release notes,closed,2025-11-23 08:18:49,2025-11-23 15:11:51,
ollama/ollama,13209,ollama ui does not launch on linux,closed,2025-11-23 06:47:18,2025-11-25 01:41:58,bug
ollama/ollama,13208,Cache Question,open,2025-11-23 03:26:10,,bug
ollama/ollama,13207,Discord invites aren't working,open,2025-11-22 19:04:39,,
ollama/ollama,13206,Structured output request don't work in Cloud's Qwen3-coder,open,2025-11-22 12:12:00,,bug
ollama/ollama,13205,ü¶ô Stable build Ollama with latest bugs fixedü¶ô,closed,2025-11-22 10:53:15,2025-11-22 11:06:58,
ollama/ollama,13204,"Local Ollama 3.3 stopped responding, I have 64 GB ram!",open,2025-11-22 08:02:54,,bug
ollama/ollama,13203,fix: prevent overwriting existing systemd service file,open,2025-11-22 06:47:59,,
ollama/ollama,13202,Low Priority - Link Error,closed,2025-11-22 03:42:25,2025-11-26 02:06:30,
ollama/ollama,13201,nomic-text-embed: set batchSize equal to contextLength ,open,2025-11-22 00:35:09,,
ollama/ollama,13200,Feature Proposal: GUI Support for Remote Ollama Instances,open,2025-11-21 23:13:20,,feature request
ollama/ollama,13199,ü¶ô Stable build Ollama with latest bugs fixedü¶ô,closed,2025-11-21 21:54:51,2025-11-21 23:03:32,
ollama/ollama,13198,deepseek-ocr via UI only answers if you query it twice,closed,2025-11-21 19:03:08,2025-11-21 19:20:41,bug
ollama/ollama,13197,Claude/audit mcp integration 016i r kd ztcf5 k1 wzq et l wr86,closed,2025-11-21 18:21:07,2025-11-21 19:20:38,
ollama/ollama,13196,amd: use GTT on iGPUs on linux,open,2025-11-21 17:57:21,,
ollama/ollama,13195,Computer Crashes When Running the Deepseek-OCR Model,open,2025-11-21 13:48:29,,bug
ollama/ollama,13194,Error: 500 Internal Server Error: unable to load model,closed,2025-11-21 12:54:35,2025-11-21 12:57:45,bug
ollama/ollama,13193,More DeepSeek-OCR quantizations,closed,2025-11-21 12:29:19,2025-11-21 13:43:05,
ollama/ollama,13192,fuckkkkkk ollama piece of shit !!!! fuckkkkkk this ollam autoupdate which cannot be disable and fuckkkkkk this piece of shit not support moe on gpu,closed,2025-11-21 12:27:53,2025-11-21 19:22:49,bug
ollama/ollama,13191,docs: add MindMap to community integrations,open,2025-11-21 12:13:05,,
ollama/ollama,13190,"Vulkan an, nur am loopen",open,2025-11-21 10:27:21,,"bug,vulkan"
ollama/ollama,13189,"Same prompt, inconsistent results based on ollama inference and direct inference",open,2025-11-21 09:23:01,,bug
ollama/ollama,13188,"AMD 780M GPU Acceleration Not Working with 2024.12 Driver, Works with 2025.11 (Ollama 0.13.0)",closed,2025-11-21 07:03:31,2025-11-21 21:52:19,"bug,amd"
ollama/ollama,13187,Custom Qwen3VLMoE models not working,open,2025-11-21 05:45:56,,bug
ollama/ollama,13186,discover: increase GPU discovery timeout when HSA_OVERRIDE_GFX_VERSION is set,open,2025-11-21 05:08:40,,
ollama/ollama,13185,docs: Fix typos,open,2025-11-21 04:58:35,,
ollama/ollama,13184,Formatted Outputs's Schemas,open,2025-11-21 01:57:38,,
ollama/ollama,13183,How to use reranker models in Ollama?,closed,2025-11-20 22:53:23,2025-11-21 21:46:24,feature request
ollama/ollama,13182,Deepseek v3 family parser,open,2025-11-20 22:42:39,,
ollama/ollama,13181,docs: clarify num_ctx parameter description,open,2025-11-20 22:18:58,,
ollama/ollama,13180,Deepseek v3 family renderer,open,2025-11-20 21:23:19,,
ollama/ollama,13179,app/ui: fix model capabilities not updating after download completion,open,2025-11-20 21:13:03,,
ollama/ollama,13178,Install script overwrites systemd config,open,2025-11-20 20:36:02,,bug
ollama/ollama,13177,deepseek-ocr :: Not able to run this model,closed,2025-11-20 20:04:17,2025-11-20 20:22:08,bug
ollama/ollama,13176,discovery: fix cuda overlap case,closed,2025-11-20 17:25:01,2025-11-20 20:15:38,
ollama/ollama,13175,Add support for reasoning_content in the OpenAI compliant API,open,2025-11-20 17:17:30,,feature request
ollama/ollama,13174,app/cmd: update ollama help to navigate to ollama doc,closed,2025-11-20 15:59:05,2025-11-20 21:30:35,
ollama/ollama,13173,New AMD memory detection routines ignores unified memory on AMD APU,open,2025-11-20 15:58:11,,"bug,amd"
ollama/ollama,13172,Version stuck at 0.13.0,closed,2025-11-20 15:12:16,2025-11-23 22:22:29,bug
ollama/ollama,13171,Error: 500 Internal Server Error: llama runner process has terminated: exit status 2,closed,2025-11-20 06:05:54,2025-11-21 01:36:32,bug
ollama/ollama,13170,Failed to concurrent call embed api,open,2025-11-20 06:02:45,,bug
ollama/ollama,13169,Ollama 0.13.0 docker fails with cuda on ARM,open,2025-11-20 05:04:12,,"bug,nvidia,docker"
ollama/ollama,13168,deepseek-ocrÊ®°ÂûãÂØπËØùÈáçÂ§ç,closed,2025-11-20 03:58:12,2025-11-26 05:55:39,bug
ollama/ollama,13167,download.go: no resumable download support ‚Äî failed transfers restart from zero,open,2025-11-20 02:56:08,,
ollama/ollama,13166,deepseek2: upgrade to run v3+ models,closed,2025-11-20 00:16:45,2025-11-20 01:05:39,
ollama/ollama,13165,Support overriding tensor-split from config,open,2025-11-20 00:08:34,,
ollama/ollama,13164,app: open app instead of always navigating to / on connect,closed,2025-11-19 23:00:10,2025-11-20 20:59:18,
ollama/ollama,13163,Ollama 0.12.11 Not Using GPU on RTX 5070 Ti (Blackwell/CC 12.0),closed,2025-11-19 20:59:14,2025-11-28 11:09:57,"bug,nvidia"
ollama/ollama,13162,nomic-embed-text:v2: model implementation ,open,2025-11-19 20:06:48,,
ollama/ollama,13161,app/ui: add gemini-3-pro-preview to featured list,open,2025-11-19 20:01:27,,
ollama/ollama,13160,kvcache: Run tests both with and without PermutedV,closed,2025-11-19 20:01:13,2025-11-20 00:45:30,
ollama/ollama,13159,app/ui: handle unspecified bind addresses and wait for server in ollama proxy,open,2025-11-19 18:58:35,,
ollama/ollama,13158,app/ui: handle unspecified bind addresses in ollama proxy,closed,2025-11-19 16:28:57,2025-11-19 17:36:43,
ollama/ollama,13157,which quantization ollama cloud models use,closed,2025-11-19 14:25:57,2025-11-28 10:34:34,feature request
ollama/ollama,13156,Vulkan Backhand on R5 3500u,closed,2025-11-19 12:05:50,2025-11-19 22:48:54,bug
ollama/ollama,13155,‰∏çÊîØÊåÅÊ∑ªÂä†api key,closed,2025-11-19 10:37:06,2025-11-19 11:08:13,bug
ollama/ollama,13154,`think` Parameter Not Suppressing Reasoning in qwen3:4b When Set to `False`,closed,2025-11-19 06:20:55,2025-11-19 07:58:54,bug
ollama/ollama,13153,Vulkan,open,2025-11-19 05:58:50,,"bug,performance,vulkan"
ollama/ollama,13152,GPT-OSS: 120B doesnt share between CPU/GPU @ CTX over 8192,open,2025-11-19 05:53:08,,"bug,needs more info"
ollama/ollama,13151,models: enable deepseek2 (deepseek v3.1 w/ MLA) on the new engine,closed,2025-11-19 05:52:56,2025-11-19 06:03:50,
ollama/ollama,13150,"""runtime error: invalid memory address or nil pointer dereference"" with Qwen3 VL",open,2025-11-19 05:01:58,,bug
ollama/ollama,13149,chore: mark vulkan shaders as vendored files,closed,2025-11-19 04:00:01,2025-11-19 20:01:23,
ollama/ollama,13148,Add ToolCalls to ChatResponse,closed,2025-11-19 03:40:59,2025-11-23 06:55:56,feature request
ollama/ollama,13147,Ollama add remote server support,open,2025-11-19 03:36:53,,
ollama/ollama,13146,The qwen2.5-vl:30b model encounters an error when running under Vulkan.,open,2025-11-19 03:12:31,,"bug,vulkan"
ollama/ollama,13145,Parser for Cogito v2,closed,2025-11-19 02:59:08,2025-11-20 01:21:07,
ollama/ollama,13144,nomic-embed: nomic-embed-text defaulted to ollama runner ,closed,2025-11-19 02:46:03,2025-11-19 21:03:45,
ollama/ollama,13143,Deepseekv2 models default to new engine,closed,2025-11-19 02:22:24,2025-11-19 19:19:38,
ollama/ollama,13142,feat: add API¬†endpoint to cancel/abort ongoing model downloads,open,2025-11-19 02:22:04,,feature request
ollama/ollama,13141,kvcache: Use SetRows to store cache data,closed,2025-11-19 00:21:00,2025-11-19 04:42:28,
ollama/ollama,13140,refactor rope,open,2025-11-19 00:17:26,,
ollama/ollama,13139,Renderer for Cogito,closed,2025-11-18 23:28:11,2025-11-19 03:06:34,
ollama/ollama,13138,win: exit instead of abort,closed,2025-11-18 22:00:33,2025-11-19 00:33:33,
ollama/ollama,13137,app/ui: use requestAnimationFrame to prevent bottom line cutoff in streaming thinking display,open,2025-11-18 21:55:32,,
ollama/ollama,13136,python not detecting ollama,closed,2025-11-18 20:28:12,2025-11-20 21:06:42,bug
ollama/ollama,13135,Ready for team review,open,2025-11-18 19:24:50,,
ollama/ollama,13134,test:  fix data race in unit tests,open,2025-11-18 16:33:23,,
ollama/ollama,13133,KUHUL OS AI,closed,2025-11-18 15:05:48,2025-11-19 20:16:21,
ollama/ollama,13132,pull model manifest: 500,closed,2025-11-18 14:24:08,2025-11-18 14:27:34,
ollama/ollama,13131,Add H'uhul Multi Hive OS - Ollama-powered multi-agent system,closed,2025-11-18 14:21:19,2025-11-19 20:13:30,
ollama/ollama,13130,Documentation about Vulkan enablement in docker for Intel iGPU/GPU missing?,open,2025-11-18 12:40:08,,"bug,docker,gpu,vulkan"
ollama/ollama,13129,Ollama unable to pull model,closed,2025-11-18 12:03:07,2025-11-18 18:04:53,bug
ollama/ollama,13128,ollamaËøîÂõûÁ±ªÂûãÊòØapplication/x-ndjsonÔºå‰∏çÊîØÊåÅÊµÅÂºè‰º†Ëæì,closed,2025-11-18 11:26:40,2025-11-18 11:40:48,
ollama/ollama,13127,Vulkan - macOS AMD GPU,open,2025-11-18 10:05:18,,"feature request,macos,vulkan"
ollama/ollama,13126,Ollama cannot properly use the GPU on a dual-GPU laptop after enabling Vulkan,open,2025-11-18 05:57:25,,"bug,vulkan"
ollama/ollama,13125,ollamaÈÉ®ÁΩ≤ÁöÑqwen3-vlËßÜËßâÊ®°ÂûãÔºåË∞ÉÁî®apiÂÅöËßÜÈ¢ëÁêÜËß£,closed,2025-11-18 03:29:24,2025-11-19 02:26:06,
ollama/ollama,13124,docs: add Void Editor to community integrations,closed,2025-11-17 23:35:13,2025-11-18 03:20:36,
ollama/ollama,13123,ci: fix missing vulkan binaries in linux bundles,closed,2025-11-17 23:29:05,2025-11-17 23:39:59,
ollama/ollama,13122,env: expose user visible num_thread override,open,2025-11-17 23:24:20,,
ollama/ollama,13121,chore: run the linter,open,2025-11-17 23:11:06,,
ollama/ollama,13120,app/cmd: restrict ollama:// URL scheme to supported paths,closed,2025-11-17 22:37:05,2025-11-18 01:10:45,
ollama/ollama,13119,Persistent 500 Internal Server Error trying to operate deepseek-r1:70b on Windows,closed,2025-11-17 21:57:59,2025-11-18 05:19:50,bug
ollama/ollama,13118,qwen3-vl failed to offloaded layer to Radeon iGPU on AI MAX 395 on windows,closed,2025-11-17 13:17:39,2025-11-18 01:15:59,bug
ollama/ollama,13117,Ollamaa,closed,2025-11-17 10:52:15,2025-11-17 19:43:39,
ollama/ollama,13116,docs: fix typo,closed,2025-11-17 10:01:29,2025-11-18 21:18:42,
ollama/ollama,13115,docs: fix typo,closed,2025-11-17 09:59:24,2025-11-17 09:59:47,
ollama/ollama,13114,How to build on Asend 910B,open,2025-11-17 09:08:10,,feature request
ollama/ollama,13113,The qwen3-vl model reports an error after uploading a small image,closed,2025-11-17 07:48:10,2025-11-18 02:43:50,bug
ollama/ollama,13112,0.12.11 much slower on gpt-oss:20b than 0.12.10,closed,2025-11-17 01:46:54,2025-11-19 04:42:29,"bug,performance,nvidia"
ollama/ollama,13111,OpenAI API compatibility layer seems not to support `n` (i.e. multi-choice output),open,2025-11-16 23:01:01,,feature request
ollama/ollama,13110,docs: don't hardcode list of cloud models,closed,2025-11-16 21:02:19,2025-11-17 04:56:09,
ollama/ollama,13109,migrate to golangci-lint v2,closed,2025-11-16 20:09:38,2025-11-18 19:00:26,
ollama/ollama,13108,vulkan lobotomizes models,open,2025-11-16 18:59:49,,"bug,vulkan"
ollama/ollama,13107,AIMAX395 with ROCM 7.0.2 have a wrong VRAM size.,open,2025-11-16 12:59:13,,bug
ollama/ollama,13106,AsyncClient().chat is not determinable,open,2025-11-16 11:34:32,,bug
ollama/ollama,13105,Ollama app's chat interface does not list all the Ollama models I have downloaded!,open,2025-11-16 11:30:34,,"bug,app"
ollama/ollama,13104,Linux tarballs for 0.12.11 don't contain Vulkan libraries.,closed,2025-11-16 00:19:57,2025-11-17 23:40:00,"bug,vulkan"
ollama/ollama,13103,"ollama with vulkan specified, does not load to GPU",open,2025-11-15 22:44:47,,"bug,linux,intel,vulkan"
ollama/ollama,13102,REMOVE THE TRANSACTION,closed,2025-11-15 21:25:58,2025-11-15 23:09:28,feature request
ollama/ollama,13101,ollama run qwen3-vl-4b-instruct error,closed,2025-11-15 16:48:18,2025-11-20 14:22:27,
ollama/ollama,13100,Allow usage of the already-installed llama.cpp when building,open,2025-11-15 16:32:35,,feature request
ollama/ollama,13099,qwen3-vl is not accepting images for ocr with latest version 0.12.11,closed,2025-11-15 13:23:26,2025-11-20 14:33:25,bug
ollama/ollama,13098,Allow Model Registry to be used with Ollama API KEY,open,2025-11-15 11:39:44,,feature request
ollama/ollama,13097,panic: failed to sample token with Vulkan,open,2025-11-15 09:24:50,,"bug,vulkan"
ollama/ollama,13096,Fix a typo in `runner.go`,closed,2025-11-15 08:53:20,2025-11-16 02:52:54,
ollama/ollama,13095,error 500 not enough system memory. Truenas 25.10.0,open,2025-11-15 07:36:19,,bug
ollama/ollama,13094,The bottom line of the real-time rendering by thinking cannot be displayed,open,2025-11-15 04:12:15,,"bug,app"
ollama/ollama,13093,"It seems Qwen models do not ""see"" tools when run thro goose",open,2025-11-15 01:25:08,,bug
ollama/ollama,13092,logprobs does not contain tool call information,open,2025-11-15 00:05:51,,bug
ollama/ollama,13091,fix(tokenizer): add special tokens to empty inputs,closed,2025-11-14 22:38:38,2025-11-18 19:16:56,
ollama/ollama,13090,docs: add logprobs to openapi,closed,2025-11-14 22:05:03,2025-11-14 22:14:58,
ollama/ollama,13089,"New and FRUSTRATING very limiting max tokens on the cloud models to only 16,384",closed,2025-11-14 18:29:39,2025-11-14 21:10:00,"bug,cloud"
ollama/ollama,13088,log: warn if user overrides detected,closed,2025-11-14 16:53:27,2025-11-14 22:36:29,
ollama/ollama,13087,Add Support for Distributed Inferencing (continued on AMD strix halo),open,2025-11-14 15:47:42,,
ollama/ollama,13086,Vulkan on intel iGPU results in gibberish,closed,2025-11-14 13:06:26,2025-11-15 21:15:10,"bug,vulkan"
ollama/ollama,13085,Is AMD's AI Pro R9700 supported?,open,2025-11-14 13:03:57,,bug
ollama/ollama,13084,Qwen3-VL produces garbled output when image inputs exceed num_ctx in multi-turn conversations,open,2025-11-14 08:48:19,,bug
ollama/ollama,13083,GPU utilization issue,closed,2025-11-14 02:33:43,2025-11-21 00:32:31,
ollama/ollama,13082,feat(model): deepseekocr,closed,2025-11-13 21:24:20,2025-11-19 00:11:37,
ollama/ollama,13081,app/ui: refactor to use Ollama endpoints for user auth and health checks,open,2025-11-13 20:21:14,,
ollama/ollama,13080,How to compute hash for model download verification ?,closed,2025-11-13 16:49:53,2025-11-17 09:18:02,
ollama/ollama,13079,app/ui: fix to point ollama client to ui backend in dev mode,closed,2025-11-13 16:35:31,2025-11-17 17:58:35,
ollama/ollama,13078,Fix function name and improve test case readability,closed,2025-11-13 15:47:08,2025-11-16 02:54:28,
ollama/ollama,13077,sched: context length reuse,open,2025-11-13 13:23:52,,
ollama/ollama,13076,tools: add tool call id to qwen3coder parser,closed,2025-11-13 09:38:58,2025-11-13 09:51:43,
ollama/ollama,13075,Ollama will not build against CUDA later than 12.6 (fixed),closed,2025-11-13 08:39:32,2025-11-16 13:29:03,bug
ollama/ollama,13074,Ubuntu 400 bug issue,closed,2025-11-13 07:40:09,2025-11-23 20:00:31,"bug,needs more info"
ollama/ollama,13073,docs: add Ollama App iOS to community integrations,open,2025-11-13 06:12:23,,
ollama/ollama,13072,docs: fix typo (VSCode -> VS Code),closed,2025-11-13 04:45:44,2025-11-13 04:49:33,
ollama/ollama,13071,nomic-embed-text model implementation ,closed,2025-11-13 03:49:15,2025-11-19 02:28:10,
ollama/ollama,13070,Failure during GPU discovery. Causing system-wide crashed 6700 xt,open,2025-11-13 03:26:22,,"bug,linux,amd"
ollama/ollama,13069,"0.12.10 uses only the CPU, while 0.9.6 uses the GPU.",closed,2025-11-13 02:56:49,2025-11-13 06:24:05,bug
ollama/ollama,13068,logprob: add bytes to logprobs,closed,2025-11-13 02:06:51,2025-11-13 21:49:25,
ollama/ollama,13067,app: remove source code for previous JavaScript-based macOS app,closed,2025-11-13 01:34:35,2025-11-13 04:37:43,
ollama/ollama,13066,"After upgrading Ollama from version 0.12.1 to 0.12.10, the PotPlayer subtitle translation plugin request timed out with status code 500.",open,2025-11-13 01:03:23,,bug
ollama/ollama,13065,Fix incorrect header for community integrations,closed,2025-11-13 00:51:52,2025-11-13 01:00:16,
ollama/ollama,13064,MacOS: Dark theme dock icon,open,2025-11-13 00:10:36,,"feature request,app"
ollama/ollama,13063,Add deepseek v3.1,closed,2025-11-12 19:33:10,2025-11-18 02:03:21,
ollama/ollama,13062,ci: fix win vulkan,closed,2025-11-12 18:31:33,2025-11-12 18:32:24,
ollama/ollama,13061,Ollama Linux with Multiple AMD GPUs Fails to Use Any AMD GPU Since 0.12.6 --AMD gfx1030 AMD 6650 GPU,closed,2025-11-12 18:15:18,2025-11-14 16:36:37,"bug,linux,amd"
ollama/ollama,13060,Panic: failed to sample token.,closed,2025-11-12 15:50:24,2025-11-13 15:36:13,bug
ollama/ollama,13059,Fix single directory selector,open,2025-11-12 11:54:55,,
ollama/ollama,13058,chore: fix incorrect function name in comment,open,2025-11-12 11:25:06,,
ollama/ollama,13057,integration: improve tool calling tests,open,2025-11-12 01:18:19,,
ollama/ollama,13056,Rename api-reference.md back to api.md since redirect stopped working,closed,2025-11-11 23:41:29,2025-11-11 23:53:06,
ollama/ollama,13055,docs/openapi: document that delete and copy responses are empty,closed,2025-11-11 23:04:33,2025-11-11 23:07:22,
ollama/ollama,13054,Ollama 0.12.10 embedding crash (nomic-embed-text-v1.5 on macOS),open,2025-11-11 21:12:41,,"bug,macos,embeddings"
ollama/ollama,13053,fix tensor merge,closed,2025-11-11 18:46:53,2025-11-13 23:32:34,
ollama/ollama,13052,flash attn: add auto mode for llama engine,open,2025-11-11 17:56:55,,
ollama/ollama,13051,GPT-oss-120b KV cache defragmentation.,closed,2025-11-11 15:15:10,2025-11-11 16:54:27,bug
ollama/ollama,13050,Ollama 0.12.10 fails to find CUDA compiler (fixed with work around),closed,2025-11-11 15:14:21,2025-11-16 13:28:45,"bug,build"
ollama/ollama,13049,When will minimax-m2 be supported?,open,2025-11-11 12:55:12,,"bug,model"
ollama/ollama,13048,Error 500 Internal Server Error: unmarshal: invalid character 'I' looking for beginning of value.,open,2025-11-11 10:33:14,,bug
ollama/ollama,13047,Does qwen3-vl multimodal support transferring multiple images?,closed,2025-11-11 08:48:10,2025-11-12 00:11:14,
ollama/ollama,13046,docs: add comprehensive 12-phase roadmap for advanced features,closed,2025-11-11 07:08:10,2025-11-11 19:07:30,
ollama/ollama,13045,docs: fix metal gpu section header,closed,2025-11-11 05:35:40,2025-11-11 05:51:22,
ollama/ollama,13044,qwen3-vl:30b-a3b-instruct-q8_0  Image recognition error,closed,2025-11-11 04:02:57,2025-11-12 14:10:39,"bug,needs more info"
ollama/ollama,13043,qwen3:30b  Severe bug found,closed,2025-11-11 02:21:04,2025-11-23 19:58:11,"bug,needs more info"
ollama/ollama,13042,Prefer dedicated GPUs over iGPUs when offloading,closed,2025-11-11 00:58:01,2025-11-11 21:11:08,
ollama/ollama,13041,app/ui: do not send thinking to prevent  errors with cloud provider,closed,2025-11-11 00:09:27,2025-11-11 21:09:24,
ollama/ollama,13040,"Ollama hangs in infinite loop during code update requests, requires service restart",closed,2025-11-10 18:27:30,2025-11-25 19:23:26,bug
ollama/ollama,13039,The Docker image tag `ollama:latest` does not point to the latest release.,closed,2025-11-10 17:40:37,2025-11-11 01:14:20,"bug,docker"
ollama/ollama,13038,TOON Format,open,2025-11-10 14:26:36,,feature request
ollama/ollama,13037,`ollama run` in Windows 0.12.4+ doesn't start the server.,open,2025-11-10 14:21:29,,"bug,windows"
ollama/ollama,13036,`ollama run` doesn't start the ollama server in Windows 0.12.4+,closed,2025-11-10 14:19:14,2025-11-10 14:30:11,bug
ollama/ollama,13035,docs: add community integration to README,closed,2025-11-10 10:07:00,2025-11-13 01:08:51,
ollama/ollama,13034,Partial download handling,open,2025-11-10 06:03:29,,feature request
ollama/ollama,13033,Issue: Ollama 0.12.10 fails on NVIDIA Jetson Thor (Regression from 0.12.9),closed,2025-11-10 05:59:04,2025-11-20 20:15:39,"bug,nvidia"
ollama/ollama,13032,"Error: 500 Internal Server Error: do load request: Post ""http://127.0.0.1:11680/load"": read tcp 127.0.0.1:11685->127.0.0.1:11680: wsarecv: An existing connection was forcibly closed by the remote host.",closed,2025-11-10 00:56:28,2025-11-23 19:49:48,bug
ollama/ollama,13031,Broken example in api.md,open,2025-11-09 22:31:56,,bug
ollama/ollama,13030,unmarshal: invalid character,closed,2025-11-09 22:00:28,2025-11-09 22:09:39,bug
ollama/ollama,13029,Vulkan fails to allocate memory buffer,open,2025-11-09 15:28:28,,"bug,windows,amd,vulkan"
ollama/ollama,13028,Kimi-k2-thinking:cloud model got some usage restrictions,closed,2025-11-09 14:52:16,2025-11-09 16:12:05,
ollama/ollama,13027,500 Internal Server Error:,closed,2025-11-09 12:00:12,2025-11-23 19:57:09,bug
ollama/ollama,13026,Qwen3-Coder:480B-Cloud,closed,2025-11-09 10:01:18,2025-11-09 22:09:21,bug
ollama/ollama,13025,Apertus-70B-Instruct-2509 Full GPU layer allocation fails on multi-GPU setup works only when at least one layer is offloaded,open,2025-11-09 04:25:31,,bug
ollama/ollama,13024,after selecting Deepseek-V3.1 in the ollama app - it will NOT allow you to change to a different model.,open,2025-11-09 03:30:13,,bug
ollama/ollama,13023,Intel Iris Xe Graphics (16GB) not detected by Ollama v0.12.10 on Windows 11 despite Vulkan/DXGI+PDH support,open,2025-11-09 02:05:53,,"bug,needs more info"
ollama/ollama,13022,change upgrade,closed,2025-11-09 01:03:32,2025-11-09 02:41:26,feature request
ollama/ollama,13021,best models are only cloud ?,closed,2025-11-08 23:48:49,2025-11-18 01:24:33,
ollama/ollama,13020,Embeddings broken in recent Ollama versions - returning empty embedding.,closed,2025-11-08 22:18:02,2025-11-09 02:40:46,bug
ollama/ollama,13019,Kimi k2 thinking,open,2025-11-08 20:24:21,,
ollama/ollama,13018,too much available memory reported,open,2025-11-08 18:56:40,,"bug,amd,gpu"
ollama/ollama,13017,Bug: 500 Internal Server Error: unmarshal: invalid character 'I' with 'kimi-k2-thinking:cloud' model,closed,2025-11-08 13:04:52,2025-11-10 02:36:37,bug
ollama/ollama,13016,cloud model,closed,2025-11-08 11:50:32,2025-11-10 00:49:25,bug
ollama/ollama,13015,Ollama 0.12.10 Error: 500 Internal Server Error: do load request: Post EOF,closed,2025-11-08 10:07:50,2025-11-23 19:52:57,bug
ollama/ollama,13014,Overfloew to swap file,open,2025-11-08 06:23:02,,feature request
ollama/ollama,13013,Ollama not compatible with AMD GPUs when ZLUDA installed,open,2025-11-08 03:54:09,,"feature request,windows,amd"
ollama/ollama,13012,3c1e4af85a036bc49a9237f5f92837e64b2708b2,closed,2025-11-08 03:31:57,2025-11-08 18:46:41,
ollama/ollama,13011,"Ollama 0.12.10: After starting the service, my computer hangs when trying to suspend",closed,2025-11-08 02:16:01,2025-11-28 16:12:45,bug
ollama/ollama,13010,bugfix: don't include both consolidated.safetensors and model-*.safetensors,closed,2025-11-08 01:48:41,2025-11-08 06:41:58,
ollama/ollama,13009,"500 Internal Server Error when using deepseek-v3:cloud ‚Äî ""invalid character 'I' looking for beginning of value""",closed,2025-11-08 01:37:00,2025-11-08 02:27:49,bug
ollama/ollama,13008,ollama loads nearly the entire model on CPU after unloading from GPU,open,2025-11-08 00:21:36,,bug
ollama/ollama,13007,api/client: handle non-json streaming errors,open,2025-11-08 00:16:44,,
ollama/ollama,13006,app/docs: remove out of date storybook instructions,closed,2025-11-08 00:16:26,2025-11-08 21:28:19,
ollama/ollama,13005,Per GPU settings,open,2025-11-07 22:50:47,,feature request
ollama/ollama,13004,api: add per-embedding performance metrics,open,2025-11-07 19:56:09,,
ollama/ollama,13003,Ollam 0.12.10 Qwen3-VL:235b-instruct-cloud can't handle more than 5000 Words in German,open,2025-11-07 19:38:13,,bug
ollama/ollama,13002,Ollama sometimes using CPU instead of GPU,open,2025-11-07 19:03:24,,"bug,linux,amd"
ollama/ollama,13001,CORS  Error,closed,2025-11-07 13:44:15,2025-11-08 22:04:48,bug
ollama/ollama,13000,Add ROCm 7 support,open,2025-11-07 12:28:45,,
ollama/ollama,12999,ËøêË°åÊ®°ÂûãÊó†Ê≥ïÂú®GPU‰∏äËøêË°å,closed,2025-11-07 08:50:57,2025-11-10 03:23:20,bug
ollama/ollama,12998,Ollama 0.12.10 Windows: 500 Internal Server Error With `Qwen3-VL` Models Set to 256k Context Size in Ollama App,open,2025-11-07 07:30:05,,bug
ollama/ollama,12997,Website ollama.com is DOWN,closed,2025-11-07 07:21:25,2025-11-07 07:26:15,
ollama/ollama,12996,"modelfile.md has been renamed into modelfile.mdx, fix 404.",closed,2025-11-07 06:19:08,2025-11-07 18:06:46,
ollama/ollama,12995,server: Edit manifest documentation,closed,2025-11-07 03:37:13,2025-11-16 03:13:15,
ollama/ollama,12994,docs: update n8n URL for Ollama,closed,2025-11-07 02:54:13,2025-11-08 04:07:26,
ollama/ollama,12993,embeddings: added cli command to embedding docs ,closed,2025-11-07 02:17:09,2025-11-13 21:24:13,
ollama/ollama,12992,ggml update to b7108,open,2025-11-07 00:56:29,,
ollama/ollama,12991,Add OllamaGUI to Web & Desktop integrations.,open,2025-11-06 22:29:43,,
ollama/ollama,12990,Ollama prevents system shutdown on OS X 26.1,closed,2025-11-06 21:33:56,2025-11-06 21:54:35,bug
ollama/ollama,12989,api: add omitempty to required tool function parameter type,closed,2025-11-06 20:59:39,2025-11-06 22:08:55,
ollama/ollama,12988,openai: fix tool call mapping,closed,2025-11-06 18:56:26,2025-11-06 23:26:25,
ollama/ollama,12987,Add Node.js CI workflow,open,2025-11-06 13:11:45,,
ollama/ollama,12986,Error: pull model manifest: file does not exist,closed,2025-11-06 12:18:07,2025-11-06 23:14:35,bug
ollama/ollama,12985,Server: Fix duplicate 'is' typo,closed,2025-11-06 07:23:07,2025-11-06 22:44:44,
ollama/ollama,12984,llama_model_load: error loading model: error loading model architecture: unknown model architecture: 'pangu-embedded',open,2025-11-06 06:32:26,,bug
ollama/ollama,12983,app/ui: prevent sidebar animation on initial load patch,open,2025-11-06 06:29:22,,
ollama/ollama,12982,Ollama 0.12.9 Windows - 500: llama runner process has terminated: cudaMalloc failed: out of memory - granite4:small-h,open,2025-11-06 03:18:51,,bug
ollama/ollama,12981,Add Proxy and security tools section to README,closed,2025-11-06 02:21:52,2025-11-06 23:21:14,
ollama/ollama,12980,convert: add deepseek converter,open,2025-11-06 01:24:33,,
ollama/ollama,12979,app/ui: prevent sidebar animation on initial load,closed,2025-11-06 01:09:50,2025-11-06 06:32:25,
ollama/ollama,12978,"Can't make ""ollama create"" follow Modelfile instructions",closed,2025-11-05 23:38:01,2025-11-05 23:41:48,
ollama/ollama,12977,Ollama 0.12.9 Error: 500 Internal Server Error: do load request: Post EOF,closed,2025-11-05 22:36:35,2025-11-09 16:09:11,"bug,nvidia,needs more info"
ollama/ollama,12976,Performance Regression on Apple Silicon M1: GPU ‚Üí CPU Fallback in v0.12.9 (works correctly in v0.12.5),open,2025-11-05 21:47:56,,"bug,performance,macos,needs more info"
ollama/ollama,12975,doc: re-add login autostart faq and GPU updates,closed,2025-11-05 21:09:11,2025-11-07 19:21:45,
ollama/ollama,12974,ci: re-enable signing,closed,2025-11-05 20:31:33,2025-11-05 20:33:01,
ollama/ollama,12973,feat: add support for WebP images in Ollama's app,closed,2025-11-05 20:29:17,2025-11-06 21:31:46,
ollama/ollama,12972,mac: fix stale VRAM data,closed,2025-11-05 19:45:19,2025-11-05 19:55:17,
ollama/ollama,12971,vulkan: cherry-pick upstream changes,closed,2025-11-05 18:48:09,2025-11-12 16:31:40,
ollama/ollama,12970,parser: add single quote support to unquote function,open,2025-11-05 17:56:03,,
ollama/ollama,12969,win: revert CPU discovery logic to 0.12.3,closed,2025-11-05 16:37:53,2025-11-05 18:32:38,
ollama/ollama,12968,"Does not support Intel integrated graphics, please add support",closed,2025-11-05 12:39:49,2025-11-05 12:42:57,bug
ollama/ollama,12967,"Does not support Intel integrated graphics, please add support",open,2025-11-05 12:28:50,,"bug,intel"
ollama/ollama,12966,Model runs forever on Windows even with minimal model smollm:135m,closed,2025-11-05 10:28:30,2025-11-05 22:18:10,bug
ollama/ollama,12965,fix: MacOS file picker extensions broken,closed,2025-11-05 08:17:58,2025-11-06 05:37:17,
ollama/ollama,12964,tests: basic benchmarking test framework,closed,2025-11-05 07:19:39,2025-11-16 02:17:41,
ollama/ollama,12963,Data Race Conditions in Scheduler Tests,open,2025-11-05 06:06:38,,bug
ollama/ollama,12962,feat: Add Support for Qwen3-vl and Qwen2.5-vl Video Mode,open,2025-11-05 01:40:59,,
ollama/ollama,12961,log: trace logging for scheduler,closed,2025-11-05 00:56:02,2025-11-05 16:12:15,
ollama/ollama,12960,log: instrument CPU discovery timing,closed,2025-11-05 00:03:53,2025-11-05 00:23:37,
ollama/ollama,12959,app: slow rendering when opening and closing sidebar with code,open,2025-11-05 00:01:44,,"bug,app"
ollama/ollama,12958,app: conversation sorting is incorrect,open,2025-11-04 23:45:49,,"bug,app"
ollama/ollama,12957,app: Ollama.app does not respond to some shutdown requests,open,2025-11-04 23:37:36,,"bug,app,macos"
ollama/ollama,12956,Add Tool Call ID,closed,2025-11-04 23:37:09,2025-11-05 00:43:33,
ollama/ollama,12955,"app: The application ""Ollama"" can't be opened (-47)",open,2025-11-04 23:35:46,,"bug,app"
ollama/ollama,12954,app: sidebar animates opening on load instead of appearing open,open,2025-11-04 23:34:27,,"bug,good first issue,app"
ollama/ollama,12953,app: pagination / lazy loading for listing past chats,open,2025-11-04 23:32:37,,"bug,app"
ollama/ollama,12952,app: file validation should re-run when selected model is changed,open,2025-11-04 23:31:59,,"bug,app"
ollama/ollama,12951,app: WebP images are reportedly not supported in Ollama's app,closed,2025-11-04 23:30:26,2025-11-06 21:31:47,"bug,good first issue,app"
ollama/ollama,12950,app: vision models claim to not support images for before downloading and 30s after,open,2025-11-04 23:29:49,,"bug,app"
ollama/ollama,12949,ui: using streamdown AI elements for markdown rendering,closed,2025-11-04 20:24:47,2025-11-10 17:05:59,
ollama/ollama,12948,Bug: OpenVINO build on Ubuntu 25.04 fails to detect Intel Arc GPU (Core Ultra 9 285H) despite successful driver installation (clinfo OK),closed,2025-11-04 16:23:52,2025-11-04 23:42:05,bug
ollama/ollama,12947,ggml-cpu: Enable build with power10 mma optimisations,open,2025-11-04 14:23:45,,
ollama/ollama,12946,ggml-cpu: Resolves a bug on power,open,2025-11-04 14:20:51,,
ollama/ollama,12945,Wiki addendum:  Grabbing AMD GPU Ids via rocminfo for GPU isolation / restricting ollama GPUs via ROCR_VISIBLE_DEVICES or ROCM_VISIBLE_DEVICES (ROCM),open,2025-11-04 11:20:54,,feature request
ollama/ollama,12944,Qwen3 VL unknown architecture on v0.12.9,closed,2025-11-04 09:14:42,2025-11-04 23:43:01,bug
ollama/ollama,12943,Custom object for openai tool call with index pointers,closed,2025-11-04 08:23:52,2025-11-04 21:04:18,
ollama/ollama,12942,"Ollama suddenly became unresponsive, reinstall did not help",closed,2025-11-04 08:19:42,2025-11-05 18:32:40,"bug,windows,nvidia"
ollama/ollama,12941,Ollama uses 40GB RAM in qwen3-4b-instruct,closed,2025-11-04 06:56:22,2025-11-05 22:20:37,bug
ollama/ollama,12940,Ollama Runner Fails with ‚ÄúExit Status 2‚Äù and Random Non-Responsive Behavior on Windows,open,2025-11-04 04:24:26,,bug
ollama/ollama,12939,ollama rocm > v0.12.6-rc0 (docker; 18d) crashes gfx1030 GPU and entire Linux system!,open,2025-11-04 02:36:21,,"bug,linux,amd"
ollama/ollama,12938,embedding failed with BGE-m3,open,2025-11-04 01:25:47,,"bug,needs more info"
ollama/ollama,12937,vulkan: enable flash attention,closed,2025-11-04 01:06:25,2025-11-04 18:31:23,
ollama/ollama,12936,Menu Organization,open,2025-11-04 00:19:22,,"feature request,app"
ollama/ollama,12935,ggml: Increase maximum graph size,closed,2025-11-03 23:19:12,2025-11-04 00:05:37,
ollama/ollama,12934,chore: update models to use slice/chunk/chunksections,closed,2025-11-03 23:06:54,2025-11-13 23:20:12,
ollama/ollama,12933,app: add code for macOS and Windows apps under 'app',closed,2025-11-03 21:20:23,2025-11-04 19:40:17,
ollama/ollama,12932,chore(gptoss): cleanup dead code,closed,2025-11-03 19:17:31,2025-11-03 19:27:15,
ollama/ollama,12931,Enable Vulkan with a temporary opt-in setting,closed,2025-11-03 19:12:06,2025-11-12 16:40:38,
ollama/ollama,12930,bugfix: show connection string for interactive cli usage,closed,2025-11-03 18:41:26,2025-11-05 19:55:04,
ollama/ollama,12929,Added Hillnote to the community integrations,closed,2025-11-03 17:59:12,2025-11-03 20:55:04,
ollama/ollama,12928,Flash attention not working when using vulkan,closed,2025-11-03 17:50:27,2025-11-04 18:31:24,bug
ollama/ollama,12927,Modelfile PARAMETER num_ctx is ignored when lower than model's native context length,closed,2025-11-03 16:29:02,2025-11-03 20:40:47,
ollama/ollama,12926,qwen3 vl video as input support,open,2025-11-03 13:27:38,,feature request
ollama/ollama,12925,Windows System Tray Icon Not Right-clickable,open,2025-11-03 12:57:32,,"bug,app"
ollama/ollama,12924,qwen3-embedding:0.6b,closed,2025-11-03 12:27:33,2025-11-03 17:02:43,bug
ollama/ollama,12923,Erro to run BitNet 1.58 model in Ollama,closed,2025-11-03 11:58:25,2025-11-04 01:02:28,bug
ollama/ollama,12922,0.12.9 [Manjaro] Ollama isn't seeing recovered VRAM when switching models,closed,2025-11-03 10:11:25,2025-11-07 17:51:59,"bug,linux,amd"
ollama/ollama,12921,Ollama qwen3-embedding:8b-fp16 embedding failure,open,2025-11-03 09:18:01,,bug
ollama/ollama,12920,0.12.7ÁâàÊú¨ËøêË°åqwen3vl-8bÊ®°ÂûãÂç°Ê≠ªËøõÁ®ã,closed,2025-11-03 08:29:38,2025-11-13 11:43:42,"bug,needs more info"
ollama/ollama,12919,Add Void Editor to Ollama Integration Documentation,closed,2025-11-03 08:20:36,2025-11-18 03:20:37,"documentation,feature request"
ollama/ollama,12918,"Cannot communicate with qwen3-vl but qwen2.5-vl works well, i use the sdk  which is  ""/api/chat""",open,2025-11-03 07:47:20,,"bug,needs more info"
ollama/ollama,12917,qwen3:4b: Can't turn off thinking,closed,2025-11-03 05:22:12,2025-11-03 05:33:09,bug
ollama/ollama,12916,The auto-start Ollama service cannot output in parallel.,open,2025-11-03 03:15:06,,bug
ollama/ollama,12915,Runner crashes checking AMD MI50 GPU,closed,2025-11-02 23:02:17,2025-11-04 17:02:58,"bug,amd,gpu"
ollama/ollama,12914,server: add ThinkLevel capability,open,2025-11-02 17:42:00,,
ollama/ollama,12913,Vulkan backend performance is slow (7.73 tokens/s),closed,2025-11-02 17:17:32,2025-11-03 23:27:58,bug
ollama/ollama,12912,Something changed after 0.12.6 in memory management in a not good way,closed,2025-11-02 12:30:32,2025-11-05 19:55:18,"bug,macos,memory"
ollama/ollama,12911,Problem witch loading models in vram,closed,2025-11-02 10:16:04,2025-11-04 01:07:37,
ollama/ollama,12910,[BUG] The qwen3-vl:30b-a3b-instruct model cannot output in parallel,closed,2025-11-02 09:09:51,2025-11-02 09:47:01,bug
ollama/ollama,12909,Add bash-completions support,open,2025-11-02 07:40:24,,feature request
ollama/ollama,12908,"ROCM Library not found, wrong location? - gfx1201 unsupported in current container image",open,2025-11-02 01:44:33,,"bug,amd,docker"
ollama/ollama,12907,"""/set nothink"" not disabling reasoning output in Qwen3:4B",closed,2025-11-02 01:24:28,2025-11-02 23:48:11,bug
ollama/ollama,12906,qwen3-vl:30b think: false not working,closed,2025-11-01 22:49:57,2025-11-01 23:41:54,bug
ollama/ollama,12905,Windows - Connection attemt failed,closed,2025-11-01 21:38:13,2025-11-13 11:42:33,bug
ollama/ollama,12904,docs: fix some of the openapi.yaml warnings,closed,2025-11-01 19:30:21,2025-11-11 23:39:35,
ollama/ollama,12903,Unable to use my AMD GPU with ollama on Ubuntu,closed,2025-11-01 19:28:50,2025-11-13 20:54:52,"bug,amd,needs more info,gpu"
ollama/ollama,12902,macOS - Cannot use models outside of default directory in 0.12.9 release,closed,2025-11-01 16:38:40,2025-11-01 23:46:08,"bug,app"
ollama/ollama,12901,Add gpu-2404 interface for ollama snap,closed,2025-11-01 14:27:52,2025-11-01 14:34:27,feature request
ollama/ollama,12900,MacOS Ollama Memory Hang,closed,2025-11-01 13:56:19,2025-11-07 19:23:16,"bug,memory"
ollama/ollama,12899,Allow returning logprobs,closed,2025-11-01 12:57:09,2025-11-11 16:49:50,
ollama/ollama,12898,ollama ui for linux,open,2025-11-01 11:28:39,,"feature request,app"
ollama/ollama,12897,ollama run modelscope.cn/unsloth/Qwen3-VL-8B-Instruct-GGUF  failed,open,2025-11-01 02:19:43,,bug
ollama/ollama,12896,HF Qwen3VL (&MoE) didn't work on Ollama,open,2025-11-01 00:53:56,,bug
ollama/ollama,12895,Disabling cuda devices breaks other discreet GPU discovery.,open,2025-10-31 23:49:23,,"bug,gpu"
ollama/ollama,12894,discovery: only retry AMD GPUs,closed,2025-10-31 23:35:25,2025-11-04 23:33:46,
ollama/ollama,12893,"GPU Crashes with ""killed entity"" errors on AMD RX 7900 XTX with newer kernels",open,2025-10-31 22:35:34,,"bug,amd,gpu"
ollama/ollama,12892,ggml: Avoid cudaMemsetAsync during memory fitting,closed,2025-10-31 22:04:45,2025-10-31 22:23:29,
ollama/ollama,12891,The OpenApi Yaml document has several operations included that are missing responses.,closed,2025-10-31 21:04:30,2025-11-11 23:39:36,bug
ollama/ollama,12890,cpu: always ensure LibOllamaPath included,closed,2025-10-31 20:54:57,2025-10-31 21:37:29,
ollama/ollama,12889,Ollama 0.12.6 running perfectly intel gpu but 0.12.7 and 0.12.8 are not working perfectly please solve it and fully support ollama in intel gpu quickly ,closed,2025-10-31 19:11:49,2025-11-05 02:04:55,"feature request,needs more info"
ollama/ollama,12888,logs: catch rocm errors,closed,2025-10-31 15:56:18,2025-10-31 16:54:26,
ollama/ollama,12887,Ollama runs Gemma3 perfectly but not llama3.2:1b,closed,2025-10-31 15:32:50,2025-11-13 11:39:56,bug
ollama/ollama,12886,ollama 0.12.[78] not using AVX in non-GPU situations,closed,2025-10-31 15:30:39,2025-10-31 21:37:31,bug
ollama/ollama,12885,Multiple server addresses from one host.,open,2025-10-31 14:30:51,,feature request
ollama/ollama,12884,error parsing tool call ... raw=... err=invalid character ... after top-level value after request to gpt-oss via api/chat,open,2025-10-31 12:46:37,,bug
ollama/ollama,12883,GUI: Models folder resets to default after macOS restart and cannot be changed,open,2025-10-31 11:56:44,,"bug,app"
ollama/ollama,12882,10x slower Qwen3 and 2.5 VL,closed,2025-10-31 11:16:35,2025-10-31 21:37:30,bug
ollama/ollama,12881,Quits during converting qwen3-vl-32b-thinking with no error,closed,2025-10-31 10:29:09,2025-11-07 13:47:14,bug
ollama/ollama,12880,[Nitpick] The ollama installer on windows ignors scalling settings.,closed,2025-10-31 09:47:52,2025-10-31 10:59:48,bug
ollama/ollama,12879,Local Ollama Proxy sends Cloud model responses for non-streaming tasks with incorrect Content-Type header,closed,2025-10-31 09:05:43,2025-10-31 11:58:45,bug
ollama/ollama,12878,APP OLLAMA: Add support for specifying IP:port of a remote Ollama server,open,2025-10-31 08:38:41,,feature request
ollama/ollama,12877,when can we use the ollama 0.12.8 version?,closed,2025-10-31 06:42:13,2025-11-01 10:12:01,feature request
ollama/ollama,12876,It is recommended to support the PaddleOCR-VL and DeepSeek OCR models,closed,2025-10-31 05:47:17,2025-10-31 08:29:39,feature request
ollama/ollama,12875,404 link in https://ollama.com/download,closed,2025-10-31 05:29:26,2025-10-31 06:30:02,bug
ollama/ollama,12874,build(deps): bump the npm_and_yarn group across 1 directory with 18 u‚Ä¶,closed,2025-10-31 05:12:10,2025-11-16 03:51:18,app
ollama/ollama,12873,Ollama 0.12.7 is 4x+ slower than Ollama 0.12.6,closed,2025-10-31 00:10:43,2025-10-31 21:37:30,bug
ollama/ollama,12872,"Ollama 0.11.3 or later fails to build with CUDA, can't find math.h in #include_next (fixed with work around)",closed,2025-10-30 22:33:14,2025-11-16 13:28:39,"bug,linux,build"
ollama/ollama,12871,bring back sysfs based VRAM information for AMD,closed,2025-10-30 22:02:09,2025-11-17 23:40:59,
ollama/ollama,12870,ml: add slice operation,closed,2025-10-30 21:34:45,2025-11-13 21:28:21,
ollama/ollama,12869,win: avoid ID mixups on refresh,closed,2025-10-30 21:25:01,2025-10-30 22:12:15,
ollama/ollama,12868,Ollama Docker: Compile for Compute Capability 10.0,open,2025-10-30 20:26:51,,
ollama/ollama,12867,testing: test more models with tool calling,closed,2025-10-30 20:04:30,2025-10-30 20:19:21,
ollama/ollama,12866,ollama ls: there is no way to sort the models,open,2025-10-30 19:52:19,,feature request
ollama/ollama,12865,Issue with running qwen3-vl:2b localy and getting a 500 error from server,closed,2025-10-30 19:37:34,2025-10-30 20:28:46,bug
ollama/ollama,12864,win: use copy for subprocess logs,closed,2025-10-30 19:13:48,2025-10-30 20:22:00,
ollama/ollama,12863,embeddings: removed redundant test,closed,2025-10-30 19:10:30,2025-10-31 00:12:33,
ollama/ollama,12862,qwen3vl: enable flash attention by default,closed,2025-10-30 17:36:27,2025-10-30 17:51:37,
ollama/ollama,12861,LLM: Add regex-based error pattern matching to catch GPU and backend errors,closed,2025-10-30 14:06:41,2025-11-06 07:19:18,
ollama/ollama,12860,Ollama on B200 with Single GPU,closed,2025-10-30 13:43:36,2025-10-30 20:27:54,bug
ollama/ollama,12859,Persistent 500 Internal Server Error with `qwen3-vl:235b-cloud` via OpenAI Interface,open,2025-10-30 12:56:02,,"bug,cloud"
ollama/ollama,12858,Error when runing a model.  Msg: error looking up nvidia GPU memory,closed,2025-10-30 12:06:22,2025-10-31 06:31:18,bug
ollama/ollama,12857,"run  qwen3:32b   ctx=32K    ‰πãÂâçÂêØÂä®ÊòæÂ≠ò32G , Áé∞Âú®ÂêØÂä®Âç†Áî®129G  (v0.12.7)",closed,2025-10-30 11:58:53,2025-10-31 13:03:40,bug
ollama/ollama,12856,fix for issue 12781 removes regression on 3d0b1734,closed,2025-10-30 11:05:48,2025-11-02 00:15:45,
ollama/ollama,12855,num_ctx for Qwen3-VL models doesn't work.,closed,2025-10-30 10:22:17,2025-11-01 10:33:24,bug
ollama/ollama,12854,Qwen3-VL Runs Significantly Slower Than Qwen2.5-VL on Windows with RTX 4090,closed,2025-10-30 09:47:50,2025-10-30 13:11:55,bug
ollama/ollama,12853,Wrong Context Size for DeepSeek Coder V2 Models,open,2025-10-30 09:11:38,,"bug,ollama.com"
ollama/ollama,12852,How to save the model to safetensor format?,closed,2025-10-30 09:07:55,2025-11-02 00:02:21,
ollama/ollama,12851,Qwen3-VL does not support images (0.12.7)?,open,2025-10-30 09:02:21,,"bug,app"
ollama/ollama,12850,Unavailable to load Qwen3-VL-32b,closed,2025-10-30 08:59:24,2025-11-02 00:05:23,bug
ollama/ollama,12849,qwen3-vl parallel request failed,open,2025-10-30 07:54:01,,bug
ollama/ollama,12848,Vulkan device selection,closed,2025-10-30 07:27:20,2025-11-12 16:40:39,feature request
ollama/ollama,12847,Unable to run qwen3-vl:32b on Apple M2 Ultra (with 192 GB unified memory),closed,2025-10-30 00:45:51,2025-10-30 00:56:13,bug
ollama/ollama,12846,v0.12.7-Qwen3-VL-30b Memory management or misunderstanding of internal workings?,closed,2025-10-30 00:22:44,2025-10-30 10:55:03,bug
ollama/ollama,12845,v0.12.7: 500 Error when loading Qwen3 VL 235B (Instruct & Thinking),closed,2025-10-29 23:59:10,2025-10-30 22:15:25,"bug,macos"
ollama/ollama,12844,tests: add tests and docs for commonly used ops,closed,2025-10-29 23:47:25,2025-10-30 17:32:45,
ollama/ollama,12843,v0.12.7 embedding no results,closed,2025-10-29 23:44:17,2025-10-30 00:07:57,bug
ollama/ollama,12842,Ollama 0.12.6 fails to find CUDA during build (fixed with work around),closed,2025-10-29 22:08:03,2025-11-16 13:28:30,"bug,linux,build"
ollama/ollama,12841,"fix: qwen2.5vl, qwen3vl composite image",closed,2025-10-29 22:02:43,2025-10-30 17:33:20,
ollama/ollama,12840,v0.12.6 Container Build Failure on Linux,closed,2025-10-29 21:55:45,2025-10-30 00:42:12,bug
ollama/ollama,12839,truncation: fixed runner truncation logic + removed server truncation,open,2025-10-29 21:51:13,,
ollama/ollama,12838,Removing whitespace between Thinking and Content in Qwen3VL,closed,2025-10-29 21:03:58,2025-10-29 22:14:28,
ollama/ollama,12837,qwen3-vl: Can't turn off thinking,closed,2025-10-29 20:30:29,2025-10-29 20:37:48,bug
ollama/ollama,12836,Integration Tests for Qwen3VL,open,2025-10-29 19:00:42,,
ollama/ollama,12835,int: harden server lifecycle,closed,2025-10-29 18:33:21,2025-10-29 18:50:56,
ollama/ollama,12834,fix: conv2d bias,closed,2025-10-29 17:48:08,2025-10-29 18:03:43,
ollama/ollama,12833,QWEN3-VL-abliterated running error,closed,2025-10-29 17:01:50,2025-10-30 01:44:49,bug
ollama/ollama,12832,fix(cmd): unload model before removal,closed,2025-10-29 16:52:11,2025-10-30 17:41:49,
ollama/ollama,12831,qwen3-vl JSON format,open,2025-10-29 16:28:54,,bug
ollama/ollama,12830,tests: fix embeddinggemma integration test,closed,2025-10-29 16:28:22,2025-10-29 18:07:28,
ollama/ollama,12829,Feature Request: Ollama Snap with GPU Support for Ubuntu Core 24,open,2025-10-29 16:23:10,,feature request
ollama/ollama,12828,there is no way to control model download concurrency (ollama pull),closed,2025-10-29 15:53:42,2025-11-13 11:36:37,bug
ollama/ollama,12827,cloud models login link account connect loop issue,closed,2025-10-29 15:19:31,2025-10-30 06:28:36,"bug,cloud"
ollama/ollama,12826,Sudden error pull model manifest 412 despite new version running,closed,2025-10-29 14:28:55,2025-10-31 12:17:37,"bug,needs more info"
ollama/ollama,12825,Ollama Shared environment context with LLM under the hood,closed,2025-10-29 13:27:10,2025-10-29 14:24:48,bug
ollama/ollama,12824,Create blank template,closed,2025-10-29 11:11:08,2025-10-29 11:15:55,
ollama/ollama,12823,Blank template,closed,2025-10-29 11:09:58,2025-11-13 11:35:53,"model,needs more info"
ollama/ollama,12822,readme: fixed broken docs links,closed,2025-10-29 10:29:10,2025-10-30 17:14:40,
ollama/ollama,12821,gemma3:27b|gemma3:27b-it-qat failed in ollama 0.12.7-rc0,closed,2025-10-29 08:28:03,2025-10-29 23:12:41,bug
ollama/ollama,12820,qwen3-vl:32b /set nothink not work and  doesn't support parallel requests,closed,2025-10-29 08:17:15,2025-10-29 09:30:48,bug
ollama/ollama,12819,OlllamaÔºàv0.12.6Ôºâdoesn't support qwen3-vl(all kinds of parameters),closed,2025-10-29 06:31:11,2025-10-29 09:27:03,bug
ollama/ollama,12818,docs: temporarily restore api.md,closed,2025-10-29 06:04:41,2025-10-29 06:25:48,
ollama/ollama,12817,What happened to API.md?,closed,2025-10-29 06:00:35,2025-10-29 06:27:05,
ollama/ollama,12816,refactor: iterator based http client for ollama server,open,2025-10-29 04:49:45,,
ollama/ollama,12815,AMD MI50 cann't load qwen3-vl into GPU VRM on docker ollama/ollama:0.12.7-rc0-rocm,open,2025-10-29 03:42:42,,"bug,amd,docker"
ollama/ollama,12814,Internal server Error when pulling qwen3-vl:30b,closed,2025-10-29 03:26:31,2025-10-29 09:26:44,bug
ollama/ollama,12813,docs: fix api docs,closed,2025-10-29 02:16:00,2025-10-29 02:17:54,
ollama/ollama,12812,docs: add new cloud model + fix openai redirect,closed,2025-10-29 02:01:47,2025-10-29 02:09:08,
ollama/ollama,12811,Enable op_offload to improve partial offload performance,closed,2025-10-29 00:44:27,2025-10-30 20:53:10,
ollama/ollama,12810,"Revert ""server: Consolidate embedding truncation in runner (#12730)""",closed,2025-10-28 21:36:52,2025-10-28 21:49:14,
ollama/ollama,12809,docs: update readme and links,closed,2025-10-28 21:07:30,2025-10-28 23:20:03,
ollama/ollama,12808,darwin: improve free memory reporting,open,2025-10-28 20:33:50,,
ollama/ollama,12807,interleaved mrope,closed,2025-10-28 20:22:05,2025-10-30 18:29:00,
ollama/ollama,12806,API Stop call,closed,2025-10-28 20:19:25,2025-10-29 00:27:25,feature request
ollama/ollama,12805,docs: add docs for docs.ollama.com,closed,2025-10-28 20:13:44,2025-10-28 20:18:48,
ollama/ollama,12804,docs: rename to mdx to setup docs site,closed,2025-10-28 20:00:23,2025-10-28 20:04:31,
ollama/ollama,12803,"Revert ""docs: add reference to docs.ollama.com""",closed,2025-10-28 19:52:09,2025-10-28 19:52:49,
ollama/ollama,12802,embeddings-test: removed duplicate test ,closed,2025-10-28 19:00:01,2025-10-28 21:51:41,
ollama/ollama,12801,Added STT inputs And TTS Output Check Readme File for the updates,closed,2025-10-28 18:59:12,2025-10-28 21:08:38,
ollama/ollama,12800,docs: add reference to docs.ollama.com,closed,2025-10-28 18:49:31,2025-10-28 19:44:03,
ollama/ollama,12799,ollama cloud models aren't working,open,2025-10-28 14:53:24,,bug
ollama/ollama,12798,Never update on MacOS Tahoe,open,2025-10-28 12:37:00,,bug
ollama/ollama,12797,docker image 0.12.5 + fails to use Nvidia GPU,closed,2025-10-28 09:26:24,2025-10-28 12:46:44,bug
ollama/ollama,12796,embed: add distance correlation test for library embed models,closed,2025-10-28 08:17:28,2025-10-28 23:57:28,
ollama/ollama,12795,embeddings: added embedding command for cl ,closed,2025-10-28 00:01:57,2025-11-05 19:58:03,
ollama/ollama,12794,Tool Calling with Jan Nano,closed,2025-10-27 22:56:59,2025-11-13 11:34:49,bug
ollama/ollama,12793,create: inherit FROM model's renderer/parser,closed,2025-10-27 22:15:12,2025-10-28 07:15:46,
ollama/ollama,12792,qwen3-coder loses renderer/parser (and tool calling ability) when `/save`d from the cli,closed,2025-10-27 22:06:34,2025-10-28 07:15:47,bug
ollama/ollama,12791,vendor: GGML update to b6840,closed,2025-10-27 18:39:47,2025-11-06 18:19:22,
ollama/ollama,12790,embeddings: modified tests to validate exact base-64 string,closed,2025-10-27 17:45:19,2025-10-28 17:37:20,
ollama/ollama,12789,Cloud doesn't support images in `/api/generate`.,closed,2025-10-27 12:44:24,2025-10-30 23:15:54,"bug,cloud"
ollama/ollama,12788,Gemma3 base model merging finetuned lora adapters,open,2025-10-27 09:39:43,,feature request
ollama/ollama,12787,"Embedding workers crash with SIGTRAP under load (regression in v0.12.4+, on MacOS)",closed,2025-10-27 05:24:41,2025-10-27 10:10:12,bug
ollama/ollama,12786,tests: move csv output to benchstat format,open,2025-10-27 01:26:43,,
ollama/ollama,12785,reshape the Conv2D,closed,2025-10-27 00:34:20,2025-10-27 00:35:31,
ollama/ollama,12784,Image channel R and B seems to be flipped when using api/generate,closed,2025-10-26 06:20:21,2025-10-26 06:36:34,bug
ollama/ollama,12783,Allow a loaded model to be used with less context size than what it was initialized for.,open,2025-10-26 03:02:17,,feature request
ollama/ollama,12782,Add a GPU temperature check during generation or streaming.,open,2025-10-25 20:59:21,,feature request
ollama/ollama,12781,After updating from ollama 0.9.3 gpu offloading stopped working on models larger than VRAM,closed,2025-10-25 19:01:59,2025-11-02 00:16:30,"bug,needs more info"
ollama/ollama,12780,Missing tools in new model created from qwen3-coder:30b,closed,2025-10-25 15:07:43,2025-10-26 07:52:21,bug
ollama/ollama,12779,api: native API should accept max_tokens as alias for num_predict (OpenAI compatibility),closed,2025-10-25 13:32:48,2025-10-25 18:17:05,bug
ollama/ollama,12778,"Ollama displays as ""com.github.Squirrel"" in menu bar settings on macOS",open,2025-10-25 12:05:10,,app
ollama/ollama,12777,feat: Set params.embeddings according to the pooling_type in GGUF,open,2025-10-25 09:45:10,,
ollama/ollama,12776,Tool calls not working with huihui_ai/kimi-k2:1026b,closed,2025-10-24 22:37:29,2025-10-25 18:15:13,bug
ollama/ollama,12775,Fix vulkan PCI ID and ID handling,closed,2025-10-24 22:28:56,2025-10-28 22:15:36,
ollama/ollama,12774,"0.12.5-rocm AMD Vega 10 ""failure during GPU discovery"" ... error=""runner crashed"". It worked on 0.12.3-rocm",closed,2025-10-24 21:53:15,2025-10-25 09:09:36,bug
ollama/ollama,12773,Failed to load qwen3-coder:30b,open,2025-10-24 21:37:12,,"bug,windows,amd,gpu"
ollama/ollama,12772,3470a5c89 ggml-alloc : make gallocr prefer chunks that allow memory reuse (#16788),closed,2025-10-24 20:30:33,2025-10-27 20:19:58,
ollama/ollama,12771,Add initial devcontainer configuration,closed,2025-10-24 20:06:50,2025-11-02 23:09:09,
ollama/ollama,12770,Nvidia Tesla T4 doesn't work since 0.9.3,closed,2025-10-24 16:55:03,2025-10-24 19:21:37,bug
ollama/ollama,12769,OCT 24 2025 | ollama version is 0.12.6 | Not running |Windows 10 #12766  Reference to this case,closed,2025-10-24 08:50:08,2025-11-05 22:11:16,"bug,needs more info"
ollama/ollama,12768,Deepseek-r1:70b seems only uses one GPU out of four,closed,2025-10-24 08:45:45,2025-10-24 16:44:03,
ollama/ollama,12767,API docs on website are outdated,open,2025-10-24 08:08:37,,documentation
ollama/ollama,12766,OCT 24 2025 | ollama version is 0.12.6 | Not running |Windows 10,closed,2025-10-24 07:50:42,2025-10-24 08:45:03,bug
ollama/ollama,12765,An optional command line parameter in ollama pull to rename a downloaded model,closed,2025-10-24 07:32:03,2025-10-24 16:46:39,feature request
ollama/ollama,12764,A way to quickly stop models via Ollama's system tray symbol in Windows,open,2025-10-24 07:28:43,,feature request
ollama/ollama,12763,Gpt-Oss Template does not handle enum arrays correctly In Tools,open,2025-10-24 05:52:35,,"bug,tools"
ollama/ollama,12762,Opening settings menu clears unsent prompts,open,2025-10-24 04:07:20,,"bug,app"
ollama/ollama,12761,fix(llama.cpp): Set final embedding to res->t_embd to fix error with PLaMo2 models,closed,2025-10-24 03:17:27,2025-10-24 18:01:21,
ollama/ollama,12760,Ê±ÇÂä©,open,2025-10-24 02:38:18,,needs more info
ollama/ollama,12759,cloud: set the proxy content-type to the same as local models,closed,2025-10-24 02:10:24,2025-10-25 17:57:11,
ollama/ollama,12758,llm: Change memory allocation backoff from exponential to incremental,closed,2025-10-23 19:03:16,2025-10-23 19:58:32,
ollama/ollama,12757,"Qwen3-Embedding-8B  ""model does not support embeddings""",open,2025-10-23 17:24:46,,"bug,needs more info"
ollama/ollama,12756,Server crashes when trying to run some models,closed,2025-10-23 13:37:12,2025-10-23 17:59:35,bug
ollama/ollama,12755,"Error: command must be one of ""from"", ""license"", ""template"", ""system"", ""adapter"", ""parameter"", or ""message""",closed,2025-10-23 12:21:41,2025-10-23 15:09:32,bug
ollama/ollama,12754,Title: ROCm GPU (RX 7900 XTX) Not Utilized on Windows 11 with iGPU Present - Fallback to CPU,closed,2025-10-23 10:48:23,2025-10-30 22:12:16,"bug,windows,amd"
ollama/ollama,12753,Â¶Ç‰ΩïÂ∞ÜÂ§öÊ®°ÊÄÅÊ®°Âûã‰ªésafetensorÊ†ºÂºèËΩ¨Âà∞ollamaÂèØÁî®ÁöÑÊ†ºÂºèÔºü,closed,2025-10-23 10:36:41,2025-11-06 11:49:50,needs more info
ollama/ollama,12752,gpt-oss:120b fails to load on Windows (Ollama 0.12.6) with AMD AI Max+ 395 (96GB VRAM) ‚Äî works fine on Ubuntu,open,2025-10-23 09:22:53,,"bug,needs more info,memory"
ollama/ollama,12751,[qwen3-coder] Improper parsing/signalling of tool calls with multiple tools,open,2025-10-23 08:47:23,,bug
ollama/ollama,12750,Trying to pull models does not work on multiple machines,closed,2025-10-23 07:34:14,2025-10-23 10:06:04,bug
ollama/ollama,12749,docs: add VT Code project to Community Integrations Terminal,closed,2025-10-23 05:51:51,2025-10-23 19:29:50,
ollama/ollama,12748,ollama 0.12.6-1 doesn't detect total vram AMD GPU - due to broken Arch linux packages,open,2025-10-23 00:35:40,,"bug,linux"
ollama/ollama,12747,kvcache: Remove special case for reservation mask,closed,2025-10-22 23:12:01,2025-10-23 00:38:04,
ollama/ollama,12746,Benchmarking Local Ollama Models (Performance & Quality) - created.,open,2025-10-22 22:18:56,,feature request
ollama/ollama,12745,docs: add Agent Platforms & Orchestration category with entry for AgentSystems,open,2025-10-22 21:47:52,,
ollama/ollama,12744,docs: add Agent Platforms & Orchestration category with AgentSystems,closed,2025-10-22 21:29:01,2025-10-22 21:35:27,
ollama/ollama,12743,olmOCR 2 7B 1025 (for local use),closed,2025-10-22 21:14:50,2025-10-23 22:05:10,model
ollama/ollama,12742,llamarunner: Record the time for all batches during prompt processing,closed,2025-10-22 19:00:44,2025-10-22 20:52:58,
ollama/ollama,12741,gpt-oss:20b generating lots of repeated chunks ( is worse when prompt contains typos ),closed,2025-10-22 18:04:13,2025-10-23 22:07:23,bug
ollama/ollama,12739,feat(ggml): sync implementations for other CPU architectures with ggml,open,2025-10-22 15:31:25,,
ollama/ollama,12738,"tools: parse tool calls that don't conform to (""name"": name, ""arguments"": args}",closed,2025-10-22 14:53:11,2025-10-22 18:34:28,
ollama/ollama,12737,cmake: restrict GGML_CPU_ALL_VARIANTS to supported architectures,open,2025-10-22 14:28:04,,
ollama/ollama,12736,qwen3 regression: thinking spills into message content with think=false,closed,2025-10-22 13:52:20,2025-10-23 08:36:38,bug
ollama/ollama,12735,feat: Enable image embeddings for vision models,closed,2025-10-22 13:36:03,2025-11-12 19:18:14,
ollama/ollama,12734,ROCM: 7.x.x support,open,2025-10-22 05:11:59,,"feature request,amd"
ollama/ollama,12733,Flush EOS piece before removing sequence,closed,2025-10-22 02:35:08,2025-10-22 04:31:59,
ollama/ollama,12732,routes/types: add tool call id,closed,2025-10-22 02:20:53,2025-11-06 06:35:25,
ollama/ollama,12731,"""Access is denied"" on Windows when built with Go 1.25.3",open,2025-10-22 02:15:44,,"bug,windows,build"
ollama/ollama,12730,server: Consolidate embedding truncation in runner,closed,2025-10-22 01:54:09,2025-10-27 18:59:13,
ollama/ollama,12729,Add Multi-Model Pull and Multi-Path Storage Support,open,2025-10-22 01:37:14,,
ollama/ollama,12728,Inaccurate last modified timestamp,closed,2025-10-21 22:50:06,2025-10-22 00:31:34,feature request
ollama/ollama,12727,Ollama server fails to load big models with Nvidia GPU installed: out of pinned memory,open,2025-10-21 21:18:48,,bug
ollama/ollama,12726,JSON Schema with pattern not working,open,2025-10-21 20:19:36,,bug
ollama/ollama,12725,Optimization with Nvidia DGX Spark,closed,2025-10-21 19:30:15,2025-10-21 21:15:42,bug
ollama/ollama,12724,cloud: don't error sending empty messages,closed,2025-10-21 18:59:51,2025-10-22 01:12:14,
ollama/ollama,12723,Add ollama embed CLI command for interactive embeddings,closed,2025-10-21 15:18:31,2025-11-06 06:51:25,
ollama/ollama,12722,100% GPU Model Runs 100% CPU,closed,2025-10-21 11:36:40,2025-10-21 12:25:11,bug
ollama/ollama,12721,feat: Enable image embeddings for vision models,closed,2025-10-21 09:46:47,2025-10-22 12:20:14,
ollama/ollama,12720,Support for Intel Arc GPU,closed,2025-10-21 08:32:54,2025-10-21 19:11:35,feature request
ollama/ollama,12719,deepseek-r1:14b does not support tools,closed,2025-10-21 06:09:04,2025-10-21 07:10:23,bug
ollama/ollama,12718,Ollama asks for admin password on Managed macOS accounts,closed,2025-10-21 03:21:27,2025-11-26 12:34:27,feature request
ollama/ollama,12717,Consolidate embeddings truncation,closed,2025-10-21 03:04:28,2025-10-27 19:16:09,bug
ollama/ollama,12716,Ling-V2,closed,2025-10-21 02:47:21,2025-10-21 07:23:55,model
ollama/ollama,12715,embeddings: base64 encoding fix ,closed,2025-10-21 00:44:06,2025-10-22 18:27:44,
ollama/ollama,12714,runner: always truncate embeddings requests,closed,2025-10-20 23:05:47,2025-10-20 23:47:05,
ollama/ollama,12713,model/parsers: remove warning for missing <think> tag for qwen3-vl,closed,2025-10-20 21:42:25,2025-10-20 23:03:43,
ollama/ollama,12712,chore: simplify server tests,open,2025-10-20 21:19:41,,
ollama/ollama,12711,docs: vulkan information,closed,2025-10-20 21:09:31,2025-11-04 23:50:14,
ollama/ollama,12710,After updating to v0.12.6 mxbai-embed-large stopped working.,closed,2025-10-20 20:53:02,2025-10-20 23:47:06,bug
ollama/ollama,12709,`ollama run` should work with embedding models,closed,2025-10-20 18:22:37,2025-11-23 20:19:32,feature request
ollama/ollama,12708,"failure during GPU discovery (AMD 7900 XT) using systemd, but OK from command line",closed,2025-10-20 17:20:58,2025-11-08 09:41:59,"bug,amd,gpu"
ollama/ollama,12707,cuda: get driver version after props,closed,2025-10-20 15:49:32,2025-10-20 17:57:27,
ollama/ollama,12706,"""vocab only"" loading issue",closed,2025-10-20 14:33:46,2025-10-20 15:06:23,bug
ollama/ollama,12705,Ollama 0.12.6 crashes Terminal.app on MacOS X,open,2025-10-20 13:01:18,,bug
ollama/ollama,12703,docker build not successful,open,2025-10-20 11:36:20,,bug
ollama/ollama,12702,deepseek-v3.1:671b-cloud not parsing tool calls correctly,closed,2025-10-20 10:27:17,2025-11-08 01:13:27,bug
ollama/ollama,12701,DeepSeek OCR,closed,2025-10-20 09:15:44,2025-11-19 02:21:24,model
ollama/ollama,12700,Ollama service changes folder perms to something it can't use,closed,2025-10-20 02:45:06,2025-11-13 11:31:56,bug
ollama/ollama,12699,ollama 0.12.4+ on GPU-less Windows machines gets wedged loading models,closed,2025-10-20 01:09:30,2025-11-05 18:32:39,"bug,windows,nvidia"
ollama/ollama,12698,ROCm 7.0 with gfx1103 not detected,closed,2025-10-19 23:48:53,2025-10-20 11:03:30,bug
ollama/ollama,12697,Ollama cant load cloud models,closed,2025-10-19 21:16:41,2025-10-22 01:12:15,bug
ollama/ollama,12696,"Model runner crashes with SIGABRT when prompt exceeds ~33,272 tokens on deepseek-v3.1:671b with num_ctx >= 40960",open,2025-10-19 18:44:20,,bug
ollama/ollama,12695,Stop all models,closed,2025-10-19 18:40:25,2025-10-19 18:46:11,feature request
ollama/ollama,12694,Incorrect content type in cloud-hosted models,closed,2025-10-19 17:38:06,2025-10-25 17:57:12,"bug,cloud"
ollama/ollama,12693,500 Internal Server Error: Unable to Load Model File for `granite4:tiny-h`,closed,2025-10-19 10:47:55,2025-10-20 03:19:39,bug
ollama/ollama,12692,Ollama running Granite 4 Hybrid reports wrong memory estimation,open,2025-10-19 07:54:18,,bug
ollama/ollama,12691,[Windows] Ollama server not starting when launching app,closed,2025-10-19 04:22:36,2025-11-14 01:01:16,"bug,windows"
ollama/ollama,12690,Web search does not work properly with non gpt-oss models,open,2025-10-18 19:41:57,,bug
ollama/ollama,12689,plamo-2-translate-gguf,open,2025-10-18 13:19:05,,model
ollama/ollama,12688,Ollama Cloud integration in Vim/Neovim,open,2025-10-18 10:16:08,,feature request
ollama/ollama,12687,RAG,closed,2025-10-18 06:24:47,2025-10-19 00:18:11,bug
ollama/ollama,12686,fix gemma3n on vulkan,closed,2025-10-18 01:27:21,2025-10-18 01:39:18,
ollama/ollama,12685,PaddleOCR-VL,open,2025-10-17 23:03:10,,model
ollama/ollama,12684,Local connection unavailable - version 0.12.6,closed,2025-10-17 22:16:28,2025-10-18 11:21:05,bug
ollama/ollama,12683,win: more verbose load failures,closed,2025-10-17 21:48:58,2025-10-18 00:13:16,
ollama/ollama,12682,Fan on passively cooled server GPUs (NVIDIA P40) runs at 100% after v1.30 update due to GPU inactivity,open,2025-10-17 20:52:16,,"bug,needs more info"
ollama/ollama,12681,rocm: give it more time to bootstrap,closed,2025-10-17 19:50:16,2025-10-20 16:43:05,
ollama/ollama,12680,Adding support for amd new GPUS 90xx series,closed,2025-10-17 19:14:45,2025-10-19 00:21:57,bug
ollama/ollama,12679,env config to bypass iGPUs,closed,2025-10-17 16:47:21,2025-11-04 23:51:07,
ollama/ollama,12678,deepseek-coder-v2:16b 8.9GB on disk unpacks to 20GB,closed,2025-10-17 14:29:15,2025-10-17 14:35:59,bug
ollama/ollama,12677,"Ollama Cloud, tool_calls function_id",open,2025-10-17 13:54:13,,feature request
ollama/ollama,12676,running the run code for llama3.2 led to stuck,closed,2025-10-17 12:09:07,2025-10-22 07:36:36,"bug,windows"
ollama/ollama,12675,glm4.6 not work,closed,2025-10-17 10:56:07,2025-10-18 11:20:43,bug
ollama/ollama,12674,Since v0.12.4 gpt-oss:20b does not run on GPU (CUDA),open,2025-10-17 10:38:32,,"feature request,nvidia,build"
ollama/ollama,12673,Vulkan not working on Linux (Ubuntu Server 25.10),open,2025-10-17 06:02:11,,bug
ollama/ollama,12672,"All ggml libraries fail to load on Windows Enterprise with ""The specified procedure could not be found.""",closed,2025-10-17 04:49:49,2025-11-17 20:33:47,"bug,windows,build"
ollama/ollama,12671,chore: simplify server tests,closed,2025-10-17 00:40:21,2025-10-20 20:18:07,
ollama/ollama,12670,cmd: remove generate path from ollama run,closed,2025-10-16 23:01:15,2025-11-15 00:18:37,
ollama/ollama,12669,renderers: add global flag for setting [img] tags,closed,2025-10-16 22:18:22,2025-10-16 23:37:32,
ollama/ollama,12668,cuda: tidy up CC settings,closed,2025-10-16 21:23:03,2025-10-16 23:39:30,
ollama/ollama,12667,[Model request] Add AMD Instella 3B model(s),open,2025-10-16 20:35:13,,model
ollama/ollama,12666,cuda: bring back CC 5.2,closed,2025-10-16 19:32:14,2025-10-16 20:07:42,
ollama/ollama,12665,feat(model): add qwen3vl,closed,2025-10-16 19:26:56,2025-10-29 00:39:48,
ollama/ollama,12664,vulkan: Add memory detection for Intel GPU using DXGI+PDH,closed,2025-10-16 18:13:58,2025-11-04 22:11:56,
ollama/ollama,12663,Delete partially downloaded models not working in wsl2,closed,2025-10-16 18:11:22,2025-10-16 20:26:48,bug
ollama/ollama,12662,test: harden scheduler tests,closed,2025-10-16 16:31:14,2025-10-17 15:56:44,
ollama/ollama,12661,test: add a few missing embedding models,closed,2025-10-16 15:38:13,2025-10-16 16:36:26,
ollama/ollama,12660,ollama:0.12.5 pull qwen3-coder:30b pulling manifest  Error: EOF,closed,2025-10-16 15:09:13,2025-10-17 07:18:20,bug
ollama/ollama,12659,qwen3-coder:30b (qwen3moe) loses tool capability by creating a new model,closed,2025-10-16 14:26:27,2025-10-16 15:02:02,bug
ollama/ollama,12658,Use pciid from ggml (which is device_id) as PCIID,closed,2025-10-16 14:11:42,2025-11-01 21:15:08,
ollama/ollama,12657,Ollama on Windows spawns a new netsh process every 5 seconds,closed,2025-10-16 11:55:02,2025-10-21 17:54:01,bug
ollama/ollama,12656,Remove unnecessary MacOs 13 and lower Patches,closed,2025-10-16 11:08:47,2025-11-06 23:52:56,
ollama/ollama,12655,vulkan: Get FilterID from Backend for Vulkan,closed,2025-10-16 10:48:49,2025-10-16 16:07:35,
ollama/ollama,12654,vulkan: Add memory detection for Intel GPU using Level Zero Sysman,open,2025-10-16 07:40:38,,
ollama/ollama,12653,When will Qwen3-VL-4B and Qwen3-VL-8B be available ?,closed,2025-10-16 07:21:26,2025-10-29 23:51:35,model
ollama/ollama,12652,Add InclusionAI's models,closed,2025-10-16 05:37:13,2025-11-23 20:21:33,model
ollama/ollama,12651,openai: make tool call conversion fns public,closed,2025-10-16 04:00:35,2025-10-16 04:10:30,
ollama/ollama,12650,rocm: fix prefiltering,closed,2025-10-16 03:32:42,2025-10-16 16:07:56,
ollama/ollama,12649,"After running `ollama pull bsahane/Qwen2.5-VL-7B-Instruct:Q4_K_M_benxh`, all `ollama` commands show a Segmentation Fault",closed,2025-10-16 02:31:58,2025-10-17 01:50:25,bug
ollama/ollama,12648,The latest version of ollama cannot use the previously installed gguf model,closed,2025-10-16 02:22:40,2025-11-01 10:27:40,"bug,needs more info"
ollama/ollama,12647,Grace/qwen3 thinking,closed,2025-10-16 00:26:46,2025-10-16 22:29:41,
ollama/ollama,12646,fs(ggml): fill in arch prefix if necessary,closed,2025-10-16 00:06:24,2025-10-20 23:42:18,
ollama/ollama,12645,glm-4.6 cloud name breaks some clients.,closed,2025-10-15 23:02:01,2025-10-15 23:44:48,"bug,cloud"
ollama/ollama,12644,Open Message to Mark Zuckerberg (for Developers and Platform Designers),closed,2025-10-15 21:21:17,2025-10-15 21:52:07,
ollama/ollama,12643,Support all text file formats (like AsciiDoc) in Windows UI,open,2025-10-15 19:30:00,,feature request
ollama/ollama,12642,"Revert ""Workaround broken NVIDIA iGPU free VRAM data (#12490)""",closed,2025-10-15 18:51:18,2025-10-16 16:09:49,
ollama/ollama,12641,perf: backport cuda iGPU sched spin,closed,2025-10-15 18:42:12,2025-10-15 18:52:14,
ollama/ollama,12640,ollama hanging,closed,2025-10-15 18:41:44,2025-11-05 18:32:39,"bug,windows"
ollama/ollama,12639,llm: Enable flash attention by default for gemma3,closed,2025-10-15 17:26:17,2025-10-15 17:42:13,
ollama/ollama,12638,Disable Ollama GUI Popup on API Requests (Windows 11),open,2025-10-15 15:29:59,,bug
ollama/ollama,12637,Wrap llama.cpp,closed,2025-10-15 15:20:49,2025-10-16 01:12:23,feature request
ollama/ollama,12636,OpenAI API Raw Completions does not seem to work properly,open,2025-10-15 15:02:40,,bug
ollama/ollama,12635,Granite4-small download fails,closed,2025-10-15 14:36:28,2025-10-15 14:51:30,bug
ollama/ollama,12634,errors: unable to allocate CUDA0 buffer | cudaMalloc failed: out of memory,closed,2025-10-15 11:51:47,2025-10-15 17:38:47,bug
ollama/ollama,12633,"Qwen3-embedding:0.6b does not work on v0.12.5 , but works on previous versions",closed,2025-10-15 10:51:47,2025-10-17 18:52:22,bug
ollama/ollama,12632,"Trying to use qwen-coder:30b results with ""The model you are attempting to pull requires a newer version of Ollama.""",closed,2025-10-15 10:05:45,2025-10-27 00:02:13,bug
ollama/ollama,12631,ÊòæÂ≠òË¢´ÈôêÂà∂‰∫ÜÔºåÊòæÂ≠òÊó†Ê≥ïÂêÉÊª°,closed,2025-10-15 08:23:29,2025-10-27 00:01:36,
ollama/ollama,12630,chore: fix function name in comment,closed,2025-10-15 07:24:08,2025-10-16 04:53:38,
ollama/ollama,12629,Update README.md add Libraries achatbot-go a multimodal chatbot.,closed,2025-10-15 07:17:58,2025-10-16 04:54:16,
ollama/ollama,12628,"The thinking content of OLLAMA should be placed within key ""reasoning_content""",closed,2025-10-15 06:11:04,2025-10-30 07:53:23,bug
ollama/ollama,12627,Qwen 3 vl-30b request,closed,2025-10-15 06:02:30,2025-10-29 23:43:13,model
ollama/ollama,12626,run qwen3-vl error 500,closed,2025-10-15 04:43:54,2025-10-29 23:53:05,bug
ollama/ollama,12625,types: send index for tool calls,closed,2025-10-15 02:02:42,2025-10-15 02:35:15,
ollama/ollama,12624,Download failures on corporate networks with SSL inspection,closed,2025-10-15 01:44:07,2025-11-13 11:28:18,bug
ollama/ollama,12623,llm: Perform eviction when num_gpu is set with new estimates,closed,2025-10-15 00:37:14,2025-10-15 00:46:36,
ollama/ollama,12622,Ollama fails to detect GPU after upgrading to 0.12.5 (WSL),closed,2025-10-15 00:20:45,2025-10-17 03:50:45,bug
ollama/ollama,12621,qwen3-coder: support anyOf when parsing tool calls,closed,2025-10-14 22:33:21,2025-10-14 22:51:24,
ollama/ollama,12620,Qwen3-VL-235B-A22B,closed,2025-10-14 20:48:28,2025-10-14 20:50:21,model
ollama/ollama,12619,ml/backend/ggml: NVML fallback for unified memory GPUs,closed,2025-10-14 20:26:35,2025-10-15 18:40:06,
ollama/ollama,12618,Ollama serve fails to detect Nvidia GPUs after updating to the latest version,open,2025-10-14 19:42:44,,"bug,windows,nvidia"
ollama/ollama,12617,envconfig: default to port 443 when connecting to ollama.com,closed,2025-10-14 19:15:40,2025-10-15 06:38:24,
ollama/ollama,12616,"llm: Optimize memory calculation on ZFS, Add env var control for memory check and log level",open,2025-10-14 19:14:39,,
ollama/ollama,12615,"Is `use_mmap` ineffective for some models? For example, GPT-OSS and Qwen3:30B-A3B?",closed,2025-10-14 18:04:23,2025-10-21 08:04:08,bug
ollama/ollama,12614,CI: Set up temporary opt-out Vulkan support,closed,2025-10-14 18:01:37,2025-10-15 21:18:01,
ollama/ollama,12613,ggml-backend and ollama crash using Playwright from docker local mcptoocls gateway (granite4:tiny-h and gpt-oss:x20b),open,2025-10-14 17:16:52,,"bug,macos"
ollama/ollama,12612,Unable to allocate VRAM or system RAM despite both reporting enough available,open,2025-10-14 15:57:04,,bug
ollama/ollama,12611,The embedding result of `qwen3-embedding:8b` model are all zeros.,closed,2025-10-14 15:09:09,2025-10-16 13:44:54,"bug,macos"
ollama/ollama,12610,Can't disable thinking for qwen3:30b on API & CLI,closed,2025-10-14 14:01:09,2025-10-21 16:09:53,bug
ollama/ollama,12609,"Using the command `curl -fsSL https://ollama.com/install.sh | sh` upgrades the new version, why would it modify my ollama.service?",closed,2025-10-14 13:24:04,2025-10-27 00:01:09,bug
ollama/ollama,12608,Error occurred when starting the model,closed,2025-10-14 13:08:10,2025-10-15 03:44:51,"bug,macos"
ollama/ollama,12607,feat(models): add support for apertus architecture,open,2025-10-14 10:54:26,,model
ollama/ollama,12606,GPT-OSS:20b reasoning loop when reasoning==high,open,2025-10-14 08:44:07,,bug
ollama/ollama,12605,At,closed,2025-10-14 04:42:10,2025-10-14 06:13:55,feature request
ollama/ollama,12604,runner: fix shifting on llama runner,closed,2025-10-13 20:09:54,2025-10-13 20:46:33,
ollama/ollama,12603,fix(qwen3): deepseek distill,closed,2025-10-13 19:53:26,2025-10-13 20:30:30,
ollama/ollama,12602,"[0.12.4][Linux] RX 9060 XT reports 0 VRAM, only runs on CPU",closed,2025-10-13 19:07:23,2025-10-15 00:41:17,"bug,amd"
ollama/ollama,12601,Need Qwen3 - Reranking models on Ollama,closed,2025-10-13 17:30:58,2025-10-13 19:06:20,model
ollama/ollama,12600,Continue support for AMD gfx906,open,2025-10-13 17:12:51,,"feature request,amd"
ollama/ollama,12599,NVIDIA GPU not recognized,closed,2025-10-13 16:44:49,2025-10-13 17:23:04,bug
ollama/ollama,12598,I uninstalled your app because it updates every other day.,closed,2025-10-13 15:28:17,2025-10-13 19:09:18,feature request
ollama/ollama,12597,Ollama v0.7.0 not support ollama run gpt-oss and ollama pull qwen3-embedding,closed,2025-10-13 14:29:43,2025-10-13 19:10:29,bug
ollama/ollama,12596,Does it support reordering model?,closed,2025-10-13 12:21:46,2025-10-13 19:12:51,
ollama/ollama,12595,Publish Ollama on Microsoft Store,closed,2025-10-13 11:49:55,2025-10-13 23:11:57,feature request
ollama/ollama,12594,"In Ollama v0.12.5, the reasoning model has a slower inference speed; the deepseek-r1:8b model has issues with reasoning and output, but the deepseek-r1:7b model works fine. Is this related to memory allocation?",closed,2025-10-13 10:03:54,2025-10-27 00:00:32,bug
ollama/ollama,12593,<think> tags omitted from qwen3 models,closed,2025-10-13 09:58:03,2025-10-13 11:32:08,bug
ollama/ollama,12592,DeepSeek Coder V2 fails in OpenCode: No Tooling Support ?!,closed,2025-10-13 03:17:07,2025-10-26 23:59:19,bug
ollama/ollama,12591,Best practices for concurrent embeddings in multi-node deployments,closed,2025-10-13 01:41:24,2025-10-26 23:57:58,
ollama/ollama,12590,"logs: fix bogus ""0 MiB free"" log line",closed,2025-10-12 21:38:11,2025-10-14 18:26:28,
ollama/ollama,12589,gpt-oss:120b no longer obeying Reasoning Effort setting,closed,2025-10-12 19:48:10,2025-10-12 22:23:04,bug
ollama/ollama,12588,Ollama v0.12.5 returns error when running Gemma3 model,closed,2025-10-12 16:43:48,2025-11-01 10:26:57,"bug,nvidia,needs more info"
ollama/ollama,12587,Multiline input,closed,2025-10-12 10:28:29,2025-10-13 20:59:51,
ollama/ollama,12586,ollama ps shows 100% GPU but the GPU is not utilized,closed,2025-10-12 09:16:19,2025-10-12 10:12:04,bug
ollama/ollama,12585,Ollama v0.12.5: nomic-embed-text is failing multiple times,open,2025-10-12 08:33:43,,"bug,embeddings"
ollama/ollama,12584,Ollama CUDA Error: the function requires an architectural feature absent from the device,closed,2025-10-12 02:46:46,2025-10-14 12:58:40,"bug,nvidia"
ollama/ollama,12583,Add support for Nvidia B200 GPUs,open,2025-10-12 01:26:49,,"feature request,nvidia"
ollama/ollama,12582,"Reapply ""add truncate and shift parameters",closed,2025-10-11 20:43:30,2025-10-11 23:06:14,
ollama/ollama,12581,routes: fix built-in renderers for `api/generate`,closed,2025-10-11 20:19:06,2025-10-11 21:10:23,
ollama/ollama,12580,"New Error: ""memory layout cannot be allocated"" when switching large multi-GPU models",closed,2025-10-11 19:08:27,2025-10-15 00:46:37,bug
ollama/ollama,12579,"Ollama v0.12.5 is slow comparing to v0.12.3 (43 vers. 3 s), in v0.12.5 model did not fit in GPU memory",closed,2025-10-11 19:07:45,2025-10-14 20:16:00,"bug,memory"
ollama/ollama,12578,qwen3-coder endless trash output,closed,2025-10-11 17:14:47,2025-10-11 21:10:24,bug
ollama/ollama,12577,granite4 template is missing Fill-In-the-Middle (FIM) code completions,open,2025-10-11 16:43:35,,bug
ollama/ollama,12576,"[Windows] Ollama server exits immediately on startup with ""exit status 1""",closed,2025-10-11 14:52:22,2025-10-12 12:05:20,
ollama/ollama,12575,"Using ""/no_think"" with HYBRID models does not work anymore",closed,2025-10-11 12:45:51,2025-10-21 16:11:10,bug
ollama/ollama,12574,Ollama doesn't use NVIDIA GPU after updated to v0.12.5,closed,2025-10-11 11:13:08,2025-10-11 15:11:34,bug
ollama/ollama,12573,Rocm with 9070 xt windows,open,2025-10-11 11:08:11,,bug
ollama/ollama,12572,Ollama doesn't work on Ventura 13.7.8 Mac OS after update,closed,2025-10-11 10:07:56,2025-10-11 17:28:43,bug
ollama/ollama,12571,Beepo-22B model not working in 0.12.5,closed,2025-10-11 04:49:54,2025-10-26 23:57:30,bug
ollama/ollama,12570,"In Ollama v0.12.5, the deepseek-r1:8b model outputs are chaotic, and its reasoning capabilities have significantly degraded, showing a serious performance regression compared to v0.11.11.",closed,2025-10-11 03:17:29,2025-10-13 16:15:06,bug
ollama/ollama,12569,"In Ollama v0.12.3 and v0.12.5, the deepseek-r1:8b model outputs are chaotic, and its reasoning capabilities have significantly degraded, showing a serious performance regression compared to v0.11.11.",closed,2025-10-11 03:01:15,2025-10-11 03:03:50,bug
ollama/ollama,12568,ollama model service not working,closed,2025-10-11 02:51:52,2025-10-26 23:57:12,bug
ollama/ollama,12567,doc: remove AMD EOL GPUs,closed,2025-10-11 00:05:40,2025-10-11 00:16:30,
ollama/ollama,12566,ollamarunner: fix deadlock,closed,2025-10-10 23:42:26,2025-10-10 23:49:57,
ollama/ollama,12565,discover: fix typo,closed,2025-10-10 23:00:40,2025-10-11 19:06:03,
ollama/ollama,12564,Doens't offload any layer into GPU RAM since 0.12.4 (AMD RX 7900 XTX on Windows),closed,2025-10-10 22:17:44,2025-10-30 22:12:15,"bug,windows,amd"
ollama/ollama,12563,"`wstring_convert` is deprecated in C++17, and will be removed in C++26",closed,2025-10-10 22:11:58,2025-10-10 22:52:51,
ollama/ollama,12562,add registries for parsers/renderers,closed,2025-10-10 18:55:56,2025-10-14 09:01:53,
ollama/ollama,12561,absence of thinking text of the prompt (created using previous messages) sent to the model qwen3:4b deployed in ollama,open,2025-10-10 15:28:07,,
ollama/ollama,12560,ü§î Does num_batch do anything?  If so what and how can we use it?,closed,2025-10-10 14:24:27,2025-10-21 23:13:38,bug
ollama/ollama,12559,Non-deterministic structured output with same seed,open,2025-10-10 13:47:31,,bug
ollama/ollama,12558,try1,closed,2025-10-10 04:19:05,2025-10-10 10:46:57,
ollama/ollama,12557,Ollama Tool Calling + Streaming Issue,closed,2025-10-10 02:20:45,2025-10-18 00:48:08,bug
ollama/ollama,12556,use llama runner for qwen3,closed,2025-10-10 01:09:43,2025-10-10 02:08:21,
ollama/ollama,12555,"thinking: allow `""think"": false` for non-thinking models",closed,2025-10-10 00:49:40,2025-10-10 01:46:00,
ollama/ollama,12554,incorrect gpt-oss:120b model after latest update,closed,2025-10-09 19:42:27,2025-10-09 19:59:56,bug
ollama/ollama,12553,logs: quiet down context canceled on completion and scheduler noise,closed,2025-10-09 15:43:46,2025-10-09 17:37:47,
ollama/ollama,12552,Llama cpp bump (df1b612): granite docling / mamba2 optimizations / multimodal encoding fixes,closed,2025-10-09 15:27:16,2025-10-13 22:26:18,
ollama/ollama,12551,the absence of reasoning text,closed,2025-10-09 15:15:27,2025-10-09 18:34:03,bug
ollama/ollama,12550,refactor(install.sh): major code cleanup,open,2025-10-09 14:35:36,,
ollama/ollama,12549,hf.co/LiquidAI/LFM2-8B-A1B-GGUF:latest,open,2025-10-09 08:10:25,,model
ollama/ollama,12548,Running Ollama Cloud Model with SpringBoot,closed,2025-10-09 07:53:21,2025-10-09 18:18:27,
ollama/ollama,12547,"When I use the fine tuned model to use ollama inference, there may be incorrect answers.",open,2025-10-09 07:29:45,,bug
ollama/ollama,12546,convert: add MiniCPM3-4B model support,open,2025-10-09 05:56:02,,model
ollama/ollama,12545,"Revert ""add truncate and shift parameters""",closed,2025-10-09 00:44:54,2025-10-09 00:57:57,
ollama/ollama,12544,server: continue to handle gin.H values until they are removed,closed,2025-10-09 00:36:24,2025-10-14 22:42:27,
ollama/ollama,12543,kvcache: Clean up sliding window state with independent batches,closed,2025-10-08 22:21:46,2025-10-08 23:43:15,
ollama/ollama,12542,Is it normal for the CPU to have high utilization while the GPU remains almost idle?,open,2025-10-08 21:24:01,,bug
ollama/ollama,12541,gpt-oss:20b is slower and slow,open,2025-10-08 21:10:47,,bug
ollama/ollama,12540,DRY out the runner lifecycle code,closed,2025-10-08 21:00:20,2025-10-23 18:20:02,
ollama/ollama,12539,Fine grained tags on ollama.com.,open,2025-10-08 13:34:19,,"feature request,ollama.com"
ollama/ollama,12538,Weird download phenomenon for: ollama run qwen3:8b,closed,2025-10-08 12:30:52,2025-11-01 10:25:30,bug
ollama/ollama,12537,"ollama pull <model> ignores folder settings, environment variables, etc.",closed,2025-10-08 11:38:57,2025-10-26 23:56:32,"question,needs more info"
ollama/ollama,12536,ollama pull model Error,closed,2025-10-08 07:06:20,2025-10-09 01:41:18,question
ollama/ollama,12535,The request for adapting Minicpm-v 4.5 and qwen3-VL models to the new version of ollama,closed,2025-10-08 05:06:23,2025-10-08 11:39:18,model
ollama/ollama,12534,Custom installation of Ollama on windows,closed,2025-10-08 02:25:17,2025-10-26 23:54:52,feature request
ollama/ollama,12533,thinking: turn on thinking mode for all reasoning models,closed,2025-10-08 00:21:10,2025-10-08 23:50:13,
ollama/ollama,12532,Cloud usage stats.,open,2025-10-07 23:53:14,,"feature request,cloud"
ollama/ollama,12531,Update Go version from 1.24.0 to 1.24.6,open,2025-10-07 23:21:31,,
ollama/ollama,12530,docs: improve accuracy of LLM library docs,closed,2025-10-07 23:19:55,2025-10-07 23:21:07,
ollama/ollama,12529,Bring back escape valve for llm libraries and fix Jetpack6 crash,closed,2025-10-07 22:30:37,2025-10-07 23:06:15,
ollama/ollama,12528,Jetson Thor memory release issue part II,closed,2025-10-07 21:46:27,2025-11-06 17:43:02,"bug,nvidia"
ollama/ollama,12527,discover: Disable flash attention for Jetson Xavier (CC 7.2),closed,2025-10-07 21:09:42,2025-10-08 16:56:15,
ollama/ollama,12526,Qwen3VL Cloud Parser and Renderer,closed,2025-10-07 20:50:24,2025-10-13 23:52:33,
ollama/ollama,12525,runner: update metrics,closed,2025-10-07 19:11:16,2025-10-09 22:44:05,
ollama/ollama,12524,Ollama app on mac 0.12.3.  re-sets everytime after shutting down,open,2025-10-07 13:53:06,,"bug,app"
ollama/ollama,12523,Security report submitted,open,2025-10-07 12:14:04,,
ollama/ollama,12522,Qwen3-vl Model adaptation,closed,2025-10-07 11:12:33,2025-10-07 12:52:04,model
ollama/ollama,12521,Hit error 500 when running 0.12.4-rc6 on nvidia jetpack 6,closed,2025-10-07 06:34:22,2025-10-07 23:06:16,"bug,nvidia"
ollama/ollama,12520,thinking: force newer qwen3 models to always use thinking,closed,2025-10-07 05:47:53,2025-10-08 00:21:44,
ollama/ollama,12519,add truncate and shift parameters,closed,2025-10-07 03:32:28,2025-10-09 00:05:06,
ollama/ollama,12518,WIP: stable ordering for tool args,open,2025-10-07 01:25:59,,
ollama/ollama,12517,implement nvml for linux,closed,2025-10-06 23:43:57,2025-10-10 22:15:56,
ollama/ollama,12516,convert: update gpt-oss conversion to split gate up,closed,2025-10-06 23:31:27,2025-10-10 20:25:34,
ollama/ollama,12515,Windows Smart App Control is blocking the installer,closed,2025-10-06 22:57:24,2025-10-06 23:19:30,bug
ollama/ollama,12514,discovery: prevent dup OLLAMA_LIBRARY_PATH,closed,2025-10-06 21:29:10,2025-10-06 21:36:44,
ollama/ollama,12513,win: fix build script,closed,2025-10-06 16:34:20,2025-10-06 21:46:46,
ollama/ollama,12512,Bug: GPU Hang (100% Utilization) on ROCm After Model Reload via /load Command,closed,2025-10-06 15:44:38,2025-10-29 23:11:26,bug
ollama/ollama,12511,No <think> token at start for qwen3,closed,2025-10-06 15:08:11,2025-10-23 21:33:56,bug
ollama/ollama,12510,"when ollama is running background, keeps start & stop NetSetupSvc",open,2025-10-06 09:00:22,,bug
ollama/ollama,12509,openai: refactor to split compat layer and middleware,closed,2025-10-05 21:19:11,2025-10-06 23:22:08,
ollama/ollama,12508,[Performance] embeddinggemma: 164x slowdown with excessive whitespace in text,open,2025-10-05 15:50:19,,bug
ollama/ollama,12507,Make `ollama pull` save download progress,closed,2025-10-05 13:23:31,2025-10-09 01:48:01,feature request
ollama/ollama,12506,Gibberish output at 0.12.3,closed,2025-10-05 04:17:59,2025-10-08 06:08:00,bug
ollama/ollama,12505,GLM 4.6 is unsupported,closed,2025-10-05 03:41:17,2025-10-13 01:15:25,bug
ollama/ollama,12504,Prompt evaluation is MUCH slower using the new Ollama engine,closed,2025-10-05 01:45:14,2025-10-06 18:05:49,bug
ollama/ollama,12503,Marjor security issue with install procedure and lot of non-sense unsafe practices with Linux install,open,2025-10-04 22:13:57,,bug
ollama/ollama,12502,CI: fix win arm CI,closed,2025-10-04 18:19:38,2025-10-04 18:46:45,
ollama/ollama,12501,qwen3-vl supported!,closed,2025-10-04 13:33:54,2025-10-04 13:37:21,feature request
ollama/ollama,12500,How to create a new model without a parameter using a Modelfile and ollama create from an existing parameterized modelÔºü,closed,2025-10-04 11:26:37,2025-10-04 14:13:29,
ollama/ollama,12499,INSTALL.SH: Improve robustness for unreliable connections,open,2025-10-04 09:17:41,,
ollama/ollama,12498,Qwen3 VL 30B A3B,closed,2025-10-04 07:19:17,2025-10-04 10:45:21,model
ollama/ollama,12497,ÈóÆÊ®°ÂûãÁªèÂ∏∏Âá∫ÈîôÊÄé‰πàËß£ÂÜ≥,closed,2025-10-04 07:08:47,2025-10-10 02:08:22,bug
ollama/ollama,12496,Qwen3-Omni,closed,2025-10-04 01:37:45,2025-10-04 10:46:56,model
ollama/ollama,12495,CI: alternate clang compiler for windows,closed,2025-10-03 22:55:28,2025-10-04 16:18:42,
ollama/ollama,12494,llm: Support KV cache quantization with gpt-oss,closed,2025-10-03 22:31:45,2025-10-03 23:31:58,
ollama/ollama,12493,Add /set silentthinking option,open,2025-10-03 21:39:48,,feature request
ollama/ollama,12492,tests: improvements,closed,2025-10-03 21:06:57,2025-10-08 16:51:25,
ollama/ollama,12491,fixed adding nil tensor err,closed,2025-10-03 20:55:36,2025-10-03 21:20:06,
ollama/ollama,12490,Workaround broken NVIDIA iGPU free VRAM data,closed,2025-10-03 18:24:20,2025-10-03 19:17:22,
ollama/ollama,12489,test: add template error test,closed,2025-10-03 18:19:29,2025-10-03 19:05:34,
ollama/ollama,12488,Refreshing llama.cpp to allow running Apertus model,closed,2025-10-03 17:00:10,2025-10-21 17:33:03,
ollama/ollama,12487,ci: place rocm windows in correct runner dir,closed,2025-10-03 14:27:22,2025-10-03 14:28:40,
ollama/ollama,12486,CI: workaround broken clang on windows temporarily,closed,2025-10-03 02:32:14,2025-10-03 03:31:18,
ollama/ollama,12485,ci: fix windows build,closed,2025-10-03 02:09:20,2025-10-03 02:16:01,
ollama/ollama,12484,ci: fix windows build,closed,2025-10-03 01:51:41,2025-10-03 01:59:26,
ollama/ollama,12483,templates: fix crash in improperly defined templates,closed,2025-10-03 00:06:56,2025-10-03 00:25:55,
ollama/ollama,12482,llm: Enable flash attention by default for qwen3 and qwen3moe,closed,2025-10-02 23:56:26,2025-10-03 00:04:10,
ollama/ollama,12481,AMD: block running on unsupported gfx900/gfx906,closed,2025-10-02 21:33:39,2025-10-02 23:53:05,
ollama/ollama,12480,ollama cli doesnt honor the proxy environement,closed,2025-10-02 17:29:46,2025-10-26 23:54:34,"bug,needs more info"
ollama/ollama,12479,libhipblas.so.3 in ROCm 7,closed,2025-10-02 16:42:19,2025-10-02 23:32:34,
ollama/ollama,12478,Proposal: Improve install.sh robustness for unreliable connections (curl --retry options),open,2025-10-02 14:37:03,,feature request
ollama/ollama,12477,Ollama Multiple Trace Generation with Parameter Permutations,closed,2025-10-02 11:41:38,2025-10-08 01:31:48,
ollama/ollama,12476,Official authentication for Ollama HTTP/gRPC endpoints (API key / token / mTLS),open,2025-10-02 05:42:27,,feature request
ollama/ollama,12475,fix panic on bootstrapDevices,closed,2025-10-02 00:29:42,2025-10-02 00:39:29,
ollama/ollama,12474,`num_ctx` incorrect description in documentation.,open,2025-10-01 23:33:40,,documentation
ollama/ollama,12473,llm: Allow overriding flash attention setting,closed,2025-10-01 21:57:41,2025-10-02 19:07:20,
ollama/ollama,12472,Model runner has unexpectedly stopped (GPU Hang) - Framework Desktop,open,2025-10-01 20:37:48,,"bug,amd"
ollama/ollama,12471,routes/client: add web search and fetch,closed,2025-10-01 20:20:01,2025-11-14 21:58:32,
ollama/ollama,12468,[Model Request] Apriel-1.5-15b-Thinker,closed,2025-10-01 13:57:29,2025-10-26 23:28:04,model
ollama/ollama,12466,Facing Model Runner Has Unexpectedly Stopped With The Flan-T5,open,2025-10-01 10:26:43,,bug
ollama/ollama,12465,hf.co/bartowski/nvidia_NVIDIA-Nemotron-Nano-9B-v2-GGUF:Q6_K_L,closed,2025-10-01 03:47:08,2025-10-01 17:59:57,model
ollama/ollama,12464,Alibaba DeepResearch,closed,2025-10-01 03:27:45,2025-10-26 23:54:10,model
ollama/ollama,12463,Can Ollama support a 256K context length?,closed,2025-10-01 01:48:46,2025-10-20 21:13:45,"feature request,app"
ollama/ollama,12460,routes: structured outputs for thinking models /chat endpoint,closed,2025-09-30 21:32:36,2025-10-09 02:13:38,
ollama/ollama,12459,Qwen3-235B outputs <|endoftext|> tokens not specified in modelfile,closed,2025-09-30 20:26:02,2025-10-01 23:33:43,bug
ollama/ollama,12457,bug: CUDA error during cudaMemcpyPeerAsync,open,2025-09-30 18:43:21,,bug
ollama/ollama,12456,Support binding multiple IPs,open,2025-09-30 14:27:44,,feature request
ollama/ollama,12452,db.sqlite-wal keeps plain text history,closed,2025-09-30 08:12:26,2025-10-20 21:33:02,"bug,app"
ollama/ollama,12451,Unable to launch the user interface after installation on a completely offline Windows machine.,open,2025-09-30 06:46:42,,"bug,app"
ollama/ollama,12450,websearch: added local endpoints,closed,2025-09-30 02:59:41,2025-10-01 19:10:21,
ollama/ollama,12446,Unable to download the Windows installer from Poland,closed,2025-09-29 15:32:38,2025-10-13 19:22:16,"bug,needs more info"
ollama/ollama,12444,"Qwen3:4b-instruct keeps giving tokens after <|endoftext|>, also responding with irrelevant content",closed,2025-09-29 10:18:58,2025-10-01 22:51:45,bug
ollama/ollama,12442,CUDA error running gpt-oss:20b on version 0.12.2 on Nvidia Xavier,closed,2025-09-29 02:53:10,2025-10-08 16:56:16,bug
ollama/ollama,12440,Voxtral-Mini-3B-2507 and Audio Support,closed,2025-09-28 22:13:50,2025-11-01 10:24:49,model
ollama/ollama,12436,Option to disable all Cloud and remote Search features,closed,2025-09-28 08:10:42,2025-10-12 12:36:08,feature request
ollama/ollama,12435,ollama pull model Error,closed,2025-09-28 07:56:35,2025-10-13 19:21:48,"bug,needs more info"
ollama/ollama,12433,Add Qwen3 235B-A22B 2507 to Cloud,closed,2025-09-28 02:33:49,2025-10-29 09:27:29,model
ollama/ollama,12432,Qwen3 vs Qwen3-2507 Regression caused by flash attention. AMD ROCM,closed,2025-09-27 18:42:59,2025-10-11 08:59:12,bug
ollama/ollama,12429,500 error,closed,2025-09-27 05:11:09,2025-10-13 19:21:20,"bug,needs more info"
ollama/ollama,12428,Models loading slow since 0.12 version,closed,2025-09-27 01:18:48,2025-11-01 10:24:16,bug
ollama/ollama,12427,add fixes for rocm-7 support,open,2025-09-26 16:36:40,,
ollama/ollama,12426,Cannot run finetuned embeddinggemma GGUF model,open,2025-09-26 15:19:40,,"bug,needs more info"
ollama/ollama,12424,build: fix rocm 6.4 ~ 7.0 docker running failure,closed,2025-09-26 07:46:00,2025-10-02 10:58:58,
ollama/ollama,12423,feat: support compressed content(gzip) with rest api,closed,2025-09-26 06:54:45,2025-11-06 06:53:47,
ollama/ollama,12422,failed to load model vocabulary required for format,closed,2025-09-26 06:26:24,2025-10-06 00:13:27,"bug,needs more info"
ollama/ollama,12416,Please add: CWM (Code World Model) (from Meta),closed,2025-09-25 20:29:38,2025-10-26 23:53:26,model
ollama/ollama,12415,refactor: use builtin max and min,closed,2025-09-25 20:26:08,2025-10-09 23:17:52,
ollama/ollama,12412,Fix AMDGPU_TARGETS filtering condition,closed,2025-09-25 17:49:03,2025-09-26 18:38:48,
ollama/ollama,12411,"iGPU Memory not used, System Memory is filled with loaded model",open,2025-09-25 13:12:14,,"bug,needs more info"
ollama/ollama,12410,GLM-4.5-Air,closed,2025-09-25 10:26:43,2025-10-26 23:52:20,model
ollama/ollama,12403,gpt-oss 120b garbage output since Ollama version 0.11.11/0.12.0,closed,2025-09-24 19:38:58,2025-10-08 16:53:14,bug
ollama/ollama,12397,Qwen3-VL-235B-A22B,closed,2025-09-24 08:20:36,2025-10-29 23:44:31,model
ollama/ollama,12396,Fix the inappropriate thread count for Ollama in the Docker scenario,open,2025-09-24 07:14:12,,
ollama/ollama,12395,when using cloud model ( Ollama Error: Invalid tool usage: mismatch between tool calls and tool results (status code: 400 ),open,2025-09-24 06:52:36,,"bug,cloud"
ollama/ollama,12392,docs: add Ollama App iOS to community integrations,closed,2025-09-24 02:13:21,2025-11-13 06:10:21,
ollama/ollama,12381,embeddings required but some input tokens were not marked as outputs,open,2025-09-23 14:34:33,,bug
ollama/ollama,12380,Recent ollama + qwen3-coder:30b XML error.,closed,2025-09-23 13:42:42,2025-09-23 18:11:31,bug
ollama/ollama,12377,refactor: using testing.B.Loop,closed,2025-09-23 08:07:59,2025-10-10 20:25:29,
ollama/ollama,12375,Improvement ideas regarding KV cache and Token ID.,closed,2025-09-22 23:32:45,2025-10-21 07:49:57,feature request
ollama/ollama,12371,fix: create with nested directories,open,2025-09-22 19:47:21,,
ollama/ollama,12370,"Cloud model 400 Bad Request: raw mode does not support template, system, or context",closed,2025-09-22 17:57:47,2025-10-21 08:10:05,"bug,cloud"
ollama/ollama,12362,JSON reply schema is ignored by Cloud model,open,2025-09-21 05:10:46,,"bug,cloud"
ollama/ollama,12359,Qwen3 crashes on gfx1150 iGPU,open,2025-09-20 21:04:37,,"bug,amd"
ollama/ollama,12355,Granite Docling 258m,open,2025-09-20 15:23:27,,model
ollama/ollama,12349,Qwen3-Next,open,2025-09-19 23:05:27,,model
ollama/ollama,12347,simplify expand path,open,2025-09-19 20:14:28,,
ollama/ollama,12346,Ollama Windows App - Downloaded Models Only,closed,2025-09-19 19:18:02,2025-10-02 18:51:07,"feature request,app"
ollama/ollama,12342,Bug: inconsistent to use VRAM and GTT of  iGPU of  AMD Ryzen AI Processor ,open,2025-09-19 07:30:03,,"bug,needs more info"
ollama/ollama,12341,"With v0.11.x ""CUDA error: no kernel image is available for execution on the device"" shows up; w/ v0.10.1 not",closed,2025-09-19 07:14:44,2025-10-06 00:11:50,bug
ollama/ollama,12340,ollama chat history issue,open,2025-09-19 05:09:08,,"bug,needs more info"
ollama/ollama,12335,Update linux.md,open,2025-09-18 19:10:26,,
ollama/ollama,12334,Error An existing connection was forcibly closed by the remote host,closed,2025-09-18 17:32:19,2025-11-01 10:21:21,bug
ollama/ollama,12331,CI: Enable GitHub Actions CI for ppc64le (Power architecture) support,open,2025-09-18 11:35:08,,feature request
ollama/ollama,12330,Magistral-Small-2509,closed,2025-09-18 11:00:16,2025-10-26 23:51:14,model
ollama/ollama,12328,Ollama is still broken for Thor and Spark,closed,2025-09-18 10:06:23,2025-11-06 00:16:36,bug
ollama/ollama,12327,SpikingBrain 7b,closed,2025-09-18 04:50:25,2025-11-03 15:40:20,model
ollama/ollama,12320,Ollama stopped working after install ROCm,open,2025-09-17 17:15:22,,"bug,linux,amd"
ollama/ollama,12314,"Debian 13, rocm 7.0 --> [signal SIGSEGV: segmentation violation",open,2025-09-17 07:03:06,,bug
ollama/ollama,12312,Configurable ollamaapp.exe folder alternative,closed,2025-09-17 05:54:42,2025-10-06 07:54:35,"feature request,app,windows"
ollama/ollama,12303,offloaded 0/35 layers to GPU on gfx1103,open,2025-09-16 06:28:36,,"bug,amd"
ollama/ollama,12302,parser: tidy up parameter/message parsing,open,2025-09-16 01:24:06,,
ollama/ollama,12297,gemma3: make embedding non-causal,closed,2025-09-15 22:25:55,2025-10-28 02:54:08,
ollama/ollama,12294,Ollama GUI app in macOS 26 unable to run if variable OLLAMA_HOST is set to a docker container,open,2025-09-15 20:37:21,,"bug,app,macos"
ollama/ollama,12287,docs(wsl): add WSL dev quickstart + helper script (serve-dev-wsl.sh),open,2025-09-14 22:07:56,,
ollama/ollama,12283,Jetson Thor memory release issue,closed,2025-09-14 12:04:28,2025-10-04 21:24:49,"bug,nvidia"
ollama/ollama,12277,install: break up windows and linux bundles,open,2025-09-12 21:52:17,,
ollama/ollama,12268,Explore ways to optimize installation size,open,2025-09-12 15:04:58,,"feature request,install"
ollama/ollama,12264,Ollama serve crashes immediately when CUDA SDK exists but NVIDIA GPU is missing,closed,2025-09-12 08:09:42,2025-11-19 00:33:34,"bug,windows,nvidia"
ollama/ollama,12255,chore: cleanup ml interface,closed,2025-09-11 20:53:30,2025-10-28 19:08:49,
ollama/ollama,12254,Add AMD RX 9000-series to supported GPU documentation,open,2025-09-11 19:52:27,,
ollama/ollama,12251,Zoom in and out feature for MacOS application,closed,2025-09-11 14:10:44,2025-10-02 19:02:45,"feature request,app"
ollama/ollama,12245,Update GGML to b6646 - drop MacOS v12 and v13 support,closed,2025-09-10 23:32:41,2025-10-02 21:47:10,
ollama/ollama,12242,feat: add dimensions field to embed requests,closed,2025-09-10 19:47:52,2025-09-11 17:36:10,
ollama/ollama,12240,uninstaller,open,2025-09-10 17:25:03,,feature request
ollama/ollama,12239,[Performance] `embeddinggemma` seems to be slow,open,2025-09-10 15:17:31,,
ollama/ollama,12235,Add Ollama Bash Lib to README Libraries list,closed,2025-09-09 22:06:17,2025-11-02 23:44:56,
ollama/ollama,12227,Add support for agentic models like ui-tars for vision tasks besides captioning and OCR.,open,2025-09-09 13:19:47,,feature request
ollama/ollama,12226,Ollama granite3.1-moe:1b model cannot detect function call as a separate field,closed,2025-09-09 12:34:45,2025-10-06 00:08:56,bug
ollama/ollama,12219,runner: enable structured outputs for gpt-oss,closed,2025-09-08 22:17:54,2025-11-02 23:46:26,
ollama/ollama,12218,Black screen + CUDA error on Windows after model starts generating,closed,2025-09-08 19:37:47,2025-11-14 01:38:36,"bug,needs more info"
ollama/ollama,12214,Add a setting to hide the Dock icon on macOS,open,2025-09-08 10:06:51,,"feature request,app"
ollama/ollama,12212,Add user-friendly error handling for port binding failures on Windows,closed,2025-09-07 23:20:05,2025-11-02 23:49:06,
ollama/ollama,12209,Version 11 bombing out and responds with GGGGGGGGGGGGGGG,open,2025-09-07 10:51:11,,"bug,nvidia,needs more info"
ollama/ollama,12206,Port Collison Bug (Windows),open,2025-09-07 04:05:44,,"bug,app"
ollama/ollama,12203,gpt-oss:120b - thinking being returned in tool calls,open,2025-09-06 13:25:04,,bug
ollama/ollama,12202,More information about the model in the local api,closed,2025-09-06 13:17:25,2025-10-06 00:08:09,feature request
ollama/ollama,12201,Ollama App: Toggle Dark/Light Theme,open,2025-09-06 12:32:01,,feature request
ollama/ollama,12187,GPT-OSS not completing tool calls,open,2025-09-04 23:36:49,,bug
ollama/ollama,12180,Can't disable thinking on deepseek-r1 with openai sdk,closed,2025-09-04 14:17:37,2025-10-06 00:07:34,feature request
ollama/ollama,12179,"Model stuck in ""Stopping..."" state",closed,2025-09-04 12:53:55,2025-10-26 23:48:01,bug
ollama/ollama,12173,Unable to load cudart library /usr/lib/x86_64-linux-gnu/libcuda.so.570.169: cuda driver library init failure: 802,closed,2025-09-03 15:18:56,2025-10-26 23:47:45,bug
ollama/ollama,12154,Change models folder via ollama gui does not change the models folder for the ollama command line (solved),closed,2025-09-02 16:48:32,2025-09-02 17:49:49,"bug,app"
ollama/ollama,12152,LLM (llama2:7b) spams '#' in output,closed,2025-09-02 14:35:59,2025-10-26 23:47:19,bug
ollama/ollama,12149,[Model Request] Support new Apertus model,open,2025-09-02 09:32:45,,model
ollama/ollama,12148,Apple's FastVLM support,open,2025-09-02 08:22:53,,"feature request,model"
ollama/ollama,12145,Support for BGE-VL-Screenshot Model as Image Vector Embedding Model,closed,2025-09-02 01:10:50,2025-10-06 00:02:58,model
ollama/ollama,12143,How to disable low vram mode. or change the threshold to 8 GiB?,closed,2025-09-01 12:39:21,2025-10-26 23:46:46,question
ollama/ollama,12140,Qwen-Image-Edit,closed,2025-09-01 08:19:53,2025-10-26 23:45:59,feature request
ollama/ollama,12137,openbmb/minicpm-v4.5:8b,open,2025-09-01 05:42:23,,bug
ollama/ollama,12136,CUDA Detection issues,closed,2025-09-01 01:12:57,2025-11-12 16:04:24,"bug,linux,nvidia"
ollama/ollama,12135,Ollama 'run' command hangs when loading Llama3 and other large models,closed,2025-08-31 19:39:54,2025-10-26 23:45:02,bug
ollama/ollama,12132,Ollama Turbo gpt-oss:20b leaking template into the tool call?,closed,2025-08-31 15:52:05,2025-11-08 01:16:40,"bug,cloud"
ollama/ollama,12131,gpt-oss-120b failing with Memory access fault by GPU node-1,closed,2025-08-31 13:52:04,2025-10-29 09:54:06,bug
ollama/ollama,12128,self-built Ollama not using GPU,closed,2025-08-30 23:14:47,2025-10-26 23:43:29,bug
ollama/ollama,12126,[Model Request] NVIDIA-Nemotron-Nano-9B-v2,closed,2025-08-30 16:53:47,2025-11-01 10:17:14,model
ollama/ollama,12124,Fail Install on Raspbian OS,closed,2025-08-30 15:10:36,2025-10-26 23:44:20,bug
ollama/ollama,12118,feat: add OLLAMA_IP_ALLOWLIST env for security,closed,2025-08-29 16:29:53,2025-11-02 23:50:50,
ollama/ollama,12113,ollama panic when run gpt-oss:120b with OLLAMA_FLASH_ATTENTION=1 on V100,closed,2025-08-29 08:38:47,2025-10-08 16:58:05,bug
ollama/ollama,12090,Use runners for GPU discovery,closed,2025-08-26 20:30:21,2025-10-01 22:12:32,
ollama/ollama,12085,PowerPC Optimization: Enable MMA in ollama,closed,2025-08-26 11:47:40,2025-11-04 15:03:52,
ollama/ollama,12082,Option to set a system prompt directly in the Ollama app,open,2025-08-26 05:52:39,,"feature request,app"
ollama/ollama,12078,[model] Support MiniCPM-V 4.5,open,2025-08-25 20:32:58,,
ollama/ollama,12074,Seed-OSS-36B-Instruct-GGUF:Q4_K_M,open,2025-08-25 16:10:31,,model
ollama/ollama,12071,Windows: AMD gfx1103 (Radeon 780M / RX 7700S) not accepted by Ollama GPU backend (gfx1103),open,2025-08-25 14:00:32,,"bug,windows,amd"
ollama/ollama,12070,PowerPC:Fix for issue #12076,closed,2025-08-25 11:47:36,2025-11-04 15:03:20,
ollama/ollama,12064,Tool call parsing errors,open,2025-08-25 00:45:02,,bug
ollama/ollama,12057,Blank window when running ollama via GUI (not via CLI),open,2025-08-23 21:58:00,,bug
ollama/ollama,12053,Regression: ollama serve only listens on localhost in v0.11.5/0.11.6 (no longer binds to external interfaces),closed,2025-08-23 17:17:49,2025-08-23 18:08:24,bug
ollama/ollama,12050,Couldn't open a window after update,closed,2025-08-23 14:17:54,2025-11-13 08:43:50,"bug,app"
ollama/ollama,12037,Excessive time to first token with new engine and partial offload (prompt eval about 9x slower),closed,2025-08-22 20:20:53,2025-10-30 20:53:11,bug
ollama/ollama,12036,nvcc fatal   : Unsupported gpu architecture 'compute_35' (fixed with work around),closed,2025-08-22 18:24:18,2025-11-16 13:28:09,"bug,linux,nvidia,build"
ollama/ollama,12026,Ollama Service Killed During ‚Äôconverting model‚Äò When Adding Fine-Tuned Model on Linux,closed,2025-08-22 06:31:21,2025-09-06 10:26:56,bug
ollama/ollama,12020,Add `OLLAMA_SKIP_UPDATE_CHECK` variable which disables periodic checking of updates.,closed,2025-08-22 01:05:30,2025-11-16 03:17:53,
ollama/ollama,12014,qwen3-embedding crashes starting from 0.11.5,closed,2025-08-21 18:37:26,2025-10-27 21:49:20,"bug,embeddings"
ollama/ollama,12005,Show the model name below when query is answered from ollama desktop chat,open,2025-08-21 08:52:08,,"feature request,app"
ollama/ollama,12003,Only show downloaded models in airplane mode,closed,2025-08-21 07:58:51,2025-10-07 19:23:16,"feature request,app"
ollama/ollama,11991,Ollama gpt-oss:20b fail on tool call in Cline,open,2025-08-20 15:22:03,,"bug,tools,gpt-oss"
ollama/ollama,11981,Update README.md,closed,2025-08-20 09:21:38,2025-11-06 06:48:43,
ollama/ollama,11972,"""Restart to update"" doesn't on Mac",open,2025-08-20 00:04:10,,"bug,app"
ollama/ollama,11939,`gpt-oss` Uses `100% CPU`,closed,2025-08-16 13:15:31,2025-08-21 19:39:08,bug
ollama/ollama,11937,Save/Export Chat in Ollama Chat App,open,2025-08-16 11:17:52,,"feature request,app"
ollama/ollama,11931,uninstall a model & mark installed,open,2025-08-16 03:45:30,,"feature request,app"
ollama/ollama,11925,Download before deleting old installation,open,2025-08-15 15:59:12,,
ollama/ollama,11918,test: Add comprehensive edge case tests for bytes formatting functions,closed,2025-08-15 10:30:29,2025-11-02 23:54:56,
ollama/ollama,11911,Please allow us to pass through GBNF directly to llama.cpp,closed,2025-08-15 03:34:52,2025-11-13 10:44:18,feature request
ollama/ollama,11905,Local mode instead of Airplane mode,open,2025-08-14 20:56:45,,"feature request,app"
ollama/ollama,11897,Cogito v2,closed,2025-08-14 09:38:27,2025-11-20 14:34:35,model
ollama/ollama,11895,model: using reflect.TypeFor,closed,2025-08-14 05:31:36,2025-10-18 12:13:33,
ollama/ollama,11877,Add KDeps to Community Integrations,closed,2025-08-13 05:30:58,2025-11-16 03:19:03,
ollama/ollama,11869,Updating DSET from 3.12. to 3.14,open,2025-08-12 19:08:03,,app
ollama/ollama,11855,[#6308] Add missing redirect check if direct URL is already Present,open,2025-08-11 11:34:58,,
ollama/ollama,11854,Can't use latest rocm Ôºà6.4.3Ôºâ,open,2025-08-11 11:07:04,,"bug,linux,amd"
ollama/ollama,11850,In Ollama app after first reply second prompt simply stucks - no response at all,open,2025-08-11 09:16:01,,bug
ollama/ollama,11847,"In the Ollam app when editing a message, a new chat isn't created so you don't lose messages and replies.",open,2025-08-10 23:56:34,,"feature request,app"
ollama/ollama,11845,add cuda 13.x support,closed,2025-08-10 14:33:18,2025-11-05 20:44:25,
ollama/ollama,11837,Updated to show ram usage,closed,2025-08-10 02:19:35,2025-11-02 23:55:57,
ollama/ollama,11835,Vulkan based on #9650,closed,2025-08-09 20:01:39,2025-10-14 17:59:58,
ollama/ollama,11829,Model list showing on device and downloadable status.,closed,2025-08-09 07:41:09,2025-10-07 18:44:55,"feature request,app"
ollama/ollama,11822,add model benchmark,closed,2025-08-08 21:56:37,2025-10-29 00:11:47,
ollama/ollama,11817,build: update go stdlib to 1.24.6,open,2025-08-08 18:51:07,,
ollama/ollama,11810,Clarification/feature request: utilization‚Äëaware multi‚ÄëGPU scheduling with a single model instance (and multi‚Äëmodel placement),open,2025-08-08 13:38:34,,feature request
ollama/ollama,11806,Accessibility issues with screen readers in Ollama for Windows,open,2025-08-08 07:48:59,,"bug,app"
ollama/ollama,11805,Tool call arguments structure inconsistent ‚Äî extra nesting added in certain cases,open,2025-08-08 06:55:46,,bug
ollama/ollama,11800,gpt-oss:20b and gpt-oss:120b tool use throws unexpected error format in response (status code: 500),closed,2025-08-08 00:44:33,2025-08-09 00:04:50,bug
ollama/ollama,11798,Feature Request: Add Audio Input Support for Multimodal Models,open,2025-08-07 22:52:24,,
ollama/ollama,11792,Feature Request: Support for Intel Arc B580 GPU Acceleration,open,2025-08-07 18:31:52,,"feature request,intel"
ollama/ollama,11791,Gui opening while called by bash script,closed,2025-08-07 18:28:12,2025-10-02 19:15:52,"feature request,app"
ollama/ollama,11774,[BUG] Intel Iris Xe GPU (i5-13500 / Raptor Lake) not detected in Docker on Ubuntu 24.04,open,2025-08-07 06:44:17,,"bug,intel"
ollama/ollama,11772,use cpu to offload moe weights to reduce the VRAM usage.,open,2025-08-07 05:08:58,,feature request
ollama/ollama,11766,Support reasoning effort for gpt-oss,open,2025-08-07 01:56:22,,"feature request,app"
ollama/ollama,11762,Font size and Gpu selection.,open,2025-08-07 00:37:30,,"feature request,app"
ollama/ollama,11749,Accessing web search tool using API,closed,2025-08-06 17:02:33,2025-11-08 01:19:29,feature request
ollama/ollama,11743,"[Feature Request] Add separate ""Models"" sidebar tab in Ollama GUI for installed & available models",open,2025-08-06 14:41:36,,"feature request,app"
ollama/ollama,11740,Add Strands Agents to Libraries that support Ollama in README.md,closed,2025-08-06 14:00:08,2025-11-03 00:01:28,
ollama/ollama,11728,"gpt-oss:20b execution error: ""SIGABRT: abort""",open,2025-08-06 09:07:13,,bug
ollama/ollama,11726,Is Ollama APP (GUI) open sourceÔºü,closed,2025-08-06 08:28:49,2025-11-04 22:24:57,
ollama/ollama,11721,Error: 500 Internal Server Error: llama runner process has terminated: exit status 2,closed,2025-08-06 06:47:31,2025-08-07 03:11:10,bug
ollama/ollama,11715,chore: remove dependencies on 16-bit floats,open,2025-08-06 03:50:42,,
ollama/ollama,11714,gpt-oss 20b gguf model fail to run,open,2025-08-06 03:43:23,,bug
ollama/ollama,11693,Documentation on Thinking,open,2025-08-05 20:42:30,,feature request
ollama/ollama,11691,Structured output with OpenAI SDK and gpt-oss:20b not working,open,2025-08-05 20:21:17,,"bug,gpt-oss"
ollama/ollama,11676,Ollama not using NVIDIA GPUs with gpt-oss models,closed,2025-08-05 18:01:08,2025-09-01 19:34:56,bug
ollama/ollama,11652,rocblaslt error: Could not load /usr/local/lib/ollama/rocm/hipblaslt/library/TensileLibrary_lazy_gfx1200.dat,open,2025-08-03 18:30:43,,bug
ollama/ollama,11645,Ollama Desktop App add Custom AI API URL,open,2025-08-02 22:11:14,,"feature request,app"
ollama/ollama,11642,docs: add info about log levels to troubleshooting.md,open,2025-08-02 18:03:33,,
ollama/ollama,11634,Clarify license of the new ollama app,closed,2025-08-01 15:28:24,2025-11-04 22:24:43,
ollama/ollama,11625,Ollama 0.10.1 falsely claims an update is available,open,2025-08-01 09:04:34,,bug
ollama/ollama,11621,Qwen3-Coder missing Tools and FIM support in template,closed,2025-08-01 07:37:07,2025-09-23 21:16:29,bug
ollama/ollama,11617,docs: fix Gentoo package link,open,2025-08-01 02:23:37,,
ollama/ollama,11609,Cross Platform GUI App,open,2025-07-31 17:13:20,,feature request
ollama/ollama,11608,Tool calling is less consistent than LMStudio with the same models,open,2025-07-31 16:07:30,,bug
ollama/ollama,11604,New UI Opens no matter what on app open,open,2025-07-31 13:48:32,,"feature request,app"
ollama/ollama,11603,Ollama UI support for MCP tools,open,2025-07-31 10:58:40,,feature request
ollama/ollama,11563,Support for zai-org/GLM-4.5 (Thinking & Non-Thinking Modes + Tool Use),closed,2025-07-29 01:38:07,2025-09-01 19:09:59,model
ollama/ollama,11547,verifying module: checksum mismatch,open,2025-07-27 14:42:46,,
ollama/ollama,11544,Organize links by language.,closed,2025-07-27 10:54:48,2025-11-03 00:02:59,
ollama/ollama,11536,readme: add ChatFlex to community integrations,closed,2025-07-26 12:10:21,2025-10-17 09:19:16,
ollama/ollama,11532,Add link to `Async::Ollama`.,closed,2025-07-26 01:13:14,2025-11-03 20:20:16,
ollama/ollama,11524,Create uninstall.sh,open,2025-07-25 13:51:06,,
ollama/ollama,11497,"Ollama counts cached memory as used, not allowing models to run even though there is enough memory available.",closed,2025-07-22 20:48:53,2025-07-26 10:09:42,bug
ollama/ollama,11458,keep_alive Parameter Ignored When Using OpenAI SDK with Ollama API,closed,2025-07-17 10:47:22,2025-08-11 22:16:55,bug
ollama/ollama,11451,"GPU not detected on Ryzen AI 300 (gfx1150) with Dynamic VRAM, but works with Fixed VRAM",open,2025-07-16 23:35:41,,feature request
ollama/ollama,11432,voxtral,open,2025-07-15 20:08:25,,model
ollama/ollama,11413,runner: Pass custom thread count to backend via env varibale,open,2025-07-14 11:36:25,,
ollama/ollama,11401,Model not found. Try pulling it first,closed,2025-07-13 02:31:50,2025-07-13 06:24:15,bug
ollama/ollama,11393,Add health check into Dockerfile,closed,2025-07-12 10:51:24,2025-07-12 11:25:21,
ollama/ollama,11389,"üéØ Complete Production-Ready Reranking Implementation - 20x Performance, 100% Test Success",closed,2025-07-12 04:37:24,2025-10-11 19:47:21,
ollama/ollama,11378,Right clicking ‚ÄúView logs‚Äù in the app tray opens a blank command line.,closed,2025-07-11 14:56:49,2025-11-05 00:33:39,"bug,windows,needs more info"
ollama/ollama,11371,README: Add hle-eval-ollama to list of terminal integrations,closed,2025-07-11 10:02:22,2025-11-06 07:04:30,
ollama/ollama,11352,fix(api): align /v1/completions behavior with OpenAI's implementation,open,2025-07-10 03:05:05,,
ollama/ollama,11351,Update README.md,closed,2025-07-10 01:43:11,2025-11-06 07:36:59,
ollama/ollama,11344,An internal error occurred during the installation of OllamaSetup.exe on Windows 11 (x64),closed,2025-07-09 13:00:57,2025-07-12 13:45:27,"bug,windows,needs more info,install"
ollama/ollama,11337,Progressive streaming output versus static output,closed,2025-07-09 07:49:05,2025-10-27 00:38:01,feature request
ollama/ollama,11321,Settings window doesn't properly cleanup when closed (MacOS),closed,2025-07-07 15:04:03,2025-11-04 16:53:06,"bug,app,macos,needs more info"
ollama/ollama,11297,qwen2.5vl model crashes on small image input (height or width < 28px),closed,2025-07-04 09:11:02,2025-11-05 13:01:14,bug
ollama/ollama,11276,"Gemma 3 QAT 12b/27b issue ""</end_of_turn>"" at the end of generation",open,2025-07-02 21:48:07,,bug
ollama/ollama,11249,"feat: expose Ollama native parameters (think, keep_alive) through OpenAI API",open,2025-06-30 23:36:54,,
ollama/ollama,11222,Gemma3n is not Multimodal,closed,2025-06-27 08:31:52,2025-07-01 18:13:56,bug
ollama/ollama,11218,added bitnet support,open,2025-06-27 03:50:10,,
ollama/ollama,11200,ci: create nightly build for caching,open,2025-06-25 15:31:39,,
ollama/ollama,11199,Request for Support of AMD Ryzen AI Platform NPU,open,2025-06-25 15:30:15,,feature request
ollama/ollama,11161,Feature: Model Export and Import,open,2025-06-22 20:44:54,,
ollama/ollama,11160,Enable Intel GPU support with SYCL backend,open,2025-06-22 17:23:21,,
ollama/ollama,11159,Add model eval metrics to /metrics,open,2025-06-22 07:47:23,,
ollama/ollama,11129,runner: add flag to disable context shifting,closed,2025-06-19 00:52:16,2025-10-16 22:57:06,
ollama/ollama,11043,Please keep`Q6_K` quantizations support in Ollama,open,2025-06-11 01:04:57,,feature request
ollama/ollama,11042,server: model info caching system for improved performance,closed,2025-06-10 23:25:08,2025-11-06 07:39:08,
ollama/ollama,11032,Can't Disable Think Mode of Qwen3 and DeepSeek,closed,2025-06-10 02:07:25,2025-06-10 18:03:56,bug
ollama/ollama,10993,"crash on Radeon 8060S Graphics, gfx1151 on windows",open,2025-06-06 06:08:29,,"bug,windows,amd"
ollama/ollama,10956,Garbage output when running llama3.2-vision:11b,open,2025-06-03 15:23:17,,bug
ollama/ollama,10946,Add swaggo annotations,closed,2025-06-02 06:12:52,2025-11-06 07:47:02,
ollama/ollama,10945,KV Cache Quantization breaks Gemma3,closed,2025-06-02 03:56:27,2025-10-10 17:58:41,bug
ollama/ollama,10924,Discord invites don't work,closed,2025-05-31 02:24:43,2025-11-22 21:11:05,
ollama/ollama,10906,Recommended Citation Format for Ollama,open,2025-05-30 02:09:02,,
ollama/ollama,10896,Add multi-model pull and multi-path storage support,closed,2025-05-29 09:06:06,2025-10-22 01:27:39,
ollama/ollama,10859,Investigate V100 Flash Attention Implementation,closed,2025-05-26 04:23:40,2025-10-08 16:57:36,feature request
ollama/ollama,10844,discover/gpu.go: Add Support for Distributed Inferencing (continued),open,2025-05-24 15:58:36,,
ollama/ollama,10823,Ollama not using GPU when CURLing local API,closed,2025-05-23 02:10:22,2025-06-02 13:01:55,bug
ollama/ollama,10792,Gemma 3n,open,2025-05-21 05:55:03,,model
ollama/ollama,10750,fix: cross-domain authentication token exposure,open,2025-05-16 20:34:49,,
ollama/ollama,10724,envconfig: support config.env file for environment variables,open,2025-05-16 03:47:38,,feature request
ollama/ollama,10714,Community Made VS Code Extention,open,2025-05-15 16:01:53,,feature request
ollama/ollama,10699,official batching support,closed,2025-05-13 23:48:40,2025-05-14 00:04:20,feature request
ollama/ollama,10683,Fix typos,closed,2025-05-13 08:53:11,2025-11-16 04:22:29,
ollama/ollama,10676,"Add gfx1200 & gfx1201 support on windows, drop EOL gfx900 and gfx906 ",open,2025-05-12 20:38:05,,
ollama/ollama,10667,Axelera Metis card support,open,2025-05-11 18:31:53,,feature request
ollama/ollama,10597,"Unloading model doesn't free all GPU memory (v0.6.8, Radeon RX 7900 XTX, Windows 11)",open,2025-05-06 21:45:56,,"bug,windows,needs more info"
ollama/ollama,10584,add thinking support to the api and cli ,closed,2025-05-06 07:07:59,2025-05-29 02:38:52,
ollama/ollama,10537,docs: added OpenLLMetry link,open,2025-05-02 13:57:01,,
ollama/ollama,10458,"Qwen3 MoE 30b-a3b, poor performance and Low GPU utilization issue",open,2025-04-29 02:13:09,,bug
ollama/ollama,10456,how can i disable thinking mode?,closed,2025-04-29 02:01:13,2025-05-30 10:15:40,feature request
ollama/ollama,10453,server/python_tools: add python tool parsing logic,closed,2025-04-29 00:11:05,2025-10-21 20:42:36,
ollama/ollama,10430,Adding support for amd new GPUS 9070 and 9070 XT,open,2025-04-27 21:34:03,,"feature request,windows"
ollama/ollama,10392,Gemma3 add support for do_pan_and_scan,closed,2025-04-24 13:40:38,2025-11-20 04:00:56,feature request
ollama/ollama,10385,Add Qwen2.5-VL support,closed,2025-04-23 23:16:12,2025-05-14 03:58:03,
ollama/ollama,10347,function Pull off Registry,open,2025-04-20 01:23:56,,
ollama/ollama,10337,Add Bitnet.cpp engine,open,2025-04-18 13:45:43,,feature request
ollama/ollama,10333,CLI: image path not recognized correctly,open,2025-04-18 02:15:41,,good first issue
ollama/ollama,10331,Client2 Feedback,open,2025-04-18 00:42:48,,
ollama/ollama,10322,Add support Intel GPU by OneApi /SYCL,open,2025-04-17 10:31:26,,
ollama/ollama,10292,discover: Support cgroups cores and memory limitations,closed,2025-04-15 23:32:25,2025-11-18 00:13:03,
ollama/ollama,10283,Zeroconf (mDNS/Bonjour) Support for LAN Discovery,open,2025-04-15 07:37:31,,feature request
ollama/ollama,10254,FreeBSD patches,open,2025-04-13 01:31:42,,
ollama/ollama,10252,Add account deletion + download-my-data feature to ollama.com (GDPR/DPA compliance),open,2025-04-12 20:01:58,,ollama.com
ollama/ollama,10222,Support Jinja chat templates,open,2025-04-10 19:57:11,,feature request
ollama/ollama,10165,Capitalize Ollama in `ollama` help description,open,2025-04-07 18:16:07,,"feature request,good first issue"
ollama/ollama,10127,panic: failed to decode batch: could not find a kv cache slot (length: 6656),closed,2025-04-04 16:39:08,2025-10-08 23:43:15,bug
ollama/ollama,10097,Add an easy way to list all models and their capabilities,open,2025-04-02 23:38:12,,feature request
ollama/ollama,10071,"ollama run dimavz/whisper-tiny Error: Post ""http://127.0.0.1:11434/api/generate"": EOF",closed,2025-04-01 08:27:05,2025-04-01 08:55:40,bug
ollama/ollama,10050,Extremely Slow Model Download Speed Despite Fast Internet Connection (Windows 11),closed,2025-03-30 13:58:57,2025-03-31 09:54:05,bug
ollama/ollama,9999,[ENHANCE] Add Ubuntu Support for AMD Ryzen AI 9 HX 370 w/ Radeon 890M (gfx1150),open,2025-03-26 15:04:44,,"feature request,amd"
ollama/ollama,9991,server: Improve download reliability in bandwidth-constrained environments.,open,2025-03-26 04:21:15,,
ollama/ollama,9954,`install.sh` : add `OLLAMA_INSTALL_ROCM` to make ROCM installation optional ,closed,2025-03-23 13:47:15,2025-11-06 11:38:46,
ollama/ollama,9877,OLLAMA_MODELS directive not respected,closed,2025-03-19 03:29:21,2025-04-13 20:11:40,bug
ollama/ollama,9846,"Error: digest mismatch, file must be downloaded again: want sha256:8de95da68dc485c0889c205384c24642f83ca18d089559c977ffc6a3972a71a8, got sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855",open,2025-03-18 02:08:44,,"bug,networking"
ollama/ollama,9831,Any news on Pixtral 12B support?,closed,2025-03-17 20:19:39,2025-10-26 23:25:49,feature request
ollama/ollama,9812,AMD RX9070/9070XT support,closed,2025-03-17 09:38:30,2025-07-05 21:03:44,feature request
ollama/ollama,9809,"Model Gemma3:27b causes ""panic: failed to sample token: no tokens to sample from""",closed,2025-03-17 07:57:02,2025-03-17 16:56:40,bug
ollama/ollama,9787,Make ollama can run multiple models parallel,closed,2025-03-15 17:20:50,2025-03-17 21:16:22,feature request
ollama/ollama,9781,"`curl` missing from Ollama Docker image, causing healthcheck failures",closed,2025-03-15 10:01:06,2025-03-26 13:09:02,bug
ollama/ollama,9759,ollama run on 910b,open,2025-03-14 07:59:57,,"feature request,gpu"
ollama/ollama,9683,Gemma 3 4B & 12B is very slow when KV Cache quantization is enabled,closed,2025-03-12 10:32:37,2025-10-03 21:46:02,bug
ollama/ollama,9659,Compatibility with new OpenAI responses API,open,2025-03-11 19:24:45,,feature request
ollama/ollama,9650,Vulkan support  (replacing pull/5059),closed,2025-03-11 13:18:23,2025-10-15 20:28:45,
ollama/ollama,9648,OLLAMA_NOHISTORY=1 not working as environment variable,closed,2025-03-11 11:46:47,2025-03-12 00:57:30,bug
ollama/ollama,9647,Phi4 14b with tool calling and full quantization,closed,2025-03-11 10:11:00,2025-03-21 21:49:53,model
ollama/ollama,9609,Fix out-of-bounds access to digest,closed,2025-03-09 11:15:33,2025-10-18 03:34:36,
ollama/ollama,9605,EXAONE fails to run with quantized KV cache,closed,2025-03-09 02:36:45,2025-10-12 12:34:45,bug
ollama/ollama,9559,SmolVLM,open,2025-03-07 01:06:48,,model
ollama/ollama,9547,server: allow dynamic token generation limit with num_predict==-2,open,2025-03-06 11:46:23,,
ollama/ollama,9546,server: add num_parallel to allow per-model control,open,2025-03-06 11:38:58,,
ollama/ollama,9517,Ollama detects the GPU but still uses the CPU.,closed,2025-03-05 12:17:32,2025-03-17 15:22:44,bug
ollama/ollama,9503,NVIDIA GPU drivers not loaded on Jeston Orin Nano,open,2025-03-04 21:23:16,,"bug,linux,nvidia"
ollama/ollama,9444,Error: listen tcp 127.0.0.1:11434: bind: An attempt was made to access a socket in a way forbidden by its access permissions.,closed,2025-03-01 15:49:16,2025-03-01 17:33:44,
ollama/ollama,9409,Configuring Ollama to Use a Custom Model Registry,open,2025-02-28 03:11:30,,
ollama/ollama,9387,phi4 multimodal and mini instruct support,open,2025-02-27 04:28:38,,model
ollama/ollama,9321,server: Add `OLLAMA_NUM_PULL_PARTS` environment variable to reduce stall occurrences,open,2025-02-24 19:28:28,,
ollama/ollama,9286,"Ability to specify GPU priority for model splitting, and don't split model unless needed",closed,2025-02-22 08:30:14,2025-04-13 16:13:57,feature request
ollama/ollama,9282,runner: enable returning more info from runner processing,open,2025-02-22 00:48:53,,
ollama/ollama,9220,"cannot be used with preferred buffer type ROCm_Host, using CPU instead",closed,2025-02-19 11:12:04,2025-03-04 16:26:30,
ollama/ollama,9187,ü™ü Windows 11 24H2 | Smart App Control Blocking Ollama | Unsafe,closed,2025-02-18 07:07:55,2025-07-04 22:09:33,"bug,windows,install"
ollama/ollama,9166,Unable to use GPU at all on windows,closed,2025-02-17 08:50:35,2025-10-06 00:23:51,bug
ollama/ollama,9156,Add support for parallel_tool_calls,open,2025-02-16 20:49:48,,feature request
ollama/ollama,9152,Remove partly downloaded model in case there are no space left on device,closed,2025-02-16 17:59:09,2025-10-06 00:22:18,feature request
ollama/ollama,9137,Ollama API Mixes Reasoning Process with Final Output in content Field,open,2025-02-15 15:18:02,,"bug,api"
ollama/ollama,9124,can i use nvidia tesla k80 gpu run a model?,closed,2025-02-15 04:20:00,2025-02-16 02:22:28,bug
ollama/ollama,9118,Draft MLX go backend for new engine,open,2025-02-14 19:28:17,,
ollama/ollama,9018,CUDA error: an illegal memory access was encountered,open,2025-02-11 17:22:33,,bug
ollama/ollama,8949,"Download the model manually, drop it somewhere, and load it directly",closed,2025-02-08 10:12:34,2025-10-06 00:20:12,
ollama/ollama,8895,Don't perform memory check if client sets use_mmap true.,open,2025-02-06 19:44:25,,
ollama/ollama,8873,pull model manifest: 500,closed,2025-02-06 08:23:09,2025-02-06 21:21:41,networking
ollama/ollama,8851,Memory access fault by GPU node-1 (Agent handle: 0x5635e3db2590) on address 0x7f189722f000. Reason: Page not present or supervisor privilege. (ollama via docker),open,2025-02-05 15:57:11,,"bug,amd"
ollama/ollama,8831,Improve download stability by increasing timeout threshold from 5s to 30s,closed,2025-02-05 01:49:27,2025-02-05 18:00:26,
ollama/ollama,8818,does ollama support windows 7,closed,2025-02-04 09:02:46,2025-03-26 12:15:02,windows
ollama/ollama,8806,Empty supported_types list prevents GPU detection for older AMD GPUs (RX 5700 XT) in official Ollama distribution,closed,2025-02-04 00:07:01,2025-02-21 16:57:18,bug
ollama/ollama,8783,Ollama Auto-Starts on Windows Without User Permission,closed,2025-02-03 07:27:53,2025-07-08 22:38:06,bug
ollama/ollama,8770,Error: llama runner process has terminated: exit status 2,open,2025-02-02 02:20:43,,bug
ollama/ollama,8735,ollama does not utilize HBM3 memory on MI300A,open,2025-01-31 20:59:35,,bug
ollama/ollama,8694,"Ollama on ""old"" MAC",closed,2025-01-30 15:04:43,2025-07-04 21:23:04,feature request
ollama/ollama,8669,deepseek-r1:32b do not support tools? qwen2.5 base model should support.,closed,2025-01-29 18:49:54,2025-10-06 00:16:02,bug
ollama/ollama,8660,gpu,closed,2025-01-29 13:41:07,2025-01-29 13:51:19,bug
ollama/ollama,8601,README: Add handy-ollama to tutorial,closed,2025-01-27 04:29:41,2025-11-05 17:56:14,
ollama/ollama,8597,"Error: llama runner process has terminated: error loading model: unable to allocate CUDA0 buffer (4x L40S, 384GB system RAM, Deepseek-R1)",closed,2025-01-26 17:56:11,2025-01-30 18:05:31,bug
ollama/ollama,8594,Ollama stops accessing GPU and Reverts to CPU after runing for extended periods,closed,2025-01-26 15:52:00,2025-01-27 15:55:33,bug
ollama/ollama,8591,High idle power consumption due to persistent CUDA initialization,open,2025-01-26 11:24:50,,bug
ollama/ollama,8577,Context caching in RAM,closed,2025-01-25 06:52:14,2025-11-01 16:53:18,feature request
ollama/ollama,8558,"Fix build for loongarch64, go arch is not same with uname -m",open,2025-01-24 03:36:14,,
ollama/ollama,8536,Support for API_KEY based authentication,closed,2025-01-22 13:58:27,2025-01-22 17:23:18,feature request
ollama/ollama,8484,Issue with Ollama Model Download: Progress Reverting During Download,closed,2025-01-19 10:48:15,2025-03-04 15:36:35,"bug,networking"
ollama/ollama,8460,Llama-3_1-Nemotron-51B-Instruct,open,2025-01-16 21:21:32,,model
ollama/ollama,8444,Ollama not respecting structured outputs with some ordering of refs,open,2025-01-16 00:41:12,,bug
ollama/ollama,8427,0.5.5 Model Creation on Windows Seems Broken,closed,2025-01-14 19:52:57,2025-01-15 03:01:25,bug
ollama/ollama,8423,save with OLLAMA_MODELS set doesn't work anymore in 0.5.5,open,2025-01-14 15:46:45,,bug
ollama/ollama,8414,[Feature] Support Intel GPUs,closed,2025-01-14 04:58:03,2025-05-25 05:58:59,feature request
ollama/ollama,8359,feat(install): use curl -C to continue download,open,2025-01-09 07:24:38,,
ollama/ollama,8339,`GIN_MODE` not able to set to `release`,closed,2025-01-07 18:11:06,2025-01-16 00:01:57,bug
ollama/ollama,8305,Speed ten times slower than llamafile,closed,2025-01-04 11:49:09,2025-10-03 21:37:35,"bug,performance,needs more info"
ollama/ollama,8296,Some Models seem to be crashing while using with JSON Schema mode,open,2025-01-03 17:37:41,,bug
ollama/ollama,8293,Ollama - Gentoo Linux support ,closed,2025-01-03 10:42:43,2025-10-28 22:01:04,feature request
ollama/ollama,8279,Improved offline installation experience (install.sh),open,2025-01-01 10:36:41,,
ollama/ollama,8262,Segmentation Fault in AMD GPGPU Applications on 780M,closed,2024-12-28 13:36:31,2025-02-15 08:36:38,bug
ollama/ollama,8237,Changes macOS installer to skip symlink step if ollama is already in path.,closed,2024-12-25 08:08:00,2025-11-05 17:59:07,
ollama/ollama,8218,Question: Commercial Usage License Confirmation and Data Collection Clarification,open,2024-12-23 12:58:03,,
ollama/ollama,8165,server: add options to dry run and debug for chat and generate,closed,2024-12-18 23:20:47,2025-10-16 22:58:43,
ollama/ollama,8154,I can not connect to 11434 port,closed,2024-12-18 10:30:19,2025-10-03 21:39:25,"bug,needs more info"
ollama/ollama,8106,server: tokenize & detokenize endpoints,closed,2024-12-15 04:32:59,2025-10-16 22:57:14,
ollama/ollama,8105,Digest mismatch for llama3.3,open,2024-12-15 03:09:51,,bug
ollama/ollama,8008,Return prompt cache utilization on completion responses,open,2024-12-09 08:28:49,,feature request
ollama/ollama,7865,Model Context Protocol (MCP) support,open,2024-11-27 19:37:46,,feature request
ollama/ollama,7777,ppc64le: corrected ioctls,open,2024-11-21 11:00:06,,
ollama/ollama,7750,Disallow Tool Streaming,closed,2024-11-20 00:15:52,2024-11-22 00:42:17,
ollama/ollama,7733,"Error: POST predict: Post ""http://127.0.0.1:35943/completion"": EOF",closed,2024-11-19 03:56:41,2024-11-22 20:30:03,"bug,memory"
ollama/ollama,7550,ollama runner process has terminated: exit status 127 ,closed,2024-11-07 10:13:31,2025-10-03 22:02:18,"bug,linux,needs more info"
ollama/ollama,7424,boost embed endpoint,open,2024-10-30 16:43:31,,
ollama/ollama,7420,"Support AMD GPUs on Ampere, Raspberry Pis (arm64 ROCm)",closed,2024-10-30 15:01:36,2024-11-01 16:10:23,feature request
ollama/ollama,7256,Last character being truncated by stop sequence,closed,2024-10-18 17:27:40,2025-10-03 22:17:17,"bug,needs more info"
ollama/ollama,7252,add h2ovl-mississippi-800m and h2ovl-mississippi-2b,open,2024-10-18 09:59:30,,model
ollama/ollama,7244,Pulling models from private OCI Registries,open,2024-10-17 18:57:43,,feature request
ollama/ollama,7219,FEAT: add rerank support,closed,2024-10-16 03:37:43,2025-09-16 18:05:05,
ollama/ollama,7125,openai: support max_completion_tokens due to deprecation of max_tokens,open,2024-10-08 01:17:37,,"feature request,api"
ollama/ollama,7104,Optimizing GPU Usage for AI Models: Splitting Workloads Across Multiple GPUs Even if the Model Fits in One GPU,closed,2024-10-05 02:17:29,2024-10-05 13:44:56,feature request
ollama/ollama,7097,feat: configure auto startup in macos,open,2024-10-04 00:49:51,,
ollama/ollama,7047,Uneven split across GPUs,open,2024-09-30 15:07:23,,"bug,memory"
ollama/ollama,7001,"cli: pull models without starting server, fixes #3369",open,2024-09-27 10:12:50,,
ollama/ollama,6934,"OpenAI client expects embeddings to be base64 encoded string, not json array of floats",closed,2024-09-24 13:02:47,2025-10-27 19:15:48,"bug,api"
ollama/ollama,6922,Support for jinaai/jina-embeddings-v3 embedding model,open,2024-09-23 17:56:39,,model
ollama/ollama,6854,server: Add OLLAMA_NO_MMAP to disable mmap globally,open,2024-09-18 08:33:28,,
ollama/ollama,6826,"Massive performance regression on 0.1.32 -> GGML_CUDA_FORCE_MMQ: (SET TO NO, after  0.1.31)",closed,2024-09-16 13:04:35,2025-10-03 21:12:02,"bug,performance,nvidia,needs more info"
ollama/ollama,6748,Support Mistral's new visual model: Pixtral-12b-240910,closed,2024-09-11 08:24:58,2025-10-26 23:30:32,model
ollama/ollama,6729,discover/gpu.go: Add Support for Distributed Inferencing,closed,2024-09-10 14:24:43,2025-05-30 02:28:35,
ollama/ollama,6704,ollama model not support tool calling,closed,2024-09-09 02:12:10,2024-09-11 06:43:18,feature request
ollama/ollama,6652,Add Dracarys-Llama-3.1-70B-Instruct support,closed,2024-09-05 09:32:08,2025-11-07 16:22:50,model
ollama/ollama,6537,Add metrics endpoint and basic request metrics otel based,open,2024-08-27 23:54:05,,
ollama/ollama,6489,Error 403 occurs when I call ollama's api,closed,2024-08-24 10:31:27,2024-08-25 01:52:50,bug
ollama/ollama,6398,"When running ollama via docker, it won't respond to any request by API-call or python-client-library",closed,2024-08-17 13:51:05,2024-08-17 22:22:16,bug
ollama/ollama,6377,Full(er) JSON Schema support for tool calling,closed,2024-08-15 20:06:35,2025-10-07 20:50:00,"feature request,api"
ollama/ollama,6329,Change log for updated models on website?,open,2024-08-12 21:52:06,,"feature request,ollama.com"
ollama/ollama,6282,"AMD integrated graphic on linux kernel 6.9.9+, GTT memory, loading freeze fix",open,2024-08-09 10:43:59,,
ollama/ollama,6262,Batch embeddings get progressively worse with larger batches,open,2024-08-08 20:47:39,,bug
ollama/ollama,6230,Add Generate Embedding for Sparse vector,open,2024-08-07 10:26:27,,feature request
ollama/ollama,6224,Passing result from tool calling to model,closed,2024-08-07 05:39:45,2024-10-24 03:23:46,"feature request,needs more info"
ollama/ollama,6211,Error: max retries exceeded,closed,2024-08-06 22:42:52,2024-08-11 23:09:29,bug
ollama/ollama,6173,"Using ollama version 0.3.3, downloading all models will result in errors.",open,2024-08-05 08:17:10,,bug
ollama/ollama,6152,I read that ollama now supports AMD GPUs but it's not using it ,closed,2024-08-03 13:53:16,2024-08-09 20:52:47,"question,amd"
ollama/ollama,6119,Enable experimental windows build for OneAPI [Intel Discrete GPUs],closed,2024-08-01 13:32:57,2025-11-05 20:43:30,
ollama/ollama,6094,"""embedding generation failed: do embedding request: Post \""http://127.0.0.1:33967/embedding\"": EOF""",closed,2024-07-31 09:39:08,2024-09-02 23:36:51,bug
ollama/ollama,6068,ollama serve --choice a model name,closed,2024-07-30 05:44:11,2024-07-31 01:39:26,feature request
ollama/ollama,6024,Disable auto updates,open,2024-07-28 09:48:29,,feature request
ollama/ollama,5986,Make it easier to run the container as a non-root user,open,2024-07-26 14:54:33,,"feature request,docker"
ollama/ollama,5907,Support token embeddings for  `v1/embeddings`,open,2024-07-24 11:17:36,,"feature request,api"
ollama/ollama,5872, [Ascend ] add ascend npu support,open,2024-07-23 11:44:09,,
ollama/ollama,5800,Enable speculative decoding,open,2024-07-19 21:51:23,,"feature request,performance"
ollama/ollama,5782,Add hermes-2-pro-llama-3 to the testing matrix,closed,2024-07-18 20:24:45,2025-11-03 00:14:18,
ollama/ollama,5760,Make llama.cpp's cache_prompt parameter configurable,open,2024-07-18 02:05:28,,
ollama/ollama,5754,OLLAMA_MAX_VRAM  is ignored,closed,2024-07-17 18:23:32,2024-07-22 17:35:30,bug
ollama/ollama,5700,zfs ARC leads to incorrect system memory prediction and refusal to load models that could work,open,2024-07-15 11:23:55,,"feature request,memory"
ollama/ollama,5641,Ollama Puts out Gibberish After a While.,closed,2024-07-11 23:31:36,2025-10-03 21:08:07,"bug,needs more info"
ollama/ollama,5593,Support intel igpus,closed,2024-07-10 09:05:57,2025-10-15 20:39:45,
ollama/ollama,5549,Account removal on ollama.com,closed,2024-07-08 19:54:19,2024-07-09 04:36:27,feature request
ollama/ollama,5509,usage templating,open,2024-07-05 23:28:10,,
ollama/ollama,5495,The quality of the results returned by the embedding model  become worse,closed,2024-07-05 05:28:44,2025-10-03 19:19:48,"bug,needs more info"
ollama/ollama,5426,"Enable AMD iGPU 780M in Linux, Create amd-igpu-780m.md",open,2024-07-02 04:04:52,,
ollama/ollama,5424,"Supports voice recognition and text-to-speech capabilities, with customizable extension abilities",open,2024-07-02 00:01:13,,feature request
ollama/ollama,5356,allow for num_ctx parameter in the openai API compatibility,closed,2024-06-28 10:04:14,2025-04-16 22:20:10,feature request
ollama/ollama,5338,The main shell script runner for ollama downloader doesn't check for hash,open,2024-06-27 18:37:18,,bug
ollama/ollama,5304,Support for multimodal embedding models,open,2024-06-26 15:13:57,,feature request
ollama/ollama,5303,"Ollama keeps to randomly re-evaluate whole prompt, making chats impossible",open,2024-06-26 14:14:53,,"bug,performance"
ollama/ollama,5284,tools,closed,2024-06-25 21:26:54,2024-07-16 01:03:38,
ollama/ollama,5245,Allow importing multi-file GGUF models,open,2024-06-23 21:45:41,,bug
ollama/ollama,5233,filtering library models based on tags?,open,2024-06-23 01:02:38,,"feature request,ollama.com"
ollama/ollama,5159,"""error"": ""invalid character 'm' looking for beginning of value""",closed,2024-06-20 06:09:30,2024-06-20 14:27:54,bug
ollama/ollama,5059,Add Vulkan support to ollama,closed,2024-06-15 10:13:25,2025-10-15 20:29:43,
ollama/ollama,4960,user is not in the sudoers file - add support to install on debian 12,open,2024-06-10 10:20:51,,bug
ollama/ollama,4596,[Website] Library - Newest doesn't really sort with the newest releases,open,2024-05-23 16:53:52,,"feature request,ollama.com"
ollama/ollama,4510,Would it be possible for Ollama to support re-rank models?,closed,2024-05-18 04:05:07,2024-09-02 20:57:52,feature request
ollama/ollama,4370,Ollama‚Äôs speed in generating chat content slowed down by tenfold When switching the chat format to JSON,open,2024-05-12 03:31:46,,"bug,performance,api"
ollama/ollama,4363,Install rocm packages on host system on dnf based system,open,2024-05-11 14:32:36,,
ollama/ollama,4209,IBM-Granite ,closed,2024-05-06 21:26:04,2024-06-04 06:51:15,model
ollama/ollama,4192,feat: support registry basic auth,closed,2024-05-06 03:02:16,2024-12-27 06:06:39,
ollama/ollama,4115,Error 403 when trying to call api/chat or api/generate from REST client,closed,2024-05-03 03:06:15,2024-05-07 07:16:49,bug
ollama/ollama,4077,Stop running model without removing,closed,2024-05-01 11:58:26,2024-05-01 13:19:00,feature request
ollama/ollama,4072,Ollama should prevent sleep when working.,open,2024-05-01 08:21:53,,"feature request,good first issue,windows"
ollama/ollama,4064,Support DirectML,open,2024-04-30 22:47:27,,feature request
ollama/ollama,4034,Implement downloads via torrents,open,2024-04-29 20:51:10,,"feature request,registry"
ollama/ollama,4025,"Update linux.md, suggest containerized install",open,2024-04-29 13:43:19,,
ollama/ollama,3978,"Error: Head ""http://127.0.0.1:11434/"": dial tcp 127.0.0.1:11434: connectex:",closed,2024-04-27 12:46:00,2024-05-21 18:18:09,"bug,windows"
ollama/ollama,3953,Support VLLM as a backend,open,2024-04-26 19:29:55,,feature request
ollama/ollama,3851,"Why Ollama is so terribly slow when I set format=""json""",open,2024-04-23 19:32:01,,"bug,performance,api"
ollama/ollama,3794,"Ê®°Âûã‰∏ãËΩΩÊúÄÂêé1%ÈÄüÂ∫¶È™§ÈôçÔºåÂØºËá¥‰∏ãËΩΩÊó∂Èó¥Ë∂ÖÈïø„ÄÇThe download speed suddenly drops at the last 1%, resulting in an extremely long download time.",closed,2024-04-21 09:34:13,2024-04-30 19:20:29,bug
ollama/ollama,3769,An existing connection was forcibly closed by the remote host.Could you help me?,closed,2024-04-20 02:07:16,2024-05-02 00:24:10,bug
ollama/ollama,3748,Import from a HF model directly?,closed,2024-04-19 06:35:34,2024-05-10 00:09:36,feature request
ollama/ollama,3651,"If OLLAMA_CONTAINER_MANAGER is set, only install NVIDIA drivers",open,2024-04-15 11:33:41,,
ollama/ollama,3648,Add AutoSD and Fedora Asahi Remix to install script,open,2024-04-15 09:33:54,,
ollama/ollama,3643,how to change the max input token length when I run ‚Äò‚Äôollama run gemma:7b-instruct-v1.1-fp16‚Äò‚Äô,closed,2024-04-15 05:12:38,2024-04-17 00:46:54,
ollama/ollama,3625,can't be installed on ubuntu runing in a podman container.,closed,2024-04-13 14:01:22,2024-04-16 16:15:26,"bug,amd,docker"
ollama/ollama,3615,Install Ollama on OSTree systems,open,2024-04-12 11:47:22,,
ollama/ollama,3530,Add n_probs input option and CompletionProbabilities output option,open,2024-04-07 23:49:41,,
ollama/ollama,3526,Ollama fails to start in CPU only mode,closed,2024-04-07 16:26:08,2024-05-18 04:12:42,bug
ollama/ollama,3511,"On Windows, launching ollama from the shortcut or executable by clicking causes very slow tokens generation, but launching from commandline is fast",closed,2024-04-06 08:08:35,2024-09-21 23:54:50,"bug,windows"
ollama/ollama,3438,Bug in MODEL download directory and launching ollama service  in Linux,open,2024-04-01 13:06:01,,"bug,linux"
ollama/ollama,3368,Reranking models,open,2024-03-27 07:41:15,,
ollama/ollama,3201,Ollama is not using my GPU (Windows),closed,2024-03-17 21:27:23,2024-04-15 22:53:48,"bug,windows,nvidia"
ollama/ollama,3144,add /metrics endpoint,open,2024-03-14 16:39:01,,"feature request,api"
ollama/ollama,3107,Windows Rocm: HSA_OVERRIDE_GFX_VERSION doesn¬¥t work,open,2024-03-13 14:51:31,,"bug,windows,amd"
ollama/ollama,3105,Improve usability with Bash completion for Ollama on Linux,closed,2024-03-13 14:40:44,2024-05-09 18:58:49,
ollama/ollama,3097,Ollama CUDA on Ubuntu Issue,closed,2024-03-13 09:14:02,2024-04-12 22:20:11,"bug,linux,nvidia"
ollama/ollama,3095,Limit ollama usage of GPUs using CUDA_VISIBLE_DEVICES,closed,2024-03-13 06:42:44,2024-04-12 22:26:09,nvidia
ollama/ollama,2941,Global Configuration Variables for Ollama,closed,2024-03-05 21:32:44,2024-03-06 01:12:19,
ollama/ollama,2938,Windows install path,closed,2024-03-05 16:51:30,2024-03-21 13:20:16,windows
ollama/ollama,2745,Ability to pull/push models from/to OCI registries,closed,2024-02-25 16:14:03,2024-03-01 02:07:57,
ollama/ollama,2714,Misunderstanding of ollama num_ctx parameter and context window,closed,2024-02-23 18:00:42,2024-02-23 19:34:42,
ollama/ollama,2680,Server misbehaving pulling models,closed,2024-02-22 14:00:42,2024-02-22 19:30:58,
ollama/ollama,2672, Do Ollama support multiple GPUs working simultaneously?,closed,2024-02-22 09:37:50,2024-02-26 12:11:55,
ollama/ollama,2637,Integrated AMD GPU support,open,2024-02-21 14:56:12,,"feature request,amd"
ollama/ollama,2627,Error: listen tcp 127.0.0.1:11434: bind: ,closed,2024-02-21 09:41:05,2024-03-27 20:54:57,"bug,windows"
ollama/ollama,2578,"First attempt at Vulkan: WIP, do not merge",closed,2024-02-18 16:00:12,2025-10-15 20:30:43,
ollama/ollama,2558,Issue on Windows 10 ENT. wsarecv: An existing connection was forcibly closed by the remote host.,closed,2024-02-17 09:01:42,2024-03-11 19:43:32,bug
ollama/ollama,2496,default num_thread incorrect on some large core count system (non-hyperthreading),closed,2024-02-14 17:10:39,2024-08-05 22:20:08,bug
ollama/ollama,2491,How to install ollama on ubuntu with specific version,closed,2024-02-14 12:16:23,2024-02-20 03:59:33,
ollama/ollama,2458,Add support for running llama.cpp with SYCL for Intel GPUs,closed,2024-02-12 00:26:06,2025-10-15 20:32:51,
ollama/ollama,2453,"Add support for older AMD GPU gfx803, gfx802, gfx805 (e.g. Radeon RX 580, FirePro W7100)",open,2024-02-11 22:15:37,,"feature request,amd,build"
ollama/ollama,2415,Provide logits or logprobs in the API,closed,2024-02-08 21:02:45,2025-11-13 22:53:56,"feature request,api"
ollama/ollama,2337,Support model allenai/OLMo-7B,open,2024-02-03 12:10:31,,model
ollama/ollama,2225,Ollama stops generating output and fails to run models after a few minutes,closed,2024-01-27 06:22:10,2024-04-15 19:09:59,bug
ollama/ollama,2179,add `--upgrade-all` flag to refresh any stale models,open,2024-01-24 22:22:22,,
ollama/ollama,2147,permission denied when setting OLLAMA_MODELS in service file,closed,2024-01-22 21:59:56,2024-03-12 18:45:32,
ollama/ollama,2033,Add Vulkan runner ,open,2024-01-17 18:15:00,,"feature request,amd,intel,gpu"
ollama/ollama,2028,how to remove ollama from macos?,closed,2024-01-17 09:27:03,2024-01-22 17:51:07,
ollama/ollama,2006,Rate limit download speed on pulling new models,open,2024-01-15 15:02:22,,networking
ollama/ollama,1890,A way to update all downloaded models,open,2024-01-10 10:03:21,,feature request
ollama/ollama,1813,How to run Ollama  only on a dedicated GPU? (Instead of all GPUs),closed,2024-01-05 18:35:28,2024-03-24 18:15:04,gpu
ollama/ollama,1769,Long initial loading time.,closed,2024-01-03 18:09:10,2024-01-04 23:04:01,
ollama/ollama,1736,Download slows to a crawl at 99%,open,2023-12-29 04:47:12,,"bug,networking,registry"
ollama/ollama,1731,`pulling manifest  Error: EOF` when pulling after disk is full,open,2023-12-28 00:32:29,,bug
ollama/ollama,1730,MLX backend,open,2023-12-27 20:10:50,,feature request
ollama/ollama,1714,DeciLM-7B Support,closed,2023-12-26 00:32:49,2025-10-27 00:53:07,model
ollama/ollama,1647,curl: (60) SSL Certificate Problem,closed,2023-12-20 23:28:46,2024-02-20 01:27:50,
ollama/ollama,1645,Dark mode for ollama.com,open,2023-12-20 22:43:24,,"feature request,ollama.com"
ollama/ollama,1640,added logprobs (`n_probs`),closed,2023-12-20 19:26:27,2025-01-07 19:25:56,
ollama/ollama,1606,Added support for specifying an arbitrary GBNF compatible grammar,closed,2023-12-19 14:21:53,2024-12-05 00:43:26,
ollama/ollama,1590,Add support for Intel Arc GPUs,open,2023-12-18 23:25:37,,"feature request,intel"
ollama/ollama,1378,Is there a health check endpoint?,closed,2023-12-04 20:28:03,2023-12-04 20:57:23,
ollama/ollama,1352, Feature Request:  üõ†Ô∏è Configurable Option to Prevent Automatic Activation of ollama.service on Linux,closed,2023-12-02 13:11:15,2024-05-06 23:29:25,
ollama/ollama,1293,"Ollama list modified column shows when the model was last pulled, rather than when last modified",open,2023-11-27 23:18:06,,
ollama/ollama,1016,Support AMD GPUs on Intel Macs,open,2023-11-06 15:20:01,,"feature request,amd,macos"
ollama/ollama,941,`digest mismatch` on download,open,2023-10-28 17:47:23,,bug
ollama/ollama,849,How to secure the API with api key,closed,2023-10-20 07:30:42,2023-10-25 19:07:40,
ollama/ollama,707,127.0.0.1:11434: bind: address already in use,closed,2023-10-05 06:15:02,2023-12-04 19:39:38,question
ollama/ollama,695,Can't resume download (pull) on restart server ,closed,2023-10-04 07:08:44,2024-01-16 22:21:41,bug
ollama/ollama,669,Allow customizing allowed headers in CORS settings,closed,2023-10-01 23:31:21,2023-10-28 19:25:17,
ollama/ollama,654,How to fine tune and use it with ollama?,closed,2023-09-30 05:25:50,2023-10-01 04:42:28,
ollama/ollama,645,Allow global Ollama settings configuration,open,2023-09-29 14:33:32,,feature request
ollama/ollama,581,How to use `num_predict`?,closed,2023-09-23 23:20:10,2023-09-27 01:47:32,
ollama/ollama,294,Streaming responses should have `Content-Type` set to `application/x-ndjson `,closed,2023-08-06 03:26:04,2023-08-09 04:38:40,"bug,good first issue"
ollama/ollama,162,Don't automatically start on startup / have an option to disable this,open,2023-07-21 13:57:28,,feature request
ollama/ollama,11,interactive generate,closed,2023-06-28 18:24:35,2023-06-28 18:32:06,
langchain-ai/langgraph,6520,docs(langgraph): update old imports in docs tutorial notebooks,open,2025-11-30 07:21:05,,
langchain-ai/langgraph,6519,docs(langgraph): update old imports in docs tutorial notebooks,closed,2025-11-30 07:15:40,2025-11-30 07:17:23,
langchain-ai/langgraph,6518,docs(langgraph): update old imports in docs tutorial notebooks,closed,2025-11-30 07:11:23,2025-11-30 07:14:06,
langchain-ai/langgraph,6517,Quickstart example does not pass typecheck,open,2025-11-28 12:50:53,,"bug,pending"
langchain-ai/langgraph,6516,Graph visualisation of structured response agent is weird and cannot visualise the graph in supervisor agent pattern.,open,2025-11-28 10:26:34,,"bug,pending"
langchain-ai/langgraph,6515,docs: add clarity to use of `thread_id`,open,2025-11-28 08:40:27,,
langchain-ai/langgraph,6514,docs: add docstrings to `add_node` overloads,open,2025-11-28 07:18:46,,
langchain-ai/langgraph,6513,docs(langgraph): fix async task example (await gather) and clarify blocking behavior in sync example,open,2025-11-27 22:29:11,,
langchain-ai/langgraph,6512,fix(langgraph): validate node outputs against State schema before checkpointing,open,2025-11-27 14:14:54,,
langchain-ai/langgraph,6511,fix(checkpoint): return self in InMemorySaver context manager,open,2025-11-27 09:00:00,,
langchain-ai/langgraph,6510,GPT-5 tool calls return both content and tool_calls ‚Äî weird behavior during streaming,open,2025-11-27 08:48:17,,"bug,pending"
langchain-ai/langgraph,6509,fix(prebuilt): support generic type arguments for ToolRuntime injection,open,2025-11-27 08:28:57,,
langchain-ai/langgraph,6508,fix(prebuilt): handle ToolException in ToolNode default error handler,open,2025-11-27 07:33:12,,
langchain-ai/langgraph,6507,fix(langgraph): remove shadowed stream_writer in astream and add async tests,open,2025-11-27 05:59:24,,
langchain-ai/langgraph,6506,Async MongoDB Checkpoint Saver Implementation for LangGraph module removed from version 1.0,open,2025-11-26 20:44:18,,"bug,pending"
langchain-ai/langgraph,6505,Failed to install `langgraph-cli[inmem]` with `Python 3.14`,open,2025-11-26 02:17:22,,"bug,pending"
langchain-ai/langgraph,6504,feat: Increase default recursion limit from 25 to 250,open,2025-11-26 00:35:37,,
langchain-ai/langgraph,6503,fix: release name should be same as package name,closed,2025-11-25 23:58:25,2025-11-26 00:02:44,
langchain-ai/langgraph,6502,release(langgraph): 1.0.4,closed,2025-11-25 19:21:47,2025-11-25 20:27:50,
langchain-ai/langgraph,6501,release(sdk-py): 0.2.11,closed,2025-11-25 19:11:35,2025-11-25 19:17:41,
langchain-ai/langgraph,6500,feat(sdk-py): add sentinel to skip auto loading api key on sdk client create,closed,2025-11-25 16:43:20,2025-11-25 19:10:56,
langchain-ai/langgraph,6499,docs(tutorial): Formatted agent supervisor documentation,open,2025-11-25 06:48:25,,
langchain-ai/langgraph,6498,chore: rm swarm from docs redeploy ci,closed,2025-11-25 02:46:51,2025-11-26 00:42:09,
langchain-ai/langgraph,6497,chore: pop thread ID from configurable fields in remote graph,closed,2025-11-24 21:51:45,2025-11-25 00:09:09,
langchain-ai/langgraph,6496,chore: Bump version of `sdk-py` to `0.2.10`,closed,2025-11-24 20:00:37,2025-11-24 20:27:16,
langchain-ai/langgraph,6495,Add on_end_behavior parameter to graph.compile() for batch job support,open,2025-11-24 12:16:22,,"bug,pending"
langchain-ai/langgraph,6494,LangSmith Studio hangs when communicating with local Agent server on Windows,open,2025-11-24 10:07:34,,"bug,pending"
langchain-ai/langgraph,6493,chore(docs): Update OpenAPI spec from LangGraph API v0.5.26,closed,2025-11-24 09:07:21,2025-11-24 21:43:00,
langchain-ai/langgraph,6492,docs: update main README React agent example for LangGraph,open,2025-11-24 07:50:46,,
langchain-ai/langgraph,6491,"Invalid state saved to checkpoint without validation, causing permanent corruption.",open,2025-11-24 07:10:14,,"bug,pending"
langchain-ai/langgraph,6490,chore(deps): bump actions/checkout from 5 to 6,closed,2025-11-24 06:32:44,2025-11-24 21:41:22,"dependencies,github_actions"
langchain-ai/langgraph,6489,chore(docs): Update OpenAPI spec from LangGraph API v0.5.25,closed,2025-11-23 09:05:11,2025-11-24 21:42:35,
langchain-ai/langgraph,6488,chore: delete docs,open,2025-11-23 07:03:15,,
langchain-ai/langgraph,6487,chore: clean up some refs,closed,2025-11-23 06:59:48,2025-11-23 07:13:30,
langchain-ai/langgraph,6486,Tool node error handling disabled by default after 1.0.1,open,2025-11-22 18:39:47,,"bug,pending"
langchain-ai/langgraph,6485,feat(langgraph): add probabilistic routing via negative nodes,closed,2025-11-22 13:03:46,2025-11-26 00:41:56,
langchain-ai/langgraph,6484,chore(docs): Update OpenAPI spec from LangGraph API v0.5.24,closed,2025-11-22 03:53:46,2025-11-24 21:42:02,
langchain-ai/langgraph,6483,feat(sdk-py): Add `name` parameter to Assistants search API,closed,2025-11-22 00:08:02,2025-11-24 18:58:55,
langchain-ai/langgraph,6482,feat: custom encryption at rest,open,2025-11-21 18:54:14,,
langchain-ai/langgraph,6481,chore(docs): Update OpenAPI spec from LangGraph API v0.5.23,closed,2025-11-21 09:06:17,2025-11-24 21:41:37,
langchain-ai/langgraph,6480,chore(sdk-py): Improve type-hinting of inputs,closed,2025-11-21 01:41:56,2025-11-25 02:29:03,
langchain-ai/langgraph,6479,chore(sdk-py): Add more type checking.,closed,2025-11-21 00:59:54,2025-11-21 01:40:24,
langchain-ai/langgraph,6478,AttributeError in streaming when InMemorySaver is used,open,2025-11-20 23:58:39,,"bug,pending"
langchain-ai/langgraph,6477,prebuilt==1.0.5 breaks create_react_agent when passing a list of BaseTool,open,2025-11-20 23:08:06,,"bug,pending"
langchain-ai/langgraph,6476,fix(docs): Update documentation link for resuming interrupts,open,2025-11-20 21:09:02,,
langchain-ai/langgraph,6475,fix: interrupt stream mode values,closed,2025-11-20 18:38:38,2025-11-20 22:23:10,
langchain-ai/langgraph,6474,chore(deps): bump js-yaml from 4.1.0 to 4.1.1 in /libs/cli/js-monorepo-example,open,2025-11-20 16:46:24,,"dependencies,javascript"
langchain-ai/langgraph,6473,release: langgraph-prebuilt 1.0.5,closed,2025-11-20 16:38:57,2025-11-20 16:45:19,
langchain-ai/langgraph,6472,chore(langgraph): Add __init__.pyi file for auto import support,open,2025-11-20 11:01:26,,
langchain-ai/langgraph,6471,chore(docs): Update OpenAPI spec from LangGraph API v0.5.20,closed,2025-11-20 09:06:24,2025-11-24 21:42:05,
langchain-ai/langgraph,6470,WinError 10013 when running langgraph dev,closed,2025-11-20 03:49:33,2025-11-26 03:36:46,"bug,pending"
langchain-ai/langgraph,6469,"Using Langgraph + Amap MCP, for example, when I input a travel plan, it needs to call many tools and perform 25 steps. Does it really take that long? Is there a better solution?",open,2025-11-20 02:11:52,,"bug,pending"
langchain-ai/langgraph,6468,fix: refactor injection logic to respect function signatures,closed,2025-11-20 01:24:58,2025-11-20 16:37:55,
langchain-ai/langgraph,6467,fix: improved typing on tool call requests (will bubble up to LC),open,2025-11-19 20:54:42,,
langchain-ai/langgraph,6466,chore(docs): Update OpenAPI spec from LangGraph API v0.5.17,closed,2025-11-19 19:51:21,2025-11-24 21:42:06,
langchain-ai/langgraph,6465,`ToolRuntime[WithTypeArguments]` not supported,open,2025-11-19 19:09:31,,"bug,pending"
langchain-ai/langgraph,6464,docs: add missing docstrings to PregelNode and ChannelRead,open,2025-11-19 18:01:03,,
langchain-ai/langgraph,6463,chore(deps): bump glob from 10.4.5 to 10.5.0 in /docs/_scripts/js_translation/codeblocks,open,2025-11-19 17:49:27,,"dependencies,javascript"
langchain-ai/langgraph,6462,fix: deprecate `setattr` on `ToolCallRequest`,closed,2025-11-19 14:44:22,2025-11-19 18:12:11,
langchain-ai/langgraph,6461,chore(docs): Update OpenAPI spec from LangGraph API v0.5.16,closed,2025-11-19 09:06:08,2025-11-24 21:42:08,
langchain-ai/langgraph,6460,StateSnapshot puts the user metadata in possibly wrong place,open,2025-11-18 18:38:46,,"bug,pending"
langchain-ai/langgraph,6459,feat: Add Goto type for atomic cross-graph handoffs,open,2025-11-18 16:39:43,,
langchain-ai/langgraph,6458,fix(pregel): handle EmptyInputError for Command with stream_mode='values',open,2025-11-18 16:22:17,,
langchain-ai/langgraph,6456,TypeError: Object of type Send is not JSON serializable when using PostgresSaver checkpoint,open,2025-11-18 15:38:13,,"bug,pending"
langchain-ai/langgraph,6455,Transactional Cross-Graph Handoff,open,2025-11-17 16:22:38,,
langchain-ai/langgraph,6454,chore(docs): Update OpenAPI spec from LangGraph API v0.5.14,closed,2025-11-16 22:53:10,2025-11-24 21:41:52,
langchain-ai/langgraph,6453,feat: add asyncpg support for postgres checkpoint,open,2025-11-15 18:50:13,,
langchain-ai/langgraph,6452,chore(deps): bump js-yaml from 4.1.0 to 4.1.1 in /docs/_scripts/js_translation/codeblocks,open,2025-11-15 18:03:12,,"dependencies,javascript"
langchain-ai/langgraph,6451,"create_agent In ""astream"" with stream_mode=""messages"" does not output tokens incrementally.",closed,2025-11-15 04:34:26,2025-11-24 22:23:47,"bug,pending"
langchain-ai/langgraph,6450,refactor: separate prepare_push_* functions,closed,2025-11-14 23:56:53,2025-11-15 00:13:03,
langchain-ai/langgraph,6449,`ToolException` from MCP server not handled correclty,open,2025-11-14 21:46:35,,"bug,pending"
langchain-ai/langgraph,6448,fix(checkpoint): InMemorySaver context managers should return self instance,open,2025-11-14 14:07:29,,
langchain-ai/langgraph,6447,Async tools do not currently support 'custom' events streaming via get_stream_writer().,open,2025-11-14 13:30:10,,"bug,pending"
langchain-ai/langgraph,6446,GitHub Issue Title: InvalidUpdateError when using parallel subgraphs with shared state keys in LangGraph,open,2025-11-14 10:27:27,,"bug,pending"
langchain-ai/langgraph,6443,fix: dep warnings in prebuilt,closed,2025-11-13 18:49:22,2025-11-13 18:59:34,
langchain-ai/langgraph,6442,chore(docs): add more redirects  + catchall,closed,2025-11-13 18:39:16,2025-11-13 19:49:41,
langchain-ai/langgraph,6441,release: prebuilt 1.0.3,closed,2025-11-13 18:38:05,2025-11-13 18:38:24,
langchain-ai/langgraph,6440,fix: tidy up deprecation warnings,closed,2025-11-13 14:42:59,2025-11-13 18:46:58,
langchain-ai/langgraph,6439,"[bug] When a node relies on multiple input edges,  it will execute as soon as one of the edges meets the condition.",closed,2025-11-13 03:05:01,2025-11-14 17:01:37,"bug,pending"
langchain-ai/langgraph,6438,"fix(langgraph): allow providing both context and configurable in RemoteGraph, fixes #6342",open,2025-11-12 22:59:15,,
langchain-ai/langgraph,6437,HELP WANTED - Update langgraph docs tutorials,open,2025-11-12 22:21:46,,
langchain-ai/langgraph,6436,Tool function return Command with goto variable cause parallel running,closed,2025-11-12 12:27:24,2025-11-12 14:29:15,"bug,pending"
langchain-ai/langgraph,6435,Map-reduce fails to trace when reducing dictionary with tuple keys,open,2025-11-12 11:39:50,,"bug,pending"
langchain-ai/langgraph,6434,`ImportError: cannot import name 'CompiledStateGraph' from 'langgraph.graph'`,closed,2025-11-12 10:12:33,2025-11-12 10:15:31,"bug,pending"
langchain-ai/langgraph,6433,graph.update_state() generates a new config WITH  the 'next' ATTRITUBE empty,closed,2025-11-12 09:29:54,2025-11-12 22:32:36,"bug,pending"
langchain-ai/langgraph,6432,FilesPreview.tsx: text/csv MIME type not supported in chat file uploads,open,2025-11-12 08:35:55,,"bug,pending"
langchain-ai/langgraph,6431,ToolNode fails to execute tools with ToolRuntime parameter due to Pydantic validation error,open,2025-11-11 14:51:33,,"bug,pending"
langchain-ai/langgraph,6430,‰∏∫‰ªÄ‰πàÊó†Ê≥ïÂØºÂÖ•glm,closed,2025-11-11 13:04:20,2025-11-11 13:04:35,"bug,pending"
langchain-ai/langgraph,6429,chore: langgraph patch release,closed,2025-11-10 17:15:51,2025-11-10 17:37:35,
langchain-ai/langgraph,6428,chore(docs): Update links in notebook_hooks.py for deployment,closed,2025-11-10 14:47:19,2025-11-10 14:51:06,
langchain-ai/langgraph,6427,fix _strip_extras helper function,open,2025-11-10 10:05:58,,
langchain-ai/langgraph,6426,Checkpointer for mongodb is not updated to support langgraph-checkpoint 3.0,open,2025-11-10 09:17:02,,
langchain-ai/langgraph,6425,fix(langgraph): inject runtime with store for conditional edges in update_state operations #6340,open,2025-11-10 08:43:06,,
langchain-ai/langgraph,6424,chore(deps): bump codespell-project/actions-codespell from 2.1 to 2.2,open,2025-11-10 06:34:12,,"dependencies,github_actions"
langchain-ai/langgraph,6423,Fix lint for APIConnectionError,open,2025-11-09 18:24:51,,
langchain-ai/langgraph,6422,test(langchain) Complements changes from PR #6420 ( retry resilience) ,open,2025-11-09 14:17:51,,
langchain-ai/langgraph,6421,chore(deps): upgrade dependencies with `uv lock --upgrade`,open,2025-11-09 00:36:41,,dependencies
langchain-ai/langgraph,6420,fix(checkpoint-postgres): add minimal retry resilience to PostgresSaver for transient connection errors,open,2025-11-08 13:38:17,,
langchain-ai/langgraph,6419,fix: make ToolNode changes more backwards compat,closed,2025-11-08 13:28:53,2025-11-08 13:32:10,
langchain-ai/langgraph,6418,feat(docs): enhance return type documentation for graph run method,open,2025-11-08 01:48:34,,
langchain-ai/langgraph,6417,feat(docs): warn that `StateGraph` is a builder class,closed,2025-11-08 01:47:39,2025-11-08 02:09:15,
langchain-ai/langgraph,6416,fix(docs): `PartialState` rendering in MkDocs,closed,2025-11-08 01:46:57,2025-11-08 02:08:02,
langchain-ai/langgraph,6415,fix(docs): synchronize `invoke` and `ainvoke` docstrings,closed,2025-11-08 00:59:46,2025-11-08 01:44:33,
langchain-ai/langgraph,6414,fix(docs): synchronize `stream` and `astream` docstrings,closed,2025-11-08 00:56:27,2025-11-08 01:44:25,
langchain-ai/langgraph,6411,fix(langgraph): fix command usage with nested subgraphs,open,2025-11-07 16:51:20,,
langchain-ai/langgraph,6410,docs(langgraph): Fix docstring code examples of task function,closed,2025-11-07 16:28:18,2025-11-07 21:01:20,
langchain-ai/langgraph,6409,Returning `Command` with `goto` attribute in a graph with >= 3 total graph depth results in `langgraph.errors.ParentCommand` error,open,2025-11-07 10:18:11,,"bug,pending"
langchain-ai/langgraph,6407,docs(langgraph): Fix typo in docstring of PregelLoop.tick,closed,2025-11-07 09:17:01,2025-11-07 12:38:16,
langchain-ai/langgraph,6406,Inconsistency of concurrent nodes,closed,2025-11-07 08:51:03,2025-11-07 19:49:38,"bug,pending"
langchain-ai/langgraph,6403,fix: patch tool names when not specified on custom messages,closed,2025-11-06 16:09:20,2025-11-07 12:39:20,
langchain-ai/langgraph,6402,chore(checkpoint-postgres): bump to 3.0.1,closed,2025-11-06 15:06:35,2025-11-06 16:14:02,
langchain-ai/langgraph,6404,Deprecation message for `langgraph.prebuilt.create_react_agent` has a wrong suggestion,closed,2025-11-06 10:01:52,2025-11-07 10:48:28,
langchain-ai/langgraph,6401,Pydantic populate_by_name not respected in StateGraph input validation,closed,2025-11-06 07:40:07,2025-11-07 10:52:48,
langchain-ai/langgraph,6400,fix(checkpoint-postgres): make async PG checkpoint migration idempotent,closed,2025-11-05 23:01:43,2025-11-06 14:49:52,
langchain-ai/langgraph,6399,chore(docs): Update redirects for langgraph server to agent server rename,closed,2025-11-05 14:42:20,2025-11-05 16:21:10,
langchain-ai/langgraph,6397,Invoking a toolnode programmatically fails,open,2025-11-05 10:16:17,,
langchain-ai/langgraph,6396,Token Usage Missing with AzureChatOpenAI when run through LangGraph Studio,closed,2025-11-05 05:12:05,2025-11-07 19:55:56,"bug,pending"
langchain-ai/langgraph,6395,chore: update ormsgpack minbound and add OPT_REPLACE_SURROGATES,closed,2025-11-04 21:38:19,2025-11-04 21:51:35,
langchain-ai/langgraph,6394,Langgraph streaming conflicts with additional callbacks,closed,2025-11-04 21:33:06,2025-11-07 21:57:04,"bug,pending"
langchain-ai/langgraph,6393,Read checkpoint_pending_writes (in class PregelLoop) from Redis(use langgraph-checkpoint-redis),closed,2025-11-04 08:36:17,2025-11-07 21:14:37,"bug,pending,not maintained by langchain"
langchain-ai/langgraph,6392,docs: add license files for checkpoint-sqlite and checkpoint-postgres,closed,2025-11-04 05:18:08,2025-11-07 12:39:07,
langchain-ai/langgraph,6391,fix(langgraph): add_edge() now accepts node objects like add_node(),open,2025-11-04 00:55:31,,
langchain-ai/langgraph,6390,release(cli): 0.4.7 expand  api bounds,closed,2025-11-03 23:39:29,2025-11-03 23:47:06,
langchain-ai/langgraph,6389,fix(langgraph): do not apply pending writes when updating state,closed,2025-11-03 23:31:31,2025-11-04 22:38:41,
langchain-ai/langgraph,6388,fix(sdk-py): use correct f-string representation when loading error,closed,2025-11-03 22:27:06,2025-11-07 12:51:36,
langchain-ai/langgraph,6387,Incorrect Error Message: Authentication handler error does not interpolate variable,closed,2025-11-03 22:25:10,2025-11-07 12:51:37,"bug,pending"
langchain-ai/langgraph,6386,fix(checkpoint): update checkpoint interface specification in README,closed,2025-11-03 21:36:46,2025-11-03 21:44:41,
langchain-ai/langgraph,6385,"fix(cli): add buildkit syntax directiive, update tests",closed,2025-11-03 14:36:10,2025-11-03 18:13:20,
langchain-ai/langgraph,6384,Self-host standalone servers fail to start,open,2025-11-03 13:15:17,,"bug,pending,missing information"
langchain-ai/langgraph,6382,fix(cli): update langgraph-api dependency to support 0.5.x,closed,2025-11-02 15:27:21,2025-11-04 00:28:27,
langchain-ai/langgraph,6381,fix(langgraph): merge CONF sections and preserve runtime in subgraphs,closed,2025-11-02 06:22:45,2025-11-07 13:04:24,
langchain-ai/langgraph,6380,langgraph-cli[inmem] dependencies with langgraph-api is outdated,closed,2025-11-01 21:17:05,2025-11-03 23:47:07,"bug,pending"
langchain-ai/langgraph,6379,fix: fix previoius edge cases such as 0,closed,2025-11-01 17:14:13,2025-11-07 12:54:22,
langchain-ai/langgraph,6378,feat(sdk-py): add durability parameter for RemoteGraph stream and astream,open,2025-11-01 15:32:13,,
langchain-ai/langgraph,6377,feature request: LangGraph Debugger: Enter/Shift+Enter/Ctrl+Enter keyboard shortcuts to send a message,open,2025-11-01 14:12:22,,enhancement
langchain-ai/langgraph,6376,Inconsistency in API - `add_node` accepts objects but `add_edge` requires string literals for the same nodes.,open,2025-11-01 13:42:01,,v1.1
langchain-ai/langgraph,6375,fix(langgraph): export REMOVE_ALL_MESSAGES in __all__ to fix linting,closed,2025-11-01 06:39:19,2025-11-07 12:52:27,
langchain-ai/langgraph,6374,chore: split out cli unconditional int test ci jobs,closed,2025-11-01 00:39:10,2025-11-03 18:22:36,
langchain-ai/langgraph,6373,Unable to stream with functional api,open,2025-10-31 21:09:04,,"bug,pending"
langchain-ai/langgraph,6372,chore: Update cli config schema,closed,2025-10-31 20:15:10,2025-11-01 16:50:46,
langchain-ai/langgraph,6371,docs: Update hosting page name redirect to platform setup,closed,2025-10-31 17:02:17,2025-10-31 18:13:00,
langchain-ai/langgraph,6370,LangGraph Server not propagating Runtime Context when using SendAPI,closed,2025-10-31 14:04:50,2025-11-06 15:24:32,"bug,pending"
langchain-ai/langgraph,6369,feat(checkpoint): add Kusto/Azure Data Explorer/Eventhouse checkpoint implementation,closed,2025-10-31 11:23:19,2025-11-07 19:24:11,
langchain-ai/langgraph,6368,fix(langgraph): Properly cleanup background batch tasks in `AsyncPostgresStore`,open,2025-10-31 02:11:43,,
langchain-ai/langgraph,6367,"AsyncPostgresStore cleanup leaves pending background batch tasks causing ""Task was destroyed"" warnings",open,2025-10-31 01:39:17,,"bug,pending"
langchain-ai/langgraph,6366,fix(langgraph): get_subgraph fails when nodes share a common prefix,open,2025-10-31 00:00:37,,
langchain-ai/langgraph,6365,chore: style fixes for refs,closed,2025-10-30 21:20:01,2025-10-30 21:59:41,
langchain-ai/langgraph,6364,chore: add `pyproject.toml` links,closed,2025-10-30 20:52:58,2025-11-07 12:43:51,
langchain-ai/langgraph,6363,Breaking Change in `langgraph-prebuilt==1.0.2` Without Proper Version Constraints,open,2025-10-30 18:37:44,,"bug,pending"
langchain-ai/langgraph,6362,Interrupts missing in thread_state on self-hosted LangGraph (1.x),open,2025-10-30 10:52:25,,"bug,pending"
langchain-ai/langgraph,6361,chore: bump prebuilt dep for lg,closed,2025-10-29 18:28:28,2025-10-29 18:34:28,
langchain-ai/langgraph,6360,refactor: simplify Send sanitization with dict comprehension,closed,2025-10-28 23:08:31,2025-10-29 15:51:46,
langchain-ai/langgraph,6356,PostgresSaver.setup() fails with DuplicateColumn when table already contains task_path,open,2025-10-28 16:26:51,,"bug,pending"
langchain-ai/langgraph,6355,"Import ""langgraph.graph.ui"" could not be resolved",closed,2025-10-28 08:24:40,2025-11-07 11:05:32,"bug,pending"
langchain-ai/langgraph,6354,fix(langgraph): Unexpected behavior for stream_mode sequences that are not lists,closed,2025-10-27 22:09:05,2025-11-07 13:01:25,
langchain-ai/langgraph,6353,"chore(docs): Revert the revert ""fix: rollback doc redirects"" (add redirects)",closed,2025-10-27 21:36:17,2025-10-29 16:12:06,
langchain-ai/langgraph,6352,fix: replace `python.langchain` links with new `docs.langchain`,closed,2025-10-27 15:48:14,2025-10-27 17:36:13,
langchain-ai/langgraph,6351,Error LangGraph 1.0.1 version on Structured Output code: ModuleNotFoundError: No module named 'langgraph._internal',closed,2025-10-27 14:21:19,2025-10-27 14:32:18,"bug,pending"
langchain-ai/langgraph,6350,[Bug] InjectedState not passed to tools when using create_agent() ‚Äî causes ‚Äústate: Field required‚Äù error in LangGraph multi-agent setup,closed,2025-10-27 10:36:32,2025-10-27 13:43:47,"bug,pending"
langchain-ai/langgraph,6349,chore(deps): bump langchain-core from 1.0.0a1 to 1.0.1 in /libs/langgraph,closed,2025-10-27 06:47:22,2025-10-29 16:52:58,"dependencies,python"
langchain-ai/langgraph,6348,chore(deps): bump actions/download-artifact from 5 to 6,closed,2025-10-27 06:41:59,2025-10-29 17:03:19,"dependencies,github_actions"
langchain-ai/langgraph,6347,chore(deps): bump actions/upload-artifact from 4 to 5,closed,2025-10-27 06:40:01,2025-10-29 17:03:28,"dependencies,github_actions"
langchain-ai/langgraph,6346,"UI Bundler ‚Äî¬†TypeError: Expected dict, got _Environ",closed,2025-10-26 17:12:03,2025-11-07 19:17:18,"bug,pending"
langchain-ai/langgraph,6345,fix: RemoteGraph context+config support with tests and server fix docs,closed,2025-10-26 13:46:06,2025-11-07 12:45:27,
langchain-ai/langgraph,6344,docs: fix typo in README.md,closed,2025-10-26 12:57:23,2025-11-07 12:56:56,
langchain-ai/langgraph,6342,[Major BUG] - RemoteGraph in LangGraph 1.0: Cannot use `context` and `config` together - Forces choice between checkpointing OR middleware,open,2025-10-25 14:42:57,,"bug,pending"
langchain-ai/langgraph,6341,Python dependencies not getting installed when using langgraph up,closed,2025-10-24 22:42:21,2025-11-07 19:45:36,"bug,pending"
langchain-ai/langgraph,6340,Store on conditional edge causes error.,open,2025-10-24 21:17:55,,"pending,v1.1"
langchain-ai/langgraph,6339,chore(deps): bump hono from 4.9.7 to 4.10.3 in /docs/_scripts/js_translation/codeblocks,closed,2025-10-24 19:40:13,2025-11-04 21:52:16,"dependencies,javascript"
langchain-ai/langgraph,6338,"LangGraph do not resume with graph.resume(config,...) or graph.stream(none,config,...) for a interrupt in a node with a create_agent agent",closed,2025-10-24 19:02:08,2025-10-26 02:04:57,"bug,pending"
langchain-ai/langgraph,6337,fix(cache): Implement explicit async Redis client validation to resolve #6247,open,2025-10-24 13:44:23,,
langchain-ai/langgraph,6336,Run graph nodes in parallel,closed,2025-10-24 12:25:50,2025-11-07 17:31:57,"bug,pending"
langchain-ai/langgraph,6335,Run graph nodes in parallel,closed,2025-10-24 12:06:33,2025-10-24 12:15:22,"bug,pending"
langchain-ai/langgraph,6333,"Pydantic v2 deprecations in langgraph/gregel/_messages.py:104 (__fields__, __fields_set__, instance access of computed fields)",open,2025-10-24 09:35:22,,"bug,pending"
langchain-ai/langgraph,6332,The tutorial is completely out of sync with all the reference libraries of the current latest version!,closed,2025-10-23 13:58:55,2025-11-07 17:40:07,"bug,pending"
langchain-ai/langgraph,6330,Preserve event metadata of custom streaming events,open,2025-10-23 09:24:28,,"bug,pending"
langchain-ai/langgraph,6328,fix(checkpoint-postgres): Replace f-string SQL formatting with parameterized queries in migration statements,closed,2025-10-23 06:17:14,2025-11-07 13:00:11,
langchain-ai/langgraph,6326,time travel works unexpected when using return Command,open,2025-10-23 01:58:25,,"bug,pending"
langchain-ai/langgraph,6324,chore(deps): bump hono from 4.9.7 to 4.10.2 in /docs/_scripts/js_translation/codeblocks,closed,2025-10-22 15:40:55,2025-10-24 19:40:17,"dependencies,javascript"
langchain-ai/langgraph,6323,chore: bump core dep for prebuilt,closed,2025-10-22 15:01:12,2025-10-29 16:51:51,
langchain-ai/langgraph,6322,fix(sdk-py): remove `body` param (`Auth.authenticate`),closed,2025-10-22 14:51:13,2025-10-22 17:35:16,
langchain-ai/langgraph,6321,chore: port tool node improvements back to langgraph,closed,2025-10-22 12:14:03,2025-10-29 16:58:07,
langchain-ai/langgraph,6320,Superstep parallelism: a slow sibling appears to block downstream progress of an unrelated fast branch,closed,2025-10-21 21:35:08,2025-11-07 19:34:37,"bug,pending"
langchain-ai/langgraph,6319,feat: Add IFU Comparative Analysis System,open,2025-10-21 16:23:49,,
langchain-ai/langgraph,6318,ToolRuntime not supported in LangGraph's built-in ToolNode,closed,2025-10-21 11:11:46,2025-10-29 18:47:47,"bug,pending"
langchain-ai/langgraph,6316,fix(langgraph): dont persist UntrackedValue,closed,2025-10-20 23:53:57,2025-10-28 23:40:23,
langchain-ai/langgraph,6315,chore: Allow checkpoint 3.0 in 0.6.*,closed,2025-10-20 23:27:33,2025-10-20 23:33:14,
langchain-ai/langgraph,6314,docs(langgraph): update streaming guide links,closed,2025-10-20 20:16:33,2025-11-07 12:46:49,
langchain-ai/langgraph,6313,release: Checkpointers 3.0,closed,2025-10-20 17:30:49,2025-10-20 18:31:55,
langchain-ai/langgraph,6312,LangMem compatibility issue with langgraph 1.0.0,closed,2025-10-20 07:56:38,2025-11-07 20:02:38,"bug,pending"
langchain-ai/langgraph,6311,chore(deps): bump langchain-core from 1.0.0a1 to 1.0.0 in /libs/langgraph,closed,2025-10-20 06:41:43,2025-10-27 06:47:26,"dependencies,python"
langchain-ai/langgraph,6310,chore(deps): bump uvloop from 0.21.0beta1 to 0.22.1 in /libs/langgraph,closed,2025-10-20 06:40:32,2025-11-07 12:33:43,"dependencies,python"
langchain-ai/langgraph,6308,style: update docstrings to reference `StateGraph`,closed,2025-10-20 02:09:42,2025-11-07 12:47:29,
langchain-ai/langgraph,6307,fix: remove SDK inline links,closed,2025-10-20 02:09:08,2025-11-07 12:56:01,
langchain-ai/langgraph,6305,chore(deps): upgrade dependencies with `uv lock --upgrade`,closed,2025-10-19 00:36:46,2025-11-07 12:32:55,dependencies
langchain-ai/langgraph,6304,feat(cli): add modern Python dependency manager support to langgraph dev (#5463),closed,2025-10-18 04:28:15,2025-11-07 21:33:26,enhancement
langchain-ai/langgraph,6303,Ëá™Âä®‰ªòÊ¨æ,closed,2025-10-18 02:20:18,2025-10-20 17:16:53,
langchain-ai/langgraph,6302,SummarizationNode causing invalid chat history?,closed,2025-10-17 21:26:51,2025-11-07 12:10:54,"bug,pending"
langchain-ai/langgraph,6301,fix: rollback doc redirects,closed,2025-10-17 20:38:34,2025-10-17 20:54:42,
langchain-ai/langgraph,6300,release: langgraph + langgraph-prebuilt v1.0.0,closed,2025-10-17 19:05:42,2025-10-17 19:15:30,
langchain-ai/langgraph,6299,fix(docs): fix catchall redirect,closed,2025-10-17 11:23:58,2025-10-17 11:27:42,
langchain-ai/langgraph,6298,feat: adding cursory Python 3.14 support,closed,2025-10-17 09:35:19,2025-10-17 12:26:52,
langchain-ai/langgraph,6297,style: fixes for ref docs,closed,2025-10-17 00:30:05,2025-10-17 00:58:17,
langchain-ai/langgraph,6296,chore: release rcs for prebuilt + langgraph,closed,2025-10-17 00:29:55,2025-10-17 00:35:56,
langchain-ai/langgraph,6295,chore(prebuilt): un-deprecate tool node for now,closed,2025-10-17 00:21:07,2025-10-17 00:27:25,
langchain-ai/langgraph,6294,fix(cli): install local deps in editable mode,closed,2025-10-16 23:50:05,2025-10-17 00:42:11,
langchain-ai/langgraph,6293,Feature/add azure tracer examples,closed,2025-10-16 22:10:33,2025-10-16 22:10:48,
langchain-ai/langgraph,6292,chore(docs): Fix redirects,closed,2025-10-16 17:56:38,2025-10-16 18:01:55,
langchain-ai/langgraph,6290,Subgraphs channels reducer updating out of turn,open,2025-10-16 13:33:40,,"bug,pending"
langchain-ai/langgraph,6289,chore: drop Python 3.9 (and syntax),closed,2025-10-16 11:49:08,2025-10-17 00:17:47,
langchain-ai/langgraph,6288,LangGraph Platform Deployment suddenly fail to deploy with multiple dependencies,closed,2025-10-16 11:03:56,2025-10-17 00:42:12,"bug,pending"
langchain-ai/langgraph,6287,chore(docs): Update OpenAPI spec from LangGraph API v0.4.42,closed,2025-10-16 09:06:06,2025-10-16 11:15:52,
langchain-ai/langgraph,6286,feat(langgraph): add Overwrite to bypass reducer,closed,2025-10-15 20:40:53,2025-10-27 18:31:36,
langchain-ai/langgraph,6285,chore(docs): Update OpenAPI spec from LangGraph API v0.4.39,closed,2025-10-15 18:28:14,2025-10-16 11:16:05,
langchain-ai/langgraph,6283,docs: Update lgp home redirect to deployments,closed,2025-10-15 14:21:05,2025-10-15 14:47:28,
langchain-ai/langgraph,6281,fix: rename away from LangGraph Platform,closed,2025-10-15 04:15:42,2025-10-15 18:27:15,
langchain-ai/langgraph,6280,feat(checkpoint): use PyArrow/msgpack for pandas serialization,open,2025-10-14 17:50:19,,
langchain-ai/langgraph,6277,add overwrite command,closed,2025-10-14 00:08:17,2025-10-16 11:03:11,
langchain-ai/langgraph,6276,feat(cli): cache optimized dockerfile,open,2025-10-13 16:54:46,,
langchain-ai/langgraph,6275,"Stream Graph with stream_mode= 'messages', gives error :  TypeError: Type is not msgpack serializable: AIMessage",closed,2025-10-13 16:28:37,2025-10-15 17:03:21,"bug,pending"
langchain-ai/langgraph,6273,chore(deps): bump astral-sh/setup-uv from 6 to 7,closed,2025-10-13 06:30:07,2025-10-14 14:55:28,"dependencies,github_actions"
langchain-ai/langgraph,6272,KeyError: when using subgraph inside Parent workflow,closed,2025-10-11 19:32:30,2025-10-14 21:52:51,"bug,pending"
langchain-ai/langgraph,6271,feat(checkpoint): serialize pydantic network types,open,2025-10-11 15:01:34,,
langchain-ai/langgraph,6270,UI messages from nested agents don't stream after interrupt/resume,closed,2025-10-11 06:09:10,2025-10-14 22:42:34,"bug,pending"
langchain-ai/langgraph,6269,"chore: Restrict ""json"" type deserialization",closed,2025-10-10 22:14:57,2025-10-20 17:18:36,
langchain-ai/langgraph,6268,fix(checkpoint): Add proper async Redis client support to RedisCache,open,2025-10-10 22:11:49,,
langchain-ai/langgraph,6267,Running a local server chat UI error: `fetchStateHistory` must be set to `true` to use `history`,closed,2025-10-10 18:59:28,2025-10-13 09:06:17,"bug,pending"
langchain-ai/langgraph,6265,"When langgraph's stream_mode includes custom, the cached results do not contain custom data.",open,2025-10-10 11:14:45,,"enhancement,question,requires design"
langchain-ai/langgraph,6262,docs(langgraph): fixing human_in_the_loop demo code,closed,2025-10-10 05:47:18,2025-10-16 11:18:09,
langchain-ai/langgraph,6279,[langgraph]: graph.get_state_history(config) cannot output the complete snapshot history,closed,2025-10-10 03:28:01,2025-10-14 22:48:38,
langchain-ai/langgraph,6261,"fix: Revert ""fix(cli): rename studio to debugger (#6246)""",closed,2025-10-09 23:29:29,2025-10-09 23:35:36,
langchain-ai/langgraph,6260,docs: style linting,closed,2025-10-09 23:25:14,2025-10-16 11:25:50,
langchain-ai/langgraph,6259,docs: relocate init args to `__init__`,closed,2025-10-09 21:45:50,2025-10-16 11:11:42,"documentation,v1"
langchain-ai/langgraph,6258,[DRAFT] feat(prebuilt): add pre/post hooks for structured response in create_react_agent,closed,2025-10-09 18:10:35,2025-10-16 11:17:03,
langchain-ai/langgraph,6257,chore(langgraph): bump langgraph version,closed,2025-10-09 15:55:22,2025-10-09 16:39:43,
langchain-ai/langgraph,6254,feat(langgraph): optimize interrupt task scheduling,closed,2025-10-08 22:50:37,2025-10-24 23:09:36,
langchain-ai/langgraph,6252,fix(langgraph): revert selective interrupt task scheduling,closed,2025-10-08 16:08:56,2025-10-08 19:34:02,
langchain-ai/langgraph,6251,chore(cli): bump to 0.4.3,closed,2025-10-08 15:25:48,2025-10-08 15:31:54,
langchain-ai/langgraph,6250,Fix Command goto routing to execute exclusively (#6248),closed,2025-10-08 12:26:12,2025-10-08 21:03:36,
langchain-ai/langgraph,6249,Fix Command goto routing to execute exclusively (#6248),closed,2025-10-08 12:17:45,2025-10-08 21:19:13,
langchain-ai/langgraph,6248,Command goto not routing correctly in astream,closed,2025-10-08 07:19:33,2025-11-07 23:57:48,"bug,pending"
langchain-ai/langgraph,6247,langgraph.cache.redis.RedisCache does not work with async redis client,open,2025-10-08 03:02:55,,"bug,pending"
langchain-ai/langgraph,6246,fix(cli): rename studio to debugger,closed,2025-10-08 00:04:53,2025-10-08 16:40:27,
langchain-ai/langgraph,6245,chore(langgraph): bump version,closed,2025-10-07 19:47:02,2025-10-07 20:45:25,
langchain-ai/langgraph,6244,chore(checkpoint): bump patch version,closed,2025-10-07 00:51:51,2025-10-07 17:41:25,
langchain-ai/langgraph,6243,chore(cli): re-word schema arguments,closed,2025-10-06 23:28:05,2025-10-16 12:28:10,
langchain-ai/langgraph,6242,docs: Update old LDP site redirects to Platform docs merge,closed,2025-10-06 19:06:26,2025-10-14 14:23:20,
langchain-ai/langgraph,6241,LangGraph CLI treats InjectedState as required tool parameter and fails the validation,closed,2025-10-06 16:42:55,2025-11-08 00:48:53,"bug,pending"
langchain-ai/langgraph,6240,TypeError in InMemoryStore.alist_namespaces when namespace contains None values,closed,2025-10-06 09:43:16,2025-11-08 00:01:34,"bug,pending"
langchain-ai/langgraph,6239,thread_id too long for Postgres checkpoint,open,2025-10-06 08:12:52,,documentation
langchain-ai/langgraph,6237,chore(deps): upgrade dependencies with `uv lock --upgrade`,closed,2025-10-05 00:35:16,2025-10-16 11:13:39,dependencies
langchain-ai/langgraph,6236,fix(checkpoint): handle metadata.writes when serializing old checkpoints with Jsonb,closed,2025-10-03 22:52:52,2025-10-06 18:27:35,
langchain-ai/langgraph,6235,React agent returns empty response,closed,2025-10-03 08:14:46,2025-11-07 12:10:20,"bug,pending"
langchain-ai/langgraph,6234,Fix incorrect escape sequence in regular expression in `base.py`.,open,2025-10-01 20:56:55,,
langchain-ai/langgraph,6233,fix(langgraph): task result from stream mode debug / tasks should match format from get_state_history / get_state,closed,2025-10-01 02:20:45,2025-10-03 16:06:59,
langchain-ai/langgraph,6232,feat(langgraph): supports calling (multiple) handoffs and non-handoff tools at the same time,closed,2025-10-01 00:03:11,2025-11-07 12:48:48,
langchain-ai/langgraph,6228,docs(docs): remove duplicate code blocks in human-in-the-loop tutorial,closed,2025-09-30 20:57:23,2025-10-16 12:02:32,
langchain-ai/langgraph,6226,"chore: update `langchain-core` dependency version constraints, bump lock",closed,2025-09-30 17:35:47,2025-10-16 11:04:00,
langchain-ai/langgraph,6224,"Agent created with `create_react_agent` does not retain private state properties passed from earlier nodes in the graph, causing validation error",closed,2025-09-30 05:28:51,2025-11-07 10:53:47,"bug,pending"
langchain-ai/langgraph,6223,fix(langgraph): fix nested subgraph checkpoint replay,closed,2025-09-30 02:22:45,2025-09-30 18:33:58,
langchain-ai/langgraph,6214,Improve LG/chain tracing devex,open,2025-09-29 06:36:47,,enhancement
langchain-ai/langgraph,6213,/join endpoint returns HTTP 200 with error payload instead of proper HTTP error status codes,open,2025-09-29 05:37:17,,"bug,pending"
langchain-ai/langgraph,6212,Anthropic's web_Search server tool not supported by create_react_agent,closed,2025-09-28 15:46:22,2025-11-07 12:10:03,"bug,pending"
langchain-ai/langgraph,6209,fix(langgraph): accept Sequence[...] for Messages,open,2025-09-26 16:59:05,,
langchain-ai/langgraph,6207,Type checker error: langgraph.message:add_messages doesn‚Äôt accept list[BaseMessage],open,2025-09-26 16:26:27,,"bug,pending"
langchain-ai/langgraph,6203,feat(prebuilt): allow passing tool_bind_kwargs through create_react_agent,closed,2025-09-26 09:18:45,2025-09-29 09:52:12,
langchain-ai/langgraph,6202,Frontend stops receiving stream events midway but the stream continues on langsmith,open,2025-09-26 06:22:42,,"bug,pending"
langchain-ai/langgraph,6195,fix(langgraph): fix supersteps not populating task.result field,closed,2025-09-25 01:15:13,2025-09-30 19:52:59,
langchain-ai/langgraph,6192,test(checkpoint): add network timeout test for Redis cache,open,2025-09-24 21:35:31,,
langchain-ai/langgraph,6182,Error in openapi.json schema,open,2025-09-23 11:29:51,,"bug,pending"
langchain-ai/langgraph,6178,aiosqlite.Connection does not have is_alive attribute,closed,2025-09-22 13:49:29,2025-09-22 14:07:50,"bug,pending"
langchain-ai/langgraph,6445,AddableUpdatesDict does not support right-side addition,open,2025-09-22 02:28:11,,bug
langchain-ai/langgraph,6174,TypeError: '<=' not supported between instances of 'int' and 'str',closed,2025-09-20 01:02:07,2025-11-07 21:17:02,"bug,pending"
langchain-ai/langgraph,6170,More robust error handling for nodes,open,2025-09-19 19:39:58,,enhancement
langchain-ai/langgraph,6160,refactor(checkpoint): renaming checkpoint savers to checkpointers for consistency,closed,2025-09-17 11:27:51,2025-10-06 20:21:50,
langchain-ai/langgraph,6158,fix(langgraph): selective interrupt task scheduling,closed,2025-09-17 02:47:19,2025-10-06 20:11:53,
langchain-ai/langgraph,6154,fix(checkpoint-postgres): ensure vector extension is created only if not exists,closed,2025-09-16 14:23:21,2025-11-07 16:42:16,
langchain-ai/langgraph,6139,fix(prebuilt): ignore nested warnings as a result of subclassing,closed,2025-09-12 15:10:12,2025-09-14 23:32:31,
langchain-ai/langgraph,6132,Refactor injection logic,closed,2025-09-11 14:57:17,2025-11-07 20:06:55,
langchain-ai/langgraph,6123,rename `BaseCheckpointSaver`,closed,2025-09-10 11:56:13,2025-10-06 20:20:00,change
langchain-ai/langgraph,6115,ainvoke/invoke method in RemoteGraph lacks context parameter ‚Äî how to pass execution context remotely?,open,2025-09-09 13:57:02,,bug
langchain-ai/langgraph,6412,`ToolNode` `ainvoke` freezes if `sse_read_timeout`,open,2025-09-08 20:12:35,,"bug,help wanted"
langchain-ai/langgraph,6104,PostgresSaver doesn't work,closed,2025-09-08 14:27:41,2025-09-12 23:05:19,"bug,pending"
langchain-ai/langgraph,6103,"fix: generic type args should be serialized for de-serialization, clo‚Ä¶",closed,2025-09-08 10:57:42,2025-10-16 11:11:25,
langchain-ai/langgraph,6102,jsonplus cannot deserialize generic pydantic(v2) types correctly,open,2025-09-08 10:32:13,,"enhancement,checkpointer"
langchain-ai/langgraph,6095,fix(langgraph): task result from stream mode debug / tasks should match format from getStateHistory / getState,closed,2025-09-07 23:16:10,2025-10-03 16:08:57,
langchain-ai/langgraph,6093,release(langgraph): v1 working branch,closed,2025-09-07 16:54:32,2025-10-16 11:11:23,
langchain-ai/langgraph,6073,task result from stream mode debug / tasks should match format from get_state_history / get_state,closed,2025-09-03 19:51:43,2025-10-03 16:07:28,bug
langchain-ai/langgraph,6062,LangGraph V1 Alpha! üöÄ ,closed,2025-09-02 16:34:55,2025-11-07 21:17:37,
langchain-ai/langgraph,6052,DOC: Add a link for viewing older versions of the LangGraph documentation,closed,2025-09-01 06:51:37,2025-09-09 18:03:40,documentation
langchain-ai/langgraph,6037,Error: transfer_back_to_supervisor is not a valid tool,closed,2025-08-28 17:50:41,2025-11-07 12:09:12,"bug,pending"
langchain-ai/langgraph,6019,Please publish releases for all components affected by the Redis caching feature,open,2025-08-26 18:07:41,,"bug,pending"
langchain-ai/langgraph,6005,Langgraph's defer=True logic (or way of doing things) doesn't solve much and is fundamentally broken,closed,2025-08-25 11:10:19,2025-09-10 22:52:16,"bug,pending"
langchain-ai/langgraph,6003,LangGraph: Checkpointer causes MySQL collation errors (1267/1366) + Hindi TTS loses Devanagari matras,closed,2025-08-25 06:33:21,2025-11-07 20:08:35,"bug,pending"
langchain-ai/langgraph,5990,Inject Runtime into tools,closed,2025-08-22 11:56:59,2025-09-08 14:56:58,open-swe-max-auto
langchain-ai/langgraph,5987,DOC: support for a2a with langchain/langgraph,closed,2025-08-21 20:11:04,2025-09-10 15:21:44,documentation
langchain-ai/langgraph,5925,Langgraph postgres setup error when migrating for the first time.,open,2025-08-18 05:54:39,,"bug,pending"
langchain-ai/langgraph,5891,`TypeError: Type is not msgpack serializable: Send` occurs when using `create_react_agent` with a tool that uses `BaseStore`.,closed,2025-08-13 14:00:23,2025-10-08 01:07:45,bug
langchain-ai/langgraph,5872,Refactor create_react_agent to Enforce Structured Output in Agent Node,closed,2025-08-11 19:26:26,2025-08-22 11:52:08,
langchain-ai/langgraph,5856,useStream hook doesn't differentiate between messages from subgraphs and the main graph,open,2025-08-07 20:04:12,,enhancement
langchain-ai/langgraph,5829,DOC: Document behavior when node has both Command.goto and static edge,open,2025-08-05 13:28:53,,
langchain-ai/langgraph,5824,docs: Redirects for new docs,closed,2025-08-04 19:03:43,2025-10-16 16:55:30,
langchain-ai/langgraph,5819,httpx.ReadError not properly handled in LangGraph Python SDK,open,2025-08-04 11:42:04,,"bug,pending"
langchain-ai/langgraph,5790,"`langgraph dev` Ignores Checkpointer Configuration, Forcing In-Memory Storage and Preventing State Persistence**",closed,2025-08-01 01:22:22,2025-10-09 18:05:06,"bug,pending"
langchain-ai/langgraph,5788,Nicer configuration for tracing,closed,2025-08-01 00:10:14,2025-10-02 16:54:05,enhancement
langchain-ai/langgraph,5776,Better support for `Runtime` with tools,open,2025-07-31 14:55:24,,
langchain-ai/langgraph,5769,A JSON serializable problem on PostgresSaver,closed,2025-07-31 12:46:03,2025-10-06 18:27:45,"bug,pending"
langchain-ai/langgraph,5682,Can not stop sub graph when asyncio.CancelledError occurred,closed,2025-07-28 11:57:09,2025-09-11 23:32:42,"bug,pending"
langchain-ai/langgraph,5675,AsyncPostgresSaver  consistently fails with psycopg.AsyncPipeline [BAD] / psycopg.OperationalError: consuming input failed: SSL connection has been closed unexpectedly,open,2025-07-26 15:26:19,,"bug,pending"
langchain-ai/langgraph,5672,Run Cancellation Causes Loss of Streamed State Not Yet Persisted as a Checkpoint,open,2025-07-25 20:28:38,,"bug,pending"
langchain-ai/langgraph,5665,`with_structured_output()` schemas inside of a tool are being binded to an agent that uses that tool,closed,2025-07-25 11:15:40,2025-11-07 10:59:04,"bug,pending"
langchain-ai/langgraph,5656,TypeError: 'NoneType' object is not iterable when resume is used in thread.submit command,open,2025-07-25 01:21:02,,"bug,pending"
langchain-ai/langgraph,5633,Node name of the agent graph created by prebuilt.create_react_agent,closed,2025-07-23 15:37:33,2025-11-07 11:01:26,enhancement
langchain-ai/langgraph,5628,ERROR in pre_model_hook for condense messages,closed,2025-07-23 07:51:40,2025-11-07 12:08:22,"bug,pending"
langchain-ai/langgraph,5582,"LangGraph 0.3+ breaks the code with error: ""Each tool_result block must have a corresponding tool_use block in the previous message""",closed,2025-07-18 23:54:12,2025-11-07 12:07:16,"bug,pending"
langchain-ai/langgraph,5548,ReAct Agent doesn't throw `GraphRecursionError`,closed,2025-07-16 23:46:25,2025-11-07 12:06:31,"bug,pending"
langchain-ai/langgraph,5529,refactor(langgraph): make constants generally private with a few select exports,closed,2025-07-16 12:36:19,2025-07-16 13:27:04,
langchain-ai/langgraph,5528,Streaming from ReAct agents added as tools to a ReAct agent does not work,open,2025-07-16 11:54:01,,"bug,pending"
langchain-ai/langgraph,5526,how can create_tool_calling_agent get reasoning output?,closed,2025-07-16 10:40:14,2025-07-20 13:44:41,"bug,pending"
langchain-ai/langgraph,5504,fix(checkpoint): fix AsyncBatchedBaseStore getting stuck,closed,2025-07-14 22:12:55,2025-07-17 10:50:18,
langchain-ai/langgraph,5463,Support uv with  `langgraph dev`,open,2025-07-12 03:23:30,,"enhancement,server"
langchain-ai/langgraph,5439,refactor(uv): Migrate to UV Workspace approach,open,2025-07-10 15:53:59,,awaiting author revision
langchain-ai/langgraph,5415,fix[langgraph]: Fix error on < Python 3.11 with async context managers,closed,2025-07-09 17:23:17,2025-11-07 12:36:54,awaiting author revision
langchain-ai/langgraph,5360,POSTGRES_URI_CUSTOM Not Working with `langgraph dev`,closed,2025-07-06 05:29:58,2025-07-20 13:58:23,documentation
langchain-ai/langgraph,5321,"Recommend top level thread_id, etc in config",open,2025-07-02 20:54:36,,maintainer
langchain-ai/langgraph,5265,"UI bundler crashes under uvloop: TypeError Expected dict, got _Environ",closed,2025-06-30 10:46:49,2025-09-10 12:38:38,invalid
langchain-ai/langgraph,5254,Drop Python 3.9 support,closed,2025-06-29 16:17:01,2025-10-17 09:44:50,ci
langchain-ai/langgraph,5225,Default value of state variable not working with reducer function,open,2025-06-27 12:50:19,,"bug,help wanted"
langchain-ai/langgraph,5157,create_react_agent with `post_model_hook` - early stop on non unique tool call id,open,2025-06-22 13:41:54,,bug
langchain-ai/langgraph,5136,Feat:Add PostgresCache Store,closed,2025-06-18 08:20:46,2025-11-15 04:47:06,
langchain-ai/langgraph,5077,Pandas serialization/deserialization logic with msgpack,open,2025-06-12 13:35:59,,"enhancement,help wanted"
langchain-ai/langgraph,5071,running app using cli langgraph dev makes agent unable to call tools (bad tool name repeated),closed,2025-06-12 03:15:03,2025-11-07 12:04:04,"bug,pending"
langchain-ai/langgraph,5054,TypeError: Type is not msgpack serializable: ToolMessage,open,2025-06-11 11:42:35,,"bug,pending"
langchain-ai/langgraph,5040,remove use of `importlib.metadata` for version access,open,2025-06-10 13:49:30,,"good first issue,performance,help wanted"
langchain-ai/langgraph,5020,Enable `ty` across the repo,open,2025-06-09 15:36:30,,"code quality,type checking,help wanted"
langchain-ai/langgraph,5004,Remove postgres shallow checkpointer,closed,2025-06-09 14:18:44,2025-06-09 14:18:47,
langchain-ai/langgraph,6457,Feature proposition: Visual Representation of a Graph in a LangGraph application,open,2025-06-07 12:25:52,,
langchain-ai/langgraph,4973,üöß LangGraph v1 roadmap ‚Äì feedback wanted!,open,2025-06-05 14:41:50,,"help wanted,meta"
langchain-ai/langgraph,4956,TypeError: Type is not msgpack serializable: AIMessage,open,2025-06-04 11:02:51,,"bug,pending"
langchain-ai/langgraph,4866,Make state access for subgraphs easier from parent graph,open,2025-05-29 15:25:34,,"documentation,enhancement"
langchain-ai/langgraph,4756,create_react_agent with structured_response  omits or alters the raw LLM output despite including system prompt ,closed,2025-05-20 10:25:05,2025-11-07 11:02:25,bug
langchain-ai/langgraph,4754,create_react_agent fails with a Runnable chain supplied as the model when bind_tools gets called,closed,2025-05-20 05:53:32,2025-11-07 11:03:25,"bug,change"
langchain-ai/langgraph,6444,Resume to a specific subgraph node after interrupt,open,2025-03-27 11:53:18,,
langchain-ai/langgraph,4008,[NEW] Implement ResilientPostgresSaver for improved error handling and connection retries,open,2025-03-25 10:15:30,,awaiting author revision
langchain-ai/langgraph,3723,ModuleNotFoundError: No module named 'langgraph',closed,2025-03-06 18:05:08,2025-03-06 23:10:45,
langchain-ai/langgraph,3441,Checkpointer attempts to serialize RunnableConfig -- Object of type Foobar is not JSON serializable,closed,2025-02-14 13:50:34,2025-02-14 17:22:28,
langchain-ai/langgraph,3421,LangGraph custom auth not working for self-hosted deployment without version update,closed,2025-02-13 17:11:19,2025-02-13 19:44:20,
langchain-ai/langgraph,3275,"Interrupt using the same old question, when invoked second time.",open,2025-02-01 05:54:53,,bug
langchain-ai/langgraph,3261,LangSmith Studio does not work in Chrome due to CORS issue,closed,2025-01-31 10:04:27,2025-01-31 20:51:08,
langchain-ai/langgraph,3062,getting double repetative Output from agents tools langraph,closed,2025-01-16 09:20:44,2025-03-05 00:43:28,invalid
langchain-ai/langgraph,2920,LLM is slow within langgraph agent,closed,2025-01-02 17:40:22,2025-01-14 15:33:30,
langchain-ai/langgraph,2549,I want to deploy the langgraph platform on the intranet. Can I do this without entering the langsmith key?,closed,2024-11-27 09:55:14,2024-12-18 14:37:50,
langchain-ai/langgraph,2220,ToolNode doesn't detect `InjectedState` annotations for tools with Pydantic `args_schema`,closed,2024-10-29 19:42:33,2024-10-31 01:19:03,
langchain-ai/langgraph,1916,Validation Errors When Passing Graph State To Tools,closed,2024-09-30 18:49:56,2024-09-30 18:59:57,
langchain-ai/langgraph,1153,Ollama tool calls not working via openai proxy only when using langgraph,closed,2024-07-27 01:01:30,2025-11-07 11:45:10,bug
langchain-ai/langgraph,954,Final node doesn't wait for all other incoming nodes,closed,2024-07-08 21:49:43,2024-07-09 18:19:28,
langchain-ai/langgraph,137,How to stream token of agent response in agent supervisor?,closed,2024-02-22 17:40:04,2025-05-23 23:37:22,
openai/openai-cookbook,2263,Feedback on the Recent Safety Wrapper and Its Impact on ChatGPT (GPT-5.1),open,2025-11-30 03:23:20,,
openai/openai-cookbook,2262,Fixes the Agent SDK examples that are not working,open,2025-11-29 20:09:26,,
openai/openai-cookbook,2261,docs: add prisma postgres example,open,2025-11-28 00:42:24,,
openai/openai-cookbook,2260,Bump node-forge from 1.3.1 to 1.3.2 in /examples/voice_solutions/one_way_translation_using_realtime_api,open,2025-11-26 22:57:34,,"dependencies,javascript"
openai/openai-cookbook,2259,Fix: nbviewer does not render notebooks without any code cells correctly,open,2025-11-25 23:59:53,,
openai/openai-cookbook,2258,Minh/feedback correction,open,2025-11-25 19:29:32,,
openai/openai-cookbook,2257,HealthBench RFT - Updates to use synthetic data,closed,2025-11-24 09:30:50,2025-11-24 09:40:16,
openai/openai-cookbook,2256,GuessGuard ‚Äî Modular Uncertainty Control Layer (v2.0),open,2025-11-23 18:05:48,,
openai/openai-cookbook,2254,[PROBLEM],open,2025-11-22 08:21:48,,bug
openai/openai-cookbook,2253,üôè A note of appreciation from a user ‚Äî written together with ChatGPT,open,2025-11-21 15:00:09,,
openai/openai-cookbook,2252,Update markdown headers in Realtime out-of-band transcription example‚Ä¶,closed,2025-11-20 23:53:48,2025-11-20 23:55:27,
openai/openai-cookbook,2251,Add gitlab example,open,2025-11-20 06:40:35,,
openai/openai-cookbook,2250,Minh/out of band transcription,closed,2025-11-20 05:22:00,2025-11-20 23:37:17,
openai/openai-cookbook,2249,[PROBLEM]‚ÄûFeedback: LLM False-Recollection Phenomenon (User Trust Impact)‚Äú,open,2025-11-19 20:34:52,,bug
openai/openai-cookbook,2248,Code modernization cookbook,closed,2025-11-19 20:17:28,2025-11-19 20:28:09,
openai/openai-cookbook,2247,Code modernization cookbook,closed,2025-11-19 18:14:20,2025-11-19 18:41:56,
openai/openai-cookbook,2246,[FEATURE],open,2025-11-19 06:18:55,,
openai/openai-cookbook,2244,Add Optional ‚ÄúDeterministic Plan Mode‚Äù for Agent Tool Execution,open,2025-11-18 00:50:59,,
openai/openai-cookbook,2243,Fix comma in vllm sample code,open,2025-11-17 13:38:11,,
openai/openai-cookbook,2242,[FEATURE],closed,2025-11-16 21:18:18,2025-11-16 21:18:37,
openai/openai-cookbook,2241,tiktoken/core.py,open,2025-11-16 13:17:09,,
openai/openai-cookbook,2240,Bump js-yaml from 4.1.0 to 4.1.1 in /examples/mcp/building-a-supply-chain-copilot-with-agent-sdk-and-databricks-mcp/ui,open,2025-11-15 11:41:02,,"dependencies,javascript"
openai/openai-cookbook,2239,Alive AI,open,2025-11-14 00:49:57,,
openai/openai-cookbook,2238,Add coding agent with GPT-5.1 cookbook guide,closed,2025-11-13 18:31:32,2025-11-13 18:34:55,
openai/openai-cookbook,2237,5.1 fix,closed,2025-11-13 18:04:06,2025-11-13 18:05:19,
openai/openai-cookbook,2236,Add 1 liner to codex prompting guide,closed,2025-11-13 17:36:58,2025-11-13 17:38:38,
openai/openai-cookbook,2235,[docs] GPT-5.1 Prompting Guide,closed,2025-11-13 17:19:03,2025-11-13 17:43:53,
openai/openai-cookbook,2234,minor issues on model selection,closed,2025-11-12 16:24:49,2025-11-13 10:28:13,
openai/openai-cookbook,2233,[SUPPORT],open,2025-11-10 23:59:46,,support
openai/openai-cookbook,2232,yy,open,2025-11-10 16:43:19,,
openai/openai-cookbook,2231,[FEATURE],open,2025-11-10 16:42:57,,
openai/openai-cookbook,2230,[FEATURE],open,2025-11-10 16:42:52,,
openai/openai-cookbook,2229,[FEATURE],open,2025-11-10 16:42:48,,
openai/openai-cookbook,2228,ÊãâÂèñÊ®ôË™å,closed,2025-11-10 15:58:57,2025-11-15 15:12:02,
openai/openai-cookbook,2227,Á∂≤ÁãÄÂ§öÁµÑÂêàÂèñÊç®Ê©üÂà∂Á≥ªÁµ±-Èõ≤Á´ØÊãâÂèñ,closed,2025-11-10 15:54:49,2025-11-10 16:41:00,
openai/openai-cookbook,2226,(removed),open,2025-11-08 12:09:08,,
openai/openai-cookbook,2225,Update cookbook wording,open,2025-11-05 19:32:33,,
openai/openai-cookbook,2224,update code review cookbook and test agent.md,closed,2025-11-05 18:46:00,2025-11-05 19:30:47,
openai/openai-cookbook,2223,Fix typo on code-review cookbook and adjust AGENTS.md,closed,2025-11-05 16:48:21,2025-11-05 18:22:25,
openai/openai-cookbook,2222,"Broken ""View as Markdown"" link on article",open,2025-11-04 18:20:32,,bug
openai/openai-cookbook,2221,docs: explain MIT License for better clarity,open,2025-11-04 16:49:29,,
openai/openai-cookbook,2220,autonomous agent retraining cookbook v1,closed,2025-11-04 01:38:01,2025-11-07 23:47:54,
openai/openai-cookbook,2219,[FEATURE] Feature Request: User-selectable context linking between conversations in ChatGPT,open,2025-11-03 13:35:04,,
openai/openai-cookbook,2218,[PROBLEM] Guardrail output mismatch with career analysis agent,open,2025-11-01 06:40:13,,bug
openai/openai-cookbook,2217,[FEATURE],open,2025-10-31 10:26:29,,
openai/openai-cookbook,2216,fix escaped characters in sampe policy,closed,2025-10-29 12:06:16,2025-10-29 12:06:23,
openai/openai-cookbook,2215,add new safeguard guide,closed,2025-10-29 11:58:09,2025-10-29 12:00:00,
openai/openai-cookbook,2214,Feature Request: Secure Vault Connector for ChatGPT (Offline Local Storage),open,2025-10-29 11:30:39,,
openai/openai-cookbook,2213,Rename CONTRIBUTING.md to CONTRIBUTING.md..,closed,2025-10-29 03:41:16,2025-10-29 03:42:24,
openai/openai-cookbook,2212,üëãRename gpt-5_prompting_guide.ipynb to gpt-5_prompting_guide.ipynb.,open,2025-10-29 03:39:26,,
openai/openai-cookbook,2211,Enrich tag data,open,2025-10-28 21:30:55,,
openai/openai-cookbook,2210,[PROBLEM] ChatGPT references Cloudflare IPFS gateway (now discontinued),open,2025-10-28 20:39:06,,bug
openai/openai-cookbook,2208,[PROBLEM] `codex mcp` doesn't start codex CLI as an MCP server,open,2025-10-28 04:12:50,,bug
openai/openai-cookbook,2207,sudo su,open,2025-10-26 01:53:47,,
openai/openai-cookbook,2206,[FEATURE] Making of chatkit.world,open,2025-10-24 11:19:30,,
openai/openai-cookbook,2205,Fix type and add additional sections on prompt and structured outputs,closed,2025-10-24 05:39:44,2025-10-28 23:52:56,
openai/openai-cookbook,2204,sudo su && Update README.md to include 'sudo su' command,closed,2025-10-23 16:37:34,2025-10-23 19:13:42,
openai/openai-cookbook,2203,[PROBLEM] `CUDA error: device-side assert triggered` when generating responses with GPT-OSS-20B,open,2025-10-23 10:07:13,,bug
openai/openai-cookbook,2202,D'fa,open,2025-10-22 23:36:57,,
openai/openai-cookbook,2201,Build code review with codex sdk,closed,2025-10-22 04:55:08,2025-10-23 21:00:57,
openai/openai-cookbook,2200,Fix typo in Codex execution plans documentation,closed,2025-10-21 15:16:34,2025-10-23 19:14:37,
openai/openai-cookbook,2199,Fix outdated link in libraries.txt for JuliaML,closed,2025-10-21 02:37:13,2025-10-24 14:45:26,
openai/openai-cookbook,2198,Bump vite from 6.3.5 to 6.4.1 in /examples/mcp/building-a-supply-chain-copilot-with-agent-sdk-and-databricks-mcp/ui,open,2025-10-21 01:50:52,,"dependencies,javascript"
openai/openai-cookbook,2197,Add AgentKit walkthrough cookbook,closed,2025-10-16 21:11:42,2025-10-20 15:53:05,
openai/openai-cookbook,2196,Wrap codex exec plans article at 100 characters,open,2025-10-14 23:10:03,,codex
openai/openai-cookbook,2195,fix typo,closed,2025-10-14 21:14:49,2025-10-14 22:01:13,
openai/openai-cookbook,2194,small fixed to author + title,closed,2025-10-14 20:03:54,2025-10-14 20:06:01,
openai/openai-cookbook,2193,update fix bug w/ images,closed,2025-10-14 18:41:15,2025-10-14 18:51:07,
openai/openai-cookbook,2192,fix typo,open,2025-10-14 02:12:46,,
openai/openai-cookbook,2191,Mr gd crazy patch 1,closed,2025-10-11 17:26:47,2025-10-11 18:08:31,
openai/openai-cookbook,2190,byo realtime cookbook,closed,2025-10-11 08:33:45,2025-10-14 17:01:24,
openai/openai-cookbook,2189,[FEATURE],open,2025-10-11 05:27:35,,
openai/openai-cookbook,2188,Create SECURITY.md,closed,2025-10-10 14:37:14,2025-10-12 04:57:19,
openai/openai-cookbook,2187,Minor correction in session_memory.ipynb,open,2025-10-10 08:33:12,,
openai/openai-cookbook,2186,Enhance Realtime prompting guide with additional context on common tools,closed,2025-10-09 14:15:19,2025-10-09 16:37:35,
openai/openai-cookbook,2185,Document PLANS.md,closed,2025-10-08 18:13:10,2025-10-14 17:26:22,
openai/openai-cookbook,2184,Fix broken link,closed,2025-10-07 18:46:58,2025-10-07 18:50:38,
openai/openai-cookbook,2183,Fix broken links in evals cookbook,closed,2025-10-07 18:21:05,2025-10-07 18:33:02,
openai/openai-cookbook,2182,Update avatar and website url for my authors.yaml entry.,closed,2025-10-07 18:09:39,2025-10-08 19:03:38,
openai/openai-cookbook,2181,Fix links in Building_resilient_prompts_using_an_evaluation_flywheel.md,open,2025-10-07 07:21:31,,
openai/openai-cookbook,2180,Minh/sora2,closed,2025-10-06 17:58:42,2025-10-06 18:02:23,
openai/openai-cookbook,2179,Building resilient prompts with eval flywheel,closed,2025-10-06 17:28:07,2025-10-06 17:28:53,
openai/openai-cookbook,2178,Add assistant_weather_tool example: Assistants API + Open-Meteo tool,open,2025-10-06 12:48:18,,
openai/openai-cookbook,2177,docs: Simplify concurrent flow,open,2025-10-06 12:43:44,,
openai/openai-cookbook,2176,Charlie/autofix updates,closed,2025-10-04 01:15:05,2025-10-04 01:15:55,
openai/openai-cookbook,2175,Fix avatar URL,open,2025-10-03 18:49:41,,
openai/openai-cookbook,2174,Refactor to use github action,closed,2025-10-03 10:31:58,2025-10-03 19:39:29,
openai/openai-cookbook,2173,Fix images and give myself author credit,closed,2025-10-03 08:51:03,2025-10-03 16:11:17,
openai/openai-cookbook,2172,Image link fix,closed,2025-10-02 18:50:01,2025-10-02 19:08:27,
openai/openai-cookbook,2167,Add Roundtable MCP Server Example - Unified AI Assistant Management,open,2025-09-30 08:14:20,,Stale
openai/openai-cookbook,2166,Suggestion: add beginner example in Python,open,2025-09-29 21:37:16,,Stale
openai/openai-cookbook,2165,Agent SDK + Codex MCP,closed,2025-09-29 16:37:06,2025-10-02 15:40:48,
openai/openai-cookbook,2164,Compile all prompting-related example notebooks into a single directory,closed,2025-09-27 04:00:05,2025-11-02 09:57:25,
openai/openai-cookbook,2163,Rewrite the Cookbook introduction to make it more accessible,closed,2025-09-27 03:17:28,2025-11-02 09:57:24,
openai/openai-cookbook,2159,[PROBLEM],open,2025-09-23 15:41:32,,"bug,Stale"
openai/openai-cookbook,2158,[SUPPORT],open,2025-09-23 15:25:17,,"support,Stale"
openai/openai-cookbook,2154,GPT-5 loses context in multi-turn conversations,open,2025-09-20 10:11:27,,bug
openai/openai-cookbook,2153,[FEATURE],closed,2025-09-19 20:08:55,2025-11-30 02:16:36,Stale
openai/openai-cookbook,2150,[SUPPORT] Retrieve full content from Confluence pages,closed,2025-09-17 23:17:00,2025-11-28 02:11:01,"support,Stale"
openai/openai-cookbook,2146,[SUPPORT] Can't load quantised Gpt-oss-20b. (MxFP4) ,closed,2025-09-17 07:35:30,2025-11-28 02:11:03,"support,Stale"
openai/openai-cookbook,2142,Ai ,open,2025-09-15 12:26:18,,
openai/openai-cookbook,2140,[SUPPORT] Guidance on token counts for structured output schemas,open,2025-09-14 07:10:10,,
openai/openai-cookbook,2139,Bump axios from 1.8.2 to 1.12.0 in /examples/voice_solutions/one_way_translation_using_realtime_api,closed,2025-09-13 11:48:33,2025-11-23 02:16:50,"dependencies,javascript,Stale"
openai/openai-cookbook,2138,feat: add notebook explaining ChatCompletion to SFT transformation,closed,2025-09-13 00:52:24,2025-11-23 02:16:52,Stale
openai/openai-cookbook,2135,Fix #2134: Consolidate num_tokens_from_messages implementations,closed,2025-09-12 06:42:34,2025-11-23 02:16:54,Stale
openai/openai-cookbook,2134,[PROBLEM] `num_tokens_from_messages` versions inconsistent,closed,2025-09-12 06:21:25,2025-11-23 02:16:56,"bug,Stale"
openai/openai-cookbook,2133,Fix #2102: Add structured response token counting examples,closed,2025-09-12 06:18:33,2025-11-24 02:16:17,Stale
openai/openai-cookbook,2131,Specify recipient of tool response in harmony example,closed,2025-09-11 22:00:38,2025-11-22 02:08:41,Stale
openai/openai-cookbook,2130,Fix link to building TensorRT-LLM Docker documentation,open,2025-09-11 17:10:45,,Stale
openai/openai-cookbook,2128,Bump vite from 6.3.5 to 6.3.6 in /examples/mcp/building-a-supply-chain-copilot-with-agent-sdk-and-databricks-mcp/ui,closed,2025-09-10 02:03:15,2025-10-21 01:50:55,"dependencies,javascript"
openai/openai-cookbook,2127,[PROBLEM] multi_agent_portfolio_collaboration takes forever to run,closed,2025-09-09 14:03:58,2025-11-20 02:10:51,"bug,Stale"
openai/openai-cookbook,2126,Broken MD link,closed,2025-09-09 05:46:50,2025-11-20 02:10:52,"bug,Stale"
openai/openai-cookbook,2120,Remove your bots,closed,2025-09-08 05:17:24,2025-11-18 02:12:14,Stale
openai/openai-cookbook,2119,Update Cookbook introduction,closed,2025-09-07 00:16:44,2025-11-24 02:16:19,Stale
openai/openai-cookbook,2118,[SUPPORT] memory for a single H100 in fine tuning demo,open,2025-09-06 23:38:02,,support
openai/openai-cookbook,2117,[PROBLEM] jo,closed,2025-09-06 16:53:20,2025-11-20 02:10:54,"bug,Stale"
openai/openai-cookbook,2116,[New Cookbook] Semantic Search with DuckDB and OpenAI Embeddings,open,2025-09-06 16:25:36,,
openai/openai-cookbook,2114,Adding predictive agents cookbook,closed,2025-09-04 22:52:13,2025-11-21 02:11:26,Stale
openai/openai-cookbook,2113,Bump form-data in /examples/voice_solutions/one_way_translation_using_realtime_api,closed,2025-09-04 05:06:56,2025-11-14 02:12:24,"dependencies,javascript,Stale"
openai/openai-cookbook,2112,Bump brace-expansion from 1.1.11 to 1.1.12 in /examples/voice_solutions/one_way_translation_using_realtime_api,closed,2025-09-04 04:54:15,2025-11-14 02:12:26,"dependencies,javascript,Stale"
openai/openai-cookbook,2111,[FEATURE],closed,2025-09-04 04:08:29,2025-11-15 02:10:01,Stale
openai/openai-cookbook,2110,add ripgrep to codex+gitlab cookbook,closed,2025-09-03 11:24:36,2025-11-22 02:08:44,Stale
openai/openai-cookbook,2109,[SUPPORT],closed,2025-09-03 08:29:27,2025-11-16 02:15:59,"support,Stale"
openai/openai-cookbook,2108,Update examples/How_to_count_tokens_with_tiktoken.ipynb,open,2025-09-02 21:20:23,,
openai/openai-cookbook,2106,Fix typo in GPT-5 prompting guide regarding API features,open,2025-09-02 16:19:58,,
openai/openai-cookbook,2105,[How to call function with chat model] Pretty printing applied + gpt-4o instead of gpt-5 for one failing cell,closed,2025-09-01 17:22:06,2025-11-26 08:40:51,Stale
openai/openai-cookbook,2104,Update prompt-optimization-cookbook.ipynb,closed,2025-09-01 16:03:43,2025-11-22 02:08:46,Stale
openai/openai-cookbook,2103,[FEATURE] Add an rss feed,closed,2025-09-01 14:25:39,2025-11-24 02:16:20,Stale
openai/openai-cookbook,2102,Add example to count tokens for structured responses,closed,2025-08-31 19:28:36,2025-11-10 02:14:50,Stale
openai/openai-cookbook,2101,Docs: Fix typos and grammar in openai-harmony,closed,2025-08-31 08:31:14,2025-11-10 02:14:52,Stale
openai/openai-cookbook,2092,fixed directory name in GPT Actions library - AWS Middleware,open,2025-08-28 11:41:19,,
openai/openai-cookbook,2091,[FEATURE],closed,2025-08-28 03:38:18,2025-11-07 02:11:25,Stale
openai/openai-cookbook,2090,fix : Correct image path in deep research agents notebook,closed,2025-08-26 18:02:56,2025-11-24 02:16:23,Stale
openai/openai-cookbook,2087,Update authors.yaml,closed,2025-08-26 17:16:28,2025-11-06 02:12:21,Stale
openai/openai-cookbook,2083,add agents example with memory,closed,2025-08-25 12:21:21,2025-11-05 02:12:21,Stale
openai/openai-cookbook,2080,[PROBLEM],open,2025-08-22 12:39:25,,"bug,Stale"
openai/openai-cookbook,2079,"This PR promotes issue handling traits (namely, the `IssueHandlingTrait` type) to non-`@_spi` public API, now that it has been accepted by the Testing Workgroup.",closed,2025-08-22 10:58:44,2025-11-01 02:13:21,Stale
openai/openai-cookbook,2078,[SUPPORT],closed,2025-08-21 14:55:01,2025-10-31 02:11:12,"support,Stale"
openai/openai-cookbook,2077,Added Agenta to related_resources.md,open,2025-08-20 16:30:48,,
openai/openai-cookbook,2076,Add cookbook for OpenAI + dbt to query trusted data,open,2025-08-20 15:55:08,,
openai/openai-cookbook,2075,How does openAI framework converts the ChatComplete (training data ) into model ready format (SFT),closed,2025-08-20 10:13:01,2025-10-31 02:11:13,Stale
openai/openai-cookbook,2074,[FEATURE],closed,2025-08-18 20:25:41,2025-10-28 02:08:50,Stale
openai/openai-cookbook,2071,Fix: Resolve Windows clone failure from invoice directory with trailing space,open,2025-08-18 05:36:37,,
openai/openai-cookbook,2068,[PROBLEM] Blank response from transformer serve using openai/gpt-oss-20b,open,2025-08-16 20:40:15,,"bug,Stale"
openai/openai-cookbook,2065,Add new prompting resource,closed,2025-08-15 20:12:42,2025-10-26 02:13:08,Stale
openai/openai-cookbook,2063,VLLM Harmony mode not compatible with openai Agent SDK,open,2025-08-15 12:41:06,,"bug,Stale"
openai/openai-cookbook,2062,Persistent bug: Unable to switch back to basic voice mode,open,2025-08-14 18:31:30,,"bug,Stale"
openai/openai-cookbook,2061,Documentation: Disconnect between synthetic data generation and eval execution in Getting Started tutorial,closed,2025-08-14 16:02:12,2025-10-25 02:06:52,Stale
openai/openai-cookbook,2060,Fixed Typo in gpt-5_prompting_guide.ipynb,closed,2025-08-14 11:36:17,2025-11-25 02:12:48,Stale
openai/openai-cookbook,2058,[FEATURE]Please retain the o4-mini model as an option after o5 release for precise code-refactoring tasks,closed,2025-08-14 07:14:30,2025-10-25 02:06:53,Stale
openai/openai-cookbook,2050,[FEATURE],closed,2025-08-12 20:05:36,2025-10-22 02:11:31,Stale
openai/openai-cookbook,2046,[Default#color#Bug#fix],open,2025-08-12 08:08:29,,"bug,Stale"
openai/openai-cookbook,2042,"Getting ""ModuleNotFoundError: No module named 'triton.tools.ragged_tma'""",closed,2025-08-11 13:34:16,2025-10-21 02:09:35,Stale
openai/openai-cookbook,2041,Fix OSS 20B vLLM example: add offline-serve workflow (no flash-infer sm7+) - Update run-vllm.md,closed,2025-08-11 06:11:27,2025-10-21 02:09:37,Stale
openai/openai-cookbook,2040,"Fix typos, links, section and phrasing on all GPT-5 examples",open,2025-08-10 12:48:09,,
openai/openai-cookbook,2038,Update openai-harmony.md,closed,2025-08-10 03:49:24,2025-10-22 02:11:33,Stale
openai/openai-cookbook,2037,[PROBLEM] possible typo,open,2025-08-09 10:11:37,,"bug,Stale"
openai/openai-cookbook,2035,chore: fix typo in gpt-5_new_params_and_tools.ipynb,closed,2025-08-08 20:27:31,2025-10-19 02:15:14,Stale
openai/openai-cookbook,2033,fix prompt typos,closed,2025-08-08 16:35:12,2025-10-19 02:15:16,Stale
openai/openai-cookbook,2029,Fix typos in gpt-5_new_params_and_tools.ipynb,closed,2025-08-08 12:58:06,2025-10-19 02:15:17,Stale
openai/openai-cookbook,2026,Fix content issues with GPT-5 prompting guide,closed,2025-08-07 22:02:02,2025-10-17 02:06:52,Stale
openai/openai-cookbook,2024,Bump on-headers and compression in /examples/voice_solutions/one_way_translation_using_realtime_api,closed,2025-08-07 20:56:22,2025-10-17 02:06:54,"dependencies,javascript,Stale"
openai/openai-cookbook,2019,Remove duplicate paragraph in prompting guide,closed,2025-08-07 19:45:20,2025-10-12 03:56:27,Stale
openai/openai-cookbook,2012,Came across a recursive suppression pattern study while digging into prompt reflection. Might help model training behavior.,closed,2025-08-07 16:42:30,2025-10-17 02:06:56,Stale
openai/openai-cookbook,2011,Fragment sentence in the new gpt-oss guide,closed,2025-08-07 15:35:00,2025-10-18 02:03:29,Stale
openai/openai-cookbook,2002,vLLM deployment example with SkyPilot,closed,2025-08-06 17:33:08,2025-10-22 02:11:34,Stale
openai/openai-cookbook,2001,[I cannot finetune the model],open,2025-08-06 09:23:14,,"bug,Stale"
openai/openai-cookbook,1997,Update Chroma notebook,closed,2025-08-06 01:51:06,2025-10-20 02:14:00,Stale
openai/openai-cookbook,1996,OpenAI harmony format hyperlink in the gpt-oss llama blog points to example.com,closed,2025-08-06 01:29:57,2025-10-23 02:08:18,Stale
openai/openai-cookbook,1995,[PROBLEM] Incomplete sentence in ‚ÄúResponses API workarounds‚Äù section,open,2025-08-06 00:37:22,,"bug,Stale"
openai/openai-cookbook,1994,VLLM Serve with OSS model is broken,closed,2025-08-05 22:09:06,2025-11-14 22:57:26,bug
openai/openai-cookbook,1980,create codex environment for a specific branch,closed,2025-08-03 15:06:23,2025-10-13 02:11:32,Stale
openai/openai-cookbook,1979,fix(#1937): use dot notation for ActionSearch to fix TypeError,open,2025-08-01 02:13:59,,
openai/openai-cookbook,1977,fix: broken image links in deep research notebooks,closed,2025-07-29 09:34:02,2025-10-23 02:08:20,Stale
openai/openai-cookbook,1976,feat: Add Dragonfly vector database cookbook with hybrid search examples,open,2025-07-28 16:28:10,,
openai/openai-cookbook,1974,[Feature Request] Support for Persistent Custom Prompt Modules and Attribution Hooks,closed,2025-07-25 15:53:27,2025-10-05 02:11:56,Stale
openai/openai-cookbook,1972,Add OpenAI Responses API support to parallel processor,closed,2025-07-24 21:11:04,2025-10-05 02:11:58,Stale
openai/openai-cookbook,1966,Adding new encoding description for gpt-4.1 and gpt-4.5 models,open,2025-07-20 00:33:31,,Stale
openai/openai-cookbook,1933,Fix typo and grammar in introduction_to_deep_research_api.ipynb,open,2025-07-05 02:03:54,,
openai/openai-cookbook,1927,Add SurrealDB + semantic search blog post,open,2025-06-27 02:41:30,,Stale
openai/openai-cookbook,1872,Fix typos,open,2025-05-30 13:01:33,,
openai/openai-cookbook,1853,Add new article-to-podcast cookbook,closed,2025-05-23 15:27:51,2025-10-05 02:12:00,Stale
openai/openai-cookbook,1823,docs: Simplify concurrent flow,closed,2025-05-07 21:23:04,2025-10-04 02:01:40,Stale
openai/openai-cookbook,1271,new-contrib: Audio Whisper API with Local Device Microphones ,open,2024-07-06 06:08:01,,
milvus-io/pymilvus,3109,[Bug]: ËøûÊé•milvusËæìÂÖ•Áî®Êà∑ÂêçÂíåÂØÜÁ†ÅÂíåtokenÁöÑÊÉÖÂÜµÔºåtoken‰∏∫NoneËøûÊé•‰ºöÊä•Èîô,open,2025-11-28 08:29:04,,kind/bug
milvus-io/pymilvus,3108,[enhance]: Add some missing features to AsyncMilvusClient and refactor common code,closed,2025-11-27 08:39:10,2025-11-27 09:57:03,"approved,lgtm,ci-passed,size/XL,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3107,[Enhancement]: refactor client module use a base client class for aysnc/sync client,open,2025-11-27 07:03:29,,kind/enhancement
milvus-io/pymilvus,3106,[WIP] ci: add auto cherry-pick workflow for backporting,open,2025-11-27 06:56:38,,"size/L,ci-passed,do-not-merge/work-in-progress,dco-passed"
milvus-io/pymilvus,3105,fix: use deepcopy for struct_schema in add_field to prevent reuse issues,closed,2025-11-27 03:58:34,2025-11-27 09:53:03,"approved,lgtm,size/M,ci-passed,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3104,"[Bug]: Reusing struct_schema object in multiple ARRAY<STRUCT> fields causes ""duplicated field name"" error",closed,2025-11-27 03:51:38,2025-11-27 09:53:04,
milvus-io/pymilvus,3103,[Bug]: BulkWriter fails after upgrading MinIO to 7.2.19,open,2025-11-25 17:48:01,,kind/bug
milvus-io/pymilvus,3102,enhance: improve AsyncMilvusClient connection error message,closed,2025-11-25 06:50:39,2025-11-28 02:09:09,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3101,fix: AsyncMilvusClient.search support EmbeddingList,closed,2025-11-25 06:27:27,2025-11-26 10:19:03,"approved,lgtm,size/L,ci-passed,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3100,fix: AsyncMilvusClient.create_index support nested fields,closed,2025-11-25 06:25:57,2025-11-26 08:55:03,"approved,lgtm,size/L,ci-passed,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3099,[Enhancement]: improve Async Connection Error Message,open,2025-11-25 02:40:09,,kind/enhancement
milvus-io/pymilvus,3098,fix: fixed the key names for retrieving segment info in the MilvusClient,closed,2025-11-24 09:44:24,2025-11-24 09:49:02,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3097,[Bug]: AsyncMilvusClient.create_index fails for nested fields (Array of Struct) validation,open,2025-11-24 06:31:17,,kind/bug
milvus-io/pymilvus,3096,[cherry-pick] cherry-pick mutiple commits,closed,2025-11-21 08:01:34,2025-11-25 08:39:03,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3095,cherry-pick2.6 :fix(async) include event loop ID in connection alias to prevent reusing closed connections,closed,2025-11-21 06:41:25,2025-11-21 07:59:43,"size/M,ci-passed,needs-dco"
milvus-io/pymilvus,3094,Modify timestamptz example (#3093),closed,2025-11-21 06:14:38,2025-11-21 06:33:00,"approved,lgtm,ci-passed,size/XL,dco-passed"
milvus-io/pymilvus,3093,Modify timestamptz example,closed,2025-11-21 04:15:49,2025-11-21 04:19:00,"approved,lgtm,ci-passed,size/XL,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3092,feat: timestamptz support (#3002),closed,2025-11-21 03:13:48,2025-11-21 03:37:00,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3091,[WIP] Breaking change: Rename Stage to Volume,open,2025-11-21 03:04:22,,"ci-passed,do-not-merge/work-in-progress,size/XL,needs-dco"
milvus-io/pymilvus,3090,fix:async flush() not waiting for segments to be flushed,closed,2025-11-21 02:26:16,2025-11-21 06:31:02,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3089,fix: Add db_name parameter for list_import_jobs(),closed,2025-11-20 08:56:44,2025-11-20 09:11:16,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3088,fix: Add db_name parameter for list_import_jobs(),closed,2025-11-20 08:55:58,2025-11-20 09:05:03,"approved,lgtm,size/XS,ci-passed,dco-passed"
milvus-io/pymilvus,3087,[Bug]: async connection try to reuse closed event loop,open,2025-11-20 08:44:27,,kind/bug
milvus-io/pymilvus,3086,fix(async): include event loop ID in connection alias to prevent reusing closed connections,closed,2025-11-20 07:46:34,2025-11-21 06:31:02,"approved,lgtm,size/M,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3085,fix: Fix a bug of reset connection,closed,2025-11-20 07:32:51,2025-11-20 09:01:03,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3084,fix: Fix a bug of reset connection,closed,2025-11-20 07:30:47,2025-11-20 09:11:05,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3083,enhance: Add snapshot management APIs for collection backup and restore,open,2025-11-20 03:41:28,,"size/XXL,dco-passed"
milvus-io/pymilvus,3082,fix: Fix async flush() not waiting for segments to be flushed (#3060),closed,2025-11-19 09:41:00,2025-11-20 09:01:25,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3081,[cherry-pick] support stageManager & stageFileManager (#2936),closed,2025-11-18 11:47:00,2025-11-19 03:55:59,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3080,[cp 2.6] function sdk,open,2025-11-18 10:18:07,,"size/L,ci-passed,do-not-merge/work-in-progress,dco-passed"
milvus-io/pymilvus,3079,Support function edit sdk,open,2025-11-18 09:50:58,,"size/XXL,ci-passed,do-not-merge/work-in-progress,dco-passed"
milvus-io/pymilvus,3078,[Enhancement] allows to specufy temp local path for RemoteBulkWriter for temporarily persisting data files,closed,2025-11-14 09:16:32,2025-11-14 09:45:34,"approved,lgtm,size/S,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3077,[Enhancement] allows to specufy temp local path for RemoteBulkWriter for temporarily persisting data files,closed,2025-11-14 09:15:30,2025-11-14 09:47:34,"approved,lgtm,size/S,ci-passed,dco-passed"
milvus-io/pymilvus,3076,Fix: MilvusClient.insert() does not pass **kwargs to underlying insert_rows() call,closed,2025-11-14 04:02:39,2025-11-18 03:31:35,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3075,[Bug]: MilvusClient.insert() does not pass **kwargs to underlying insert_rows() call,closed,2025-11-14 03:45:02,2025-11-18 03:31:36,kind/bug
milvus-io/pymilvus,3074,fix: fix return empty array with STRUCT field [2.6],closed,2025-11-14 03:11:05,2025-11-14 06:15:34,"approved,lgtm,size/M,ci-passed,dco-passed"
milvus-io/pymilvus,3073,fix: fix return empty array with STRUCT field,closed,2025-11-14 03:07:56,2025-11-14 06:15:34,"approved,lgtm,size/M,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3072,fix: Fix a bug of connection cache for collection schema,closed,2025-11-13 10:03:11,2025-11-13 10:17:34,"approved,lgtm,size/XS,ci-passed,dco-passed"
milvus-io/pymilvus,3071,fix: Fix a bug of connection cache for collection schema,closed,2025-11-13 10:02:16,2025-11-13 11:25:33,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3070,fix: Add support for numpy ndarrays in Array fields (#3069),closed,2025-11-12 11:17:39,2025-11-14 07:35:33,"approved,lgtm,size/S,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3069,"[Bug]: {paragraphs} field should be a struct array, but got a {<class 'numpy.ndarray'>} instead. Detail: Field 'paragraphs': Expected  list, got ndarray)>",closed,2025-11-12 07:09:24,2025-11-14 07:35:34,kind/bug
milvus-io/pymilvus,3068,[cherry-pick] Support struct field for BulkWriter,closed,2025-11-11 08:27:59,2025-11-12 04:35:33,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3067,[Bug]: partial_update not working,open,2025-11-09 17:06:09,,kind/bug
milvus-io/pymilvus,3066,Bug: keep_aliveËá™Âä®ÈáçËøû‰∏¢Â§±using_databaseËÆæÁΩÆÔºåÂØºËá¥db_nameÂõûÈÄÄ‰∏∫default,closed,2025-11-07 09:59:48,2025-11-21 09:04:58,kind/bug
milvus-io/pymilvus,3065,[Bug]: [async] async create collection missing authorization in header,closed,2025-11-07 02:55:05,2025-11-24 07:07:59,kind/bug
milvus-io/pymilvus,3064,feat: Add target size in compaction,closed,2025-11-06 09:38:19,2025-11-20 09:10:39,"approved,lgtm,ci-passed,size/XL,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3063,[cherry-pick] GCP supports using stage,closed,2025-11-06 08:35:54,2025-11-06 08:53:38,"approved,lgtm,size/M,ci-passed,dco-passed"
milvus-io/pymilvus,3062,[cherry-pick] GCP supports using stage,closed,2025-11-06 08:34:28,2025-11-06 08:53:38,"approved,lgtm,size/M,ci-passed,dco-passed"
milvus-io/pymilvus,3061,enhance: pymilvus support minhash function,open,2025-11-05 11:54:33,,"size/L,ci-passed,do-not-merge/hold,dco-passed"
milvus-io/pymilvus,3060,[Bug]: async flush() does not wait for completed,closed,2025-11-05 11:49:37,2025-11-25 03:29:10,kind/bug
milvus-io/pymilvus,3059,[Bug]: AsyncMilvusClient does not support struct array,open,2025-11-05 09:26:42,,kind/bug
milvus-io/pymilvus,3058,[Bug]:,closed,2025-11-05 03:56:25,2025-11-13 11:25:34,kind/bug
milvus-io/pymilvus,3057,Support struct field for BulkWriter,closed,2025-11-04 08:07:23,2025-11-11 08:27:33,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3056,GCP supports using stage,closed,2025-11-03 03:50:46,2025-11-06 08:53:38,"approved,lgtm,size/M,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3055,Support TypeParams in STRUCT [2.6],closed,2025-10-31 06:41:32,2025-11-03 03:31:31,"approved,lgtm,size/XXL,ci-passed,dco-passed"
milvus-io/pymilvus,3054,Support TypeParams in STRUCT,closed,2025-10-30 09:57:57,2025-11-03 03:31:31,"approved,lgtm,size/XXL,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3053,fix: [2.5] Bulk writer support geometry,closed,2025-10-30 03:31:39,2025-10-31 06:30:04,"approved,lgtm,size/S,ci-passed,dco-passed"
milvus-io/pymilvus,3052,fix: [2.6] Bulk writer support geometry,closed,2025-10-30 03:31:18,2025-10-31 06:30:04,"approved,lgtm,size/S,ci-passed,dco-passed"
milvus-io/pymilvus,3051,fix: Bulk writer support geometry,closed,2025-10-30 03:30:32,2025-10-31 06:30:04,"approved,lgtm,size/S,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3050,[Bug]: Bulk writer does not support struct array and geometry datatype,closed,2025-10-29 09:09:55,2025-11-13 09:34:01,kind/bug
milvus-io/pymilvus,3049,[Bug]: wait_for_index_building_complete does not work correctly,open,2025-10-28 17:34:05,,kind/bug
milvus-io/pymilvus,3048,enhance: Cherry pick multiple prs from master branch,closed,2025-10-27 07:00:51,2025-10-27 07:14:02,"approved,lgtm,size/M,ci-passed,dco-passed"
milvus-io/pymilvus,3047,Add example for STRUCT import,closed,2025-10-27 03:40:45,2025-10-30 07:52:03,"approved,lgtm,ci-passed,size/XL,dco-passed"
milvus-io/pymilvus,3046,feat: add detailed traceback to error_handler decorator,closed,2025-10-24 09:52:35,2025-11-20 09:10:04,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3045,enhance: Migrate six APIs to MilvusClient ,closed,2025-10-23 10:36:51,2025-10-28 09:42:04,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3044,enhance: Add proper deprecated warning,open,2025-10-23 07:49:32,,"size/M,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3043,"feat:Support geo data type for insert, query and search (#2925)",closed,2025-10-23 07:26:52,2025-10-23 07:29:57,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3042,[Enhancement]: Remove not inuse get in GrpcHander,open,2025-10-23 03:26:29,,kind/enhancement
milvus-io/pymilvus,3041,[Enhancement]: Remove not inused _get_info in GrpcHandler,open,2025-10-23 03:02:35,,kind/enhancement
milvus-io/pymilvus,3040,fix: Declare `pymilvus` as a namespace package.,closed,2025-10-21 16:45:06,2025-10-22 02:12:04,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3039,[Bug]: `pymilvus` is not a namespace package which can break using it with `pymilvus.model` when multiple `sys.path` entries are in play.,closed,2025-10-21 16:37:00,2025-10-22 02:12:05,kind/bug
milvus-io/pymilvus,3038,feat: Use Cython to speed up,open,2025-10-21 04:20:17,,"approved,size/L,ci-passed,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3037,feat: add STRUCT level mmap config [2.6],closed,2025-10-21 03:11:36,2025-10-21 03:15:58,"approved,lgtm,size/XS,ci-passed,dco-passed"
milvus-io/pymilvus,3036,feat: add STRUCT level mmap config,closed,2025-10-21 03:08:59,2025-10-21 03:15:58,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3035,Add PK in the upsert response (Issue: #3033),closed,2025-10-20 08:57:57,2025-10-21 09:51:57,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3034,Add async upsert response with primary key (Issue: #3033),closed,2025-10-20 08:40:49,2025-10-20 08:47:46,"size/XS,dco-passed"
milvus-io/pymilvus,3033,[Enhancement]: Async Upsert: Support response with Primary Key.,closed,2025-10-20 06:15:45,2025-11-06 06:27:04,kind/enhancement
milvus-io/pymilvus,3032,Support insert serialized json (#3024),closed,2025-10-20 03:56:49,2025-10-20 07:17:56,"approved,lgtm,size/S,ci-passed,dco-passed"
milvus-io/pymilvus,3031,Support insert serialized json (#3024),closed,2025-10-20 03:55:53,2025-10-20 08:49:56,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3030,[Bug]: High concurrent number of requests causes RecursionError,open,2025-10-18 00:14:24,,kind/bug
milvus-io/pymilvus,3029,feat(perf): Add comprehensive benchmarking framework,open,2025-10-17 10:57:56,,"approved,size/XXL,ci-passed,dco-passed,PR | need to cherry-pick to 2.x"
milvus-io/pymilvus,3028,Fix hybrid_search to support EmbeddingList in request data,closed,2025-10-17 08:52:23,2025-10-17 09:21:55,"approved,lgtm,size/S,ci-passed,dco-passed"
milvus-io/pymilvus,3027,Fix hybrid_search to support EmbeddingList in request data,closed,2025-10-17 07:32:18,2025-10-17 08:43:55,"approved,lgtm,size/S,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3026,feat: sdk for struct [2.6],closed,2025-10-17 06:56:35,2025-10-17 07:19:55,"approved,lgtm,size/XXL,ci-passed,dco-passed"
milvus-io/pymilvus,3025,[QUESTION]: Pymilvus 2.5 compatibility with Milvus 2.6.2,closed,2025-10-16 18:30:17,2025-10-27 14:29:08,kind/question
milvus-io/pymilvus,3024,Support insert serialized json,closed,2025-10-14 09:05:31,2025-10-20 03:51:56,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3023,Feature: add retry and schema cache for async pymilvus (#3018),closed,2025-10-14 03:32:36,2025-10-14 06:43:53,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,3022,wrong setuptools_scm integration due to break with setuptools_scm 10.0,open,2025-10-13 10:29:37,,
milvus-io/pymilvus,3021,enhance: [2.6] raise error when ranker with unknwon type,closed,2025-10-13 03:39:32,2025-10-13 03:51:55,"approved,lgtm,size/XS,ci-passed,dco-passed"
milvus-io/pymilvus,3020,enhance: raise error when ranker with unknwon type,closed,2025-10-13 03:34:02,2025-10-13 03:53:52,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3019,"[Bug]: insert 10000 int8_vector(dim=768), run out of memory in client-side",open,2025-10-11 08:13:16,,kind/bug
milvus-io/pymilvus,3018,Feature: add retry and schema cache for async pymilvus,closed,2025-10-11 05:08:09,2025-10-13 03:51:55,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,3017,enhance: Enable to auto pr branch2.6 and avoid dup PRs,closed,2025-10-09 07:05:41,2025-10-09 07:21:51,"approved,lgtm,size/M,ci-passed,dco-passed"
milvus-io/pymilvus,3016,enhance: Remove all annoying logs in MilvusClient,closed,2025-10-09 06:56:48,2025-10-09 10:27:51,"approved,lgtm,ci-passed,size/XL,dco-passed"
milvus-io/pymilvus,3015,bulk_writer tries to use /usr/local/lib/python3.11/site-packages/bulk_writer and runs into permission error,open,2025-10-02 20:37:41,,
milvus-io/pymilvus,3012,Milvus expr fails when using $meta JSON path with boolean OR and null check,open,2025-09-27 10:58:45,,kind/question
milvus-io/pymilvus,3008,[FEATURE]: Support mannual L0 compact,closed,2025-09-24 09:48:27,2025-11-18 06:31:32,kind/feature
milvus-io/pymilvus,3004,"[Bug]: Pymilvus requires orjson as a dependency, but it's not declared in pyproject.toml",closed,2025-09-22 12:36:14,2025-10-17 08:48:10,kind/question
milvus-io/pymilvus,3002,feat: timestamptz support,closed,2025-09-22 07:39:18,2025-09-23 03:02:07,"approved,lgtm,size/L,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,2993,enhance: Replace ujson with orjson,closed,2025-09-17 12:21:11,2025-09-19 07:11:54,"approved,lgtm,size/M,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,2992,feat: support use function scorer as ranker not one function,closed,2025-09-17 07:10:58,2025-09-30 07:43:42,"approved,lgtm,size/M,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,2991,[Enhancement]: Type-Safety for Async Milvus Client,open,2025-09-16 19:31:39,,kind/enhancement
milvus-io/pymilvus,2982,[Bug]: list_import_jobs should have a db_name argument not a collection_name argument,open,2025-09-11 00:09:13,,kind/bug
milvus-io/pymilvus,2981,[Bug]: Mmap manager has not been init ,open,2025-09-10 20:14:09,,"kind/bug,milvus-lite"
milvus-io/pymilvus,2978,feat: sdk for struct,closed,2025-09-09 08:40:38,2025-10-17 07:07:55,"approved,lgtm,size/XXL,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,2968,enhance: Remove all annoying logs in MilvusClient ,closed,2025-09-01 10:14:39,2025-10-09 07:17:50,"approved,lgtm,ci-passed,size/XL,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,2930,Avoid describe_collection when query by ids,closed,2025-08-13 07:30:05,2025-08-18 06:41:39,"approved,lgtm,size/XS,ci-passed,dco-passed"
milvus-io/pymilvus,2929,Avoid describe_collection when query by ids,closed,2025-08-13 07:28:34,2025-08-13 07:37:37,"approved,lgtm,size/XS,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,2924,[Enhancement]: Support geometry type for pymilvus,closed,2025-08-06 11:13:54,2025-11-19 07:15:49,kind/enhancement
milvus-io/pymilvus,2916,"[Bug]: MilvusException: <MilvusException: (code=0, message=104 data type not supported)>",closed,2025-07-30 18:43:52,2025-11-19 18:05:58,kind/bug
milvus-io/pymilvus,2914,fix: Return new pk value for upsert when autoid=true,closed,2025-07-30 04:42:24,2025-07-30 05:17:30,"approved,lgtm,size/XS,ci-passed,dco-passed"
milvus-io/pymilvus,2910,[Bug]: MilvusLite doesn't support nullable,open,2025-07-25 08:08:11,,"kind/bug,milvus-lite"
milvus-io/pymilvus,2909,[Bug]: not need to log error before raise exception,closed,2025-07-24 12:04:38,2025-10-10 02:12:25,kind/bug
milvus-io/pymilvus,2847,[Bug]: milvus client upsert data errorÔºöInsert missed an field `id` to collection without set nullable==true or set default_value,open,2025-06-10 07:27:40,,kind/bug
milvus-io/pymilvus,2796,[Bug]: fail to load partitions with async client,closed,2025-05-20 02:48:45,2025-11-05 09:09:36,kind/bug
milvus-io/pymilvus,2795,Function params support complex data,closed,2025-05-16 08:17:44,2025-05-21 09:22:19,"approved,lgtm,size/XS,ci-passed,dco-passed"
milvus-io/pymilvus,2784,enhance: Refactor validator for more flexible checks,closed,2025-05-08 07:33:09,2025-09-18 09:41:22,"approved,size/L,do-not-merge/work-in-progress,dco-passed"
milvus-io/pymilvus,2770,Fix function construct bug,closed,2025-04-29 02:23:37,2025-04-29 03:50:44,"approved,lgtm,size/S,ci-passed,dco-passed"
milvus-io/pymilvus,2762,Fix hybrid search params check,closed,2025-04-27 03:29:02,2025-04-28 03:22:34,"approved,lgtm,size/S,ci-passed,dco-passed,PR | cherry-picked to 2.x"
milvus-io/pymilvus,2760,Add funciton checker,closed,2025-04-25 09:37:56,2025-04-27 03:00:33,"approved,lgtm,size/S,ci-passed,dco-passed"
milvus-io/pymilvus,2758,[FEATURE]: Add check_health,closed,2025-04-25 03:12:28,2025-10-23 10:09:37,kind/feature
milvus-io/pymilvus,2755,[FEATURE]: Add transfer_node,closed,2025-04-25 03:11:14,2025-10-23 10:08:18,kind/feature
milvus-io/pymilvus,2729,Support rerank,closed,2025-04-16 09:59:14,2025-04-21 10:02:30,"approved,lgtm,size/L,ci-passed,dco-passed"
milvus-io/pymilvus,2727,[Bug]: if db_name is passed as a path in the uri it is overwritten with empty string,closed,2025-04-15 14:11:40,2025-11-19 07:43:02,kind/bug
milvus-io/pymilvus,2674,[QUESTION]: I am getting grpc.aio._call.AioRpcError when sending concurrent load_partitions requests,open,2025-02-27 09:26:01,,kind/question
milvus-io/pymilvus,2601,"[Bug]: Not providing auto ID field in upsert request raises an error ""Insert missed an field `id` to collection without set nullable==true or set default_value""",closed,2025-01-24 10:10:07,2025-11-19 07:45:07,"kind/bug,wontfix"
milvus-io/pymilvus,2166,[QUESTION]: Loosen environs package requirement,open,2024-07-03 11:14:34,,kind/question
milvus-io/pymilvus,2103,"[Bug]: <MilvusException: (code=1, message=Unexpected error, message=<Cannot invoke RPC: Channel closed!>)>",closed,2024-05-23 03:12:39,2025-01-13 06:04:24,kind/bug
milvus-io/pymilvus,944,[QUESTION]: grpc_message: Connection reset by peer,open,2022-04-06 02:22:04,,
